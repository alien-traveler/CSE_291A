# -*- coding: utf-8 -*-
"""
RAG Pipeline (Local vLLM + BGE-Reranker version)
------------------------------------------------
Features:
- FAISS vector retrieval (BGE embedding)
- CrossEncoder reranker for precision
- Local vLLM model generation via OpenAI-compatible API
"""

import os
import json
import numpy as np
import faiss
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import OpenAI


# ====================== å…¨å±€é…ç½® ======================
EMBED_MODEL = "BAAI/bge-m3"                   # å‘é‡æ¨¡å‹
RERANK_MODEL = "BAAI/bge-reranker-large"      # é‡æ’åºæ¨¡å‹
INDEX_PATH = "rag_index.faiss"
EMBED_PATH = "rag_embeddings.npy"
DATA_PATH = "rag_data.jsonl"

# âœ… æœ¬åœ° vLLM æœåŠ¡é…ç½®
VLLM_BASE_URL = "http://localhost:8082/v1"
VLLM_MODEL = "Qwen/Qwen3-0.6B"       # æ¨¡å‹åä¸å¯åŠ¨å‘½ä»¤ä¿æŒä¸€è‡´


# ====================== åˆå§‹åŒ–æ¨¡å‹ ======================
print("ğŸ”¹ Loading embedding model...")
embedder = SentenceTransformer(EMBED_MODEL, cache_folder="./model")

print("ğŸ”¹ Loading FAISS index and embeddings...")
index = faiss.read_index(INDEX_PATH)
embeddings = np.load(EMBED_PATH)
with open(DATA_PATH, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]
print(f"âœ… Loaded {len(data)} chunks from {DATA_PATH}")

print("ğŸ”¹ Loading reranker model (this may take a while)...")
reranker = CrossEncoder(RERANK_MODEL, cache_folder="./model")

# âœ… åˆå§‹åŒ–æœ¬åœ° vLLM å®¢æˆ·ç«¯
client = OpenAI(
    base_url=VLLM_BASE_URL,  # vLLM å¯åŠ¨æ—¶çš„åœ°å€
    api_key="EMPTY"           # ä»»æ„å­—ç¬¦ä¸²å³å¯ï¼ˆvLLM ä¸æ ¡éªŒï¼‰
)


# ====================== é˜¶æ®µ 1ï¼šæ£€ç´¢ ======================
def retrieve(query: str, top_k: int = 20):
    """
    Step 1: ä» FAISS æ£€ç´¢ top_k ç›¸å…³æ–‡æ®µï¼ˆç²—æ£€ç´¢ï¼‰
    """
    query_vec = embedder.encode([query], normalize_embeddings=True)
    D, I = index.search(query_vec.astype(np.float32), top_k)
    results = [data[i] for i in I[0]]
    return results


# ====================== é˜¶æ®µ 2ï¼šé‡æ’åº ======================
def rerank_results(query: str, retrieved_chunks, top_k: int = 5):
    """
    Step 2: ä½¿ç”¨ CrossEncoder å¯¹ FAISS æ£€ç´¢ç»“æœè¿›è¡ŒäºŒæ¬¡æ’åºï¼ˆç²¾æ’ï¼‰
    """
    pairs = [(query, chunk["text"]) for chunk in retrieved_chunks]
    scores = reranker.predict(pairs, batch_size=8, show_progress_bar=False)
    reranked = sorted(zip(retrieved_chunks, scores), key=lambda x: x[1], reverse=True)
    reranked_chunks = [r[0] for r in reranked[:top_k]]

    print(f"ğŸ” Reranked {len(retrieved_chunks)} â†’ top {top_k}")
    return reranked_chunks


# ====================== é˜¶æ®µ 3ï¼šç”Ÿæˆ ======================
def rag_generate(query: str, top_k: int = 5):
    """
    Step 3: åŸºäºé‡æ’åçš„æ–‡æ®µç”Ÿæˆå›ç­”ï¼ˆä½¿ç”¨æœ¬åœ° vLLMï¼‰
    """
    # â‘  FAISS æ£€ç´¢
    retrieved_chunks = retrieve(query, top_k=20)
    # â‘¡ Reranker ç²¾æ’
    reranked_chunks = rerank_results(query, retrieved_chunks, top_k=top_k)

    # æ‹¼æ¥ä¸Šä¸‹æ–‡
    context = "\n\n".join([r["text"] for r in reranked_chunks])

    # ç”Ÿæˆ prompt
    prompt = f"""
You are a helpful academic assistant.
Use the provided context to answer the question as accurately as possible.
If the context does not contain enough information, say so honestly.

Context:
{context}

Question: {query}
Answer:
"""

    # âœ… ä½¿ç”¨æœ¬åœ° vLLM ç”Ÿæˆå›ç­”
    completion = client.chat.completions.create(
        model=VLLM_MODEL,
        messages=[
            {"role": "system", "content": "You are an expert assistant for summarizing and analyzing research papers."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )

    answer = completion.choices[0].message.content.strip()
    return answer, reranked_chunks


# ====================== é˜¶æ®µ 4ï¼šäº¤äº’ç•Œé¢ ======================
if __name__ == "__main__":
    # print("\nğŸš€ Academic RAG Assistant (Local vLLM version) ready!\n")
    # while True:
    #     query = input("ğŸ” Enter your question (or 'exit' to quit): ").strip()
    #     if query.lower() in ["exit", "quit"]:
    #         break

    #     answer, refs = rag_generate(query, top_k=5)

    #     print("\nğŸ’¡ Answer:\n", answer)
    #     print("\nğŸ“š Top References:")
    #     for r in refs:
    #         meta = r["metadata"]
    #         print(f" - {meta.get('filename')} (chunk {meta.get('chunk_id')})")
    #     print("\n" + "-" * 60 + "\n")
    
    # âœ… ç›´æ¥åœ¨ä»£ç ä¸­è®¾ç½®æŸ¥è¯¢
    query = "What are the main contributions of vision-language models in 2025?"
    
    print(f"ğŸ” Query: {query}\n")
    
    answer, refs = rag_generate(query, top_k=5)

    print("\nğŸ’¡ Answer:\n", answer)
    print("\nğŸ“š Top References:")
    for r in refs:
        meta = r["metadata"]
        print(f" - {meta.get('filename')} (chunk {meta.get('chunk_id')})")
    print("\n" + "-" * 60 + "\n")


