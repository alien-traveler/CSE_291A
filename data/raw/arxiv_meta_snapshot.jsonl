{"arxiv_id":"2210.03945","title":"Understanding HTML with Large Language Models","abstract":"Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.","source":"http:\/\/arxiv.org\/pdf\/2210.03945","authors":["Izzeddin Gur","Ofir Nachum","Yingjie Miao","Mustafa Safdari","Austin Huang","Aakanksha Chowdhery","Sharan Narang","Noah Fiedel","Aleksandra Faust"],"categories":["cs.LG","cs.AI"],"published":"20221008","updated":"20230519","primary_category":"cs.LG"}
{"arxiv_id":"1711.05101","title":"Decoupled Weight Decay Regularization","abstract":"L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https:\/\/github.com\/loshchil\/AdamW-and-SGDW","source":"http:\/\/arxiv.org\/pdf\/1711.05101","authors":["Ilya Loshchilov","Frank Hutter"],"categories":["cs.LG","cs.NE","math.OC"],"published":"20171114","updated":"20190104","primary_category":"cs.LG"}
{"arxiv_id":"2305.17493","title":"The Curse of Recursion: Training on Generated Data Makes Models Forget","abstract":"Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.","source":"http:\/\/arxiv.org\/pdf\/2305.17493","authors":["Ilia Shumailov","Zakhar Shumaylov","Yiren Zhao","Yarin Gal","Nicolas Papernot","Ross Anderson"],"categories":["cs.LG","cs.AI","cs.CL","cs.CR","cs.CV"],"published":"20230527","updated":"20230531","primary_category":"cs.LG"}
{"arxiv_id":"2205.09712","title":"Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning","abstract":"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.","source":"http:\/\/arxiv.org\/pdf\/2205.09712","authors":["Antonia Creswell","Murray Shanahan","Irina Higgins"],"categories":["cs.AI","cs.CL"],"published":"20220519","updated":"20220519","primary_category":"cs.AI"}
{"arxiv_id":"2104.06001","title":"Gender Bias in Machine Translation","abstract":"Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.","source":"http:\/\/arxiv.org\/pdf\/2104.06001","authors":["Beatrice Savoldi","Marco Gaido","Luisa Bentivogli","Matteo Negri","Marco Turchi"],"categories":["cs.CL"],"published":"20210413","updated":"20210507","primary_category":"cs.CL"}
{"arxiv_id":"2002.02878","title":"I love your chain mail! Making knights smile in a fantasy game world: Open-domain goal-oriented dialogue agents","abstract":"Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a straightforward learning signal. Humans effortlessly combine the two, for example engaging in chit-chat with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning against an imitation-learned ``chit-chat'' model with two approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-K utterances from the chit-chat model. We show that both models outperform an inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals.","source":"http:\/\/arxiv.org\/pdf\/2002.02878","authors":["Shrimai Prabhumoye","Margaret Li","Jack Urbanek","Emily Dinan","Douwe Kiela","Jason Weston","Arthur Szlam"],"categories":["cs.AI","cs.CL","stat.ML"],"published":"20200207","updated":"20200210","primary_category":"cs.AI"}
{"arxiv_id":"1611.09268","title":"MS MARCO: A Human Generated MAchine Reading COmprehension Dataset","abstract":"We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.","source":"http:\/\/arxiv.org\/pdf\/1611.09268","authors":["Payal Bajaj","Daniel Campos","Nick Craswell","Li Deng","Jianfeng Gao","Xiaodong Liu","Rangan Majumder","Andrew McNamara","Bhaskar Mitra","Tri Nguyen","Mir Rosenberg","Xia Song","Alina Stoica","Saurabh Tiwary","Tong Wang"],"categories":["cs.CL","cs.IR"],"published":"20161128","updated":"20181031","primary_category":"cs.CL"}
{"arxiv_id":"2205.01663","title":"Adversarial Training for High-Stakes Reliability","abstract":"In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance. In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance. We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.","source":"http:\/\/arxiv.org\/pdf\/2205.01663","authors":["Daniel M. Ziegler","Seraphina Nix","Lawrence Chan","Tim Bauman","Peter Schmidt-Nielsen","Tao Lin","Adam Scherlis","Noa Nabeshima","Ben Weinstein-Raun","Daniel de Haas","Buck Shlegeris","Nate Thomas"],"categories":["cs.LG","cs.AI","cs.CL"],"published":"20220503","updated":"20221110","primary_category":"cs.LG"}
{"arxiv_id":"2109.07445","title":"Challenges in Detoxifying Language Models","abstract":"Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions -- highlighting further the nuances involved in careful evaluation of LM toxicity.","source":"http:\/\/arxiv.org\/pdf\/2109.07445","authors":["Johannes Welbl","Amelia Glaese","Jonathan Uesato","Sumanth Dathathri","John Mellor","Lisa Anne Hendricks","Kirsty Anderson","Pushmeet Kohli","Ben Coppin","Po-Sen Huang"],"categories":["cs.CL","cs.AI","cs.CY","cs.LG","I.2.6; I.2.7"],"published":"20210915","updated":"20210915","primary_category":"cs.CL"}
{"arxiv_id":"2301.03728","title":"Scaling Laws for Generative Mixed-Modal Language Models","abstract":"Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.","source":"http:\/\/arxiv.org\/pdf\/2301.03728","authors":["Armen Aghajanyan","Lili Yu","Alexis Conneau","Wei-Ning Hsu","Karen Hambardzumyan","Susan Zhang","Stephen Roller","Naman Goyal","Omer Levy","Luke Zettlemoyer"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20230110","updated":"20230110","primary_category":"cs.CL"}
{"arxiv_id":"1911.03343","title":"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly","abstract":"Building on Petroni et al. (2019), we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (\"Birds cannot [MASK]\") and non-negated (\"Birds can [MASK]\") cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add \"misprimes\" to cloze questions (\"Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.","source":"http:\/\/arxiv.org\/pdf\/1911.03343","authors":["Nora Kassner","Hinrich Sch\u00fctze"],"categories":["cs.CL"],"published":"20191108","updated":"20200515","primary_category":"cs.CL"}
{"arxiv_id":"2205.10625","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models","abstract":"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.","source":"http:\/\/arxiv.org\/pdf\/2205.10625","authors":["Denny Zhou","Nathanael Sch\u00e4rli","Le Hou","Jason Wei","Nathan Scales","Xuezhi Wang","Dale Schuurmans","Claire Cui","Olivier Bousquet","Quoc Le","Ed Chi"],"categories":["cs.AI","cs.CL"],"published":"20220521","updated":"20230416","primary_category":"cs.AI"}
{"arxiv_id":"1704.00109","title":"Snapshot Ensembles: Train 1, get M for free","abstract":"Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.","source":"http:\/\/arxiv.org\/pdf\/1704.00109","authors":["Gao Huang","Yixuan Li","Geoff Pleiss","Zhuang Liu","John E. Hopcroft","Kilian Q. Weinberger"],"categories":["cs.LG"],"published":"20170401","updated":"20170401","primary_category":"cs.LG"}
{"arxiv_id":"2004.07213","title":"Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims","abstract":"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","source":"http:\/\/arxiv.org\/pdf\/2004.07213","authors":["Miles Brundage","Shahar Avin","Jasmine Wang","Haydn Belfield","Gretchen Krueger","Gillian Hadfield","Heidy Khlaaf","Jingying Yang","Helen Toner","Ruth Fong","Tegan Maharaj","Pang Wei Koh","Sara Hooker","Jade Leung","Andrew Trask","Emma Bluemke","Jonathan Lebensold","Cullen O'Keefe","Mark Koren","Th\u00e9o Ryffel","JB Rubinovitz","Tamay Besiroglu","Federica Carugati","Jack Clark","Peter Eckersley","Sarah de Haas","Maritza Johnson","Ben Laurie","Alex Ingerman","Igor Krawczuk","Amanda Askell","Rosario Cammarota","Andrew Lohn","David Krueger","Charlotte Stix","Peter Henderson","Logan Graham","Carina Prunkl","Bianca Martin","Elizabeth Seger","Noa Zilberman","Se\u00e1n \u00d3 h\u00c9igeartaigh","Frens Kroeger","Girish Sastry","Rebecca Kagan","Adrian Weller","Brian Tse","Elizabeth Barnes","Allan Dafoe","Paul Scharre","Ariel Herbert-Voss","Martijn Rasser","Shagun Sodhani","Carrick Flynn","Thomas Krendl Gilbert","Lisa Dyer","Saif Khan","Yoshua Bengio","Markus Anderljung"],"categories":["cs.CY"],"published":"20200415","updated":"20200420","primary_category":"cs.CY"}
{"arxiv_id":"2304.01196","title":"Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data","abstract":"Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Furthermore, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT. The Baize models and data are released for research purposes only at https:\/\/github.com\/project-baize\/baize-chatbot. An online demo is also available at https:\/\/huggingface.co\/spaces\/project-baize\/chat-with-baize.","source":"http:\/\/arxiv.org\/pdf\/2304.01196","authors":["Canwen Xu","Daya Guo","Nan Duan","Julian McAuley"],"categories":["cs.CL","cs.AI"],"published":"20230403","updated":"20230523","primary_category":"cs.CL"}
{"arxiv_id":"2204.06125","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents","abstract":"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.","source":"http:\/\/arxiv.org\/pdf\/2204.06125","authors":["Aditya Ramesh","Prafulla Dhariwal","Alex Nichol","Casey Chu","Mark Chen"],"categories":["cs.CV"],"published":"20220413","updated":"20220413","primary_category":"cs.CV"}
{"arxiv_id":"1812.10757","title":"Advancing the State of the Art in Open Domain Dialog Systems through the Alexa Prize","abstract":"Building open domain conversational systems that allow users to have engaging conversations on topics of their choice is a challenging task. Alexa Prize was launched in 2016 to tackle the problem of achieving natural, sustained, coherent and engaging open-domain dialogs. In the second iteration of the competition in 2018, university teams advanced the state of the art by using context in dialog models, leveraging knowledge graphs for language understanding, handling complex utterances, building statistical and hierarchical dialog managers, and leveraging model-driven signals from user responses. The 2018 competition also included the provision of a suite of tools and models to the competitors including the CoBot (conversational bot) toolkit, topic and dialog act detection models, conversation evaluators, and a sensitive content detection model so that the competing teams could focus on building knowledge-rich, coherent and engaging multi-turn dialog systems. This paper outlines the advances developed by the university teams as well as the Alexa Prize team to achieve the common goal of advancing the science of Conversational AI. We address several key open-ended problems such as conversational speech recognition, open domain natural language understanding, commonsense reasoning, statistical dialog management, and dialog evaluation. These collaborative efforts have driven improved experiences by Alexa users to an average rating of 3.61, the median duration of 2 mins 18 seconds, and average turns to 14.6, increases of 14%, 92%, 54% respectively since the launch of the 2018 competition. For conversational speech recognition, we have improved our relative Word Error Rate by 55% and our relative Entity Error Rate by 34% since the launch of the Alexa Prize. Socialbots improved in quality significantly more rapidly in 2018, in part due to the release of the CoBot toolkit.","source":"http:\/\/arxiv.org\/pdf\/1812.10757","authors":["Chandra Khatri","Behnam Hedayatnia","Anu Venkatesh","Jeff Nunn","Yi Pan","Qing Liu","Han Song","Anna Gottardi","Sanjeev Kwatra","Sanju Pancholi","Ming Cheng","Qinglang Chen","Lauren Stubel","Karthik Gopalakrishnan","Kate Bland","Raefer Gabriel","Arindam Mandal","Dilek Hakkani-Tur","Gene Hwang","Nate Michel","Eric King","Rohit Prasad"],"categories":["cs.CL","cs.AI"],"published":"20181227","updated":"20181227","primary_category":"cs.CL"}
{"arxiv_id":"1810.04805","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","source":"http:\/\/arxiv.org\/pdf\/1810.04805","authors":["Jacob Devlin","Ming-Wei Chang","Kenton Lee","Kristina Toutanova"],"categories":["cs.CL"],"published":"20181011","updated":"20190524","primary_category":"cs.CL"}
{"arxiv_id":"1704.00051","title":"Reading Wikipedia to Answer Open-Domain Questions","abstract":"This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.","source":"http:\/\/arxiv.org\/pdf\/1704.00051","authors":["Danqi Chen","Adam Fisch","Jason Weston","Antoine Bordes"],"categories":["cs.CL"],"published":"20170331","updated":"20170428","primary_category":"cs.CL"}
{"arxiv_id":"1805.01667","title":"Intracranial Error Detection via Deep Learning","abstract":"Deep learning techniques have revolutionized the field of machine learning and were recently successfully applied to various classification problems in noninvasive electroencephalography (EEG). However, these methods were so far only rarely evaluated for use in intracranial EEG. We employed convolutional neural networks (CNNs) to classify and characterize the error-related brain response as measured in 24 intracranial EEG recordings. Decoding accuracies of CNNs were significantly higher than those of a regularized linear discriminant analysis. Using time-resolved deep decoding, it was possible to classify errors in various regions in the human brain, and further to decode errors over 200 ms before the actual erroneous button press, e.g., in the precentral gyrus. Moreover, deeper networks performed better than shallower networks in distinguishing correct from error trials in all-channel decoding. In single recordings, up to 100 % decoding accuracy was achieved. Visualization of the networks' learned features indicated that multivariate decoding on an ensemble of channels yields related, albeit non-redundant information compared to single-channel decoding. In summary, here we show the usefulness of deep learning for both intracranial error decoding and mapping of the spatio-temporal structure of the human error processing network.","source":"http:\/\/arxiv.org\/pdf\/1805.01667","authors":["Martin V\u00f6lker","Ji\u0159\u00ed Hammer","Robin T. Schirrmeister","Joos Behncke","Lukas D. J. Fiederer","Andreas Schulze-Bonhage","Petr Marusi\u010d","Wolfram Burgard","Tonio Ball"],"categories":["cs.LG","q-bio.NC","q-bio.QM","stat.ML","I.2.6; I.2.8; I.5.0; J.2; J.3"],"published":"20180504","updated":"20181102","primary_category":"cs.LG"}
{"arxiv_id":"2006.12467","title":"The Depth-to-Width Interplay in Self-Attention","abstract":"Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.","source":"http:\/\/arxiv.org\/pdf\/2006.12467","authors":["Yoav Levine","Noam Wies","Or Sharir","Hofit Bata","Amnon Shashua"],"categories":["cs.LG","cs.CL","stat.ML"],"published":"20200622","updated":"20210117","primary_category":"cs.LG"}
{"arxiv_id":"2006.16668","title":"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding","abstract":"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.","source":"http:\/\/arxiv.org\/pdf\/2006.16668","authors":["Dmitry Lepikhin","HyoukJoong Lee","Yuanzhong Xu","Dehao Chen","Orhan Firat","Yanping Huang","Maxim Krikun","Noam Shazeer","Zhifeng Chen"],"categories":["cs.CL","cs.LG","stat.ML"],"published":"20200630","updated":"20200630","primary_category":"cs.CL"}
{"arxiv_id":"1904.09728","title":"SocialIQA: Commonsense Reasoning about Social Interactions","abstract":"We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: \"Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?\" A: \"Make sure no one else could hear\"). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).","source":"http:\/\/arxiv.org\/pdf\/1904.09728","authors":["Maarten Sap","Hannah Rashkin","Derek Chen","Ronan LeBras","Yejin Choi"],"categories":["cs.CL"],"published":"20190422","updated":"20190909","primary_category":"cs.CL"}
{"arxiv_id":"2211.09110","title":"Holistic Evaluation of Language Models","abstract":"Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.","source":"http:\/\/arxiv.org\/pdf\/2211.09110","authors":["Percy Liang","Rishi Bommasani","Tony Lee","Dimitris Tsipras","Dilara Soylu","Michihiro Yasunaga","Yian Zhang","Deepak Narayanan","Yuhuai Wu","Ananya Kumar","Benjamin Newman","Binhang Yuan","Bobby Yan","Ce Zhang","Christian Cosgrove","Christopher D. Manning","Christopher R\u00e9","Diana Acosta-Navas","Drew A. Hudson","Eric Zelikman","Esin Durmus","Faisal Ladhak","Frieda Rong","Hongyu Ren","Huaxiu Yao","Jue Wang","Keshav Santhanam","Laurel Orr","Lucia Zheng","Mert Yuksekgonul","Mirac Suzgun","Nathan Kim","Neel Guha","Niladri Chatterji","Omar Khattab","Peter Henderson","Qian Huang","Ryan Chi","Sang Michael Xie","Shibani Santurkar","Surya Ganguli","Tatsunori Hashimoto","Thomas Icard","Tianyi Zhang","Vishrav Chaudhary","William Wang","Xuechen Li","Yifan Mai","Yuhui Zhang","Yuta Koreeda"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20221116","updated":"20231001","primary_category":"cs.CL"}
{"arxiv_id":"2112.09332","title":"WebGPT: Browser-assisted question-answering with human feedback","abstract":"We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.","source":"http:\/\/arxiv.org\/pdf\/2112.09332","authors":["Reiichiro Nakano","Jacob Hilton","Suchir Balaji","Jeff Wu","Long Ouyang","Christina Kim","Christopher Hesse","Shantanu Jain","Vineet Kosaraju","William Saunders","Xu Jiang","Karl Cobbe","Tyna Eloundou","Gretchen Krueger","Kevin Button","Matthew Knight","Benjamin Chess","John Schulman"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20211217","updated":"20220601","primary_category":"cs.CL"}
{"arxiv_id":"2306.05949","title":"Evaluating the Social Impact of Generative AI Systems in Systems and Society","abstract":"Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm. We are concurrently crafting an evaluation repository for the AI research community to contribute existing evaluations along the given categories. This version will be updated following a CRAFT session at ACM FAccT 2023.","source":"http:\/\/arxiv.org\/pdf\/2306.05949","authors":["Irene Solaiman","Zeerak Talat","William Agnew","Lama Ahmad","Dylan Baker","Su Lin Blodgett","Hal Daum\u00e9 III","Jesse Dodge","Ellie Evans","Sara Hooker","Yacine Jernite","Alexandra Sasha Luccioni","Alberto Lusoli","Margaret Mitchell","Jessica Newman","Marie-Therese Png","Andrew Strait","Apostol Vassilev"],"categories":["cs.CY","cs.AI"],"published":"20230609","updated":"20230612","primary_category":"cs.CY"}
{"arxiv_id":"2107.02137","title":"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation","abstract":"Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot\/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).","source":"http:\/\/arxiv.org\/pdf\/2107.02137","authors":["Yu Sun","Shuohuan Wang","Shikun Feng","Siyu Ding","Chao Pang","Junyuan Shang","Jiaxiang Liu","Xuyi Chen","Yanbin Zhao","Yuxiang Lu","Weixin Liu","Zhihua Wu","Weibao Gong","Jianzhong Liang","Zhizhou Shang","Peng Sun","Wei Liu","Xuan Ouyang","Dianhai Yu","Hao Tian","Hua Wu","Haifeng Wang"],"categories":["cs.CL"],"published":"20210705","updated":"20210705","primary_category":"cs.CL"}
{"arxiv_id":"2203.11171","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models","abstract":"Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).","source":"http:\/\/arxiv.org\/pdf\/2203.11171","authors":["Xuezhi Wang","Jason Wei","Dale Schuurmans","Quoc Le","Ed Chi","Sharan Narang","Aakanksha Chowdhery","Denny Zhou"],"categories":["cs.CL","cs.AI"],"published":"20220321","updated":"20230307","primary_category":"cs.CL"}
{"arxiv_id":"2210.03078","title":"Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering","abstract":"Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent. We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.","source":"http:\/\/arxiv.org\/pdf\/2210.03078","authors":["Jiacheng Liu","Skyler Hallinan","Ximing Lu","Pengfei He","Sean Welleck","Hannaneh Hajishirzi","Yejin Choi"],"categories":["cs.CL","cs.AI"],"published":"20221006","updated":"20221022","primary_category":"cs.CL"}
{"arxiv_id":"2001.08361","title":"Scaling Laws for Neural Language Models","abstract":"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model\/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.","source":"http:\/\/arxiv.org\/pdf\/2001.08361","authors":["Jared Kaplan","Sam McCandlish","Tom Henighan","Tom B. Brown","Benjamin Chess","Rewon Child","Scott Gray","Alec Radford","Jeffrey Wu","Dario Amodei"],"categories":["cs.LG","stat.ML"],"published":"20200123","updated":"20200123","primary_category":"cs.LG"}
{"arxiv_id":"2210.12283","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs","abstract":"The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.","source":"http:\/\/arxiv.org\/pdf\/2210.12283","authors":["Albert Q. Jiang","Sean Welleck","Jin Peng Zhou","Wenda Li","Jiacheng Liu","Mateja Jamnik","Timoth\u00e9e Lacroix","Yuhuai Wu","Guillaume Lample"],"categories":["cs.AI","cs.LG"],"published":"20221021","updated":"20230220","primary_category":"cs.AI"}
{"arxiv_id":"2203.08436","title":"Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search","abstract":"Abstractive summarization systems today produce fluent and relevant output, but often \"hallucinate\" statements not supported by the source text. We analyze the connection between hallucinations and training data, and find evidence that models hallucinate because they train on target summaries that are unsupported by the source. Based on our findings, we present PINOCCHIO, a new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations. Given the model states and outputs at a given step, PINOCCHIO detects likely model hallucinations based on various measures of attribution to the source text. PINOCCHIO backtracks to find more consistent output, and can opt to produce no summary at all when no consistent generation can be found. In experiments, we find that PINOCCHIO improves the consistency of generation (in terms of F1) by an average of~67% on two abstractive summarization datasets.","source":"http:\/\/arxiv.org\/pdf\/2203.08436","authors":["Daniel King","Zejiang Shen","Nishant Subramani","Daniel S. Weld","Iz Beltagy","Doug Downey"],"categories":["cs.CL"],"published":"20220316","updated":"20220316","primary_category":"cs.CL"}
{"arxiv_id":"1802.03426","title":"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction","abstract":"UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.","source":"http:\/\/arxiv.org\/pdf\/1802.03426","authors":["Leland McInnes","John Healy","James Melville"],"categories":["stat.ML","cs.CG","cs.LG"],"published":"20180209","updated":"20200918","primary_category":"cs.CG"}
{"arxiv_id":"1911.03064","title":"Reducing Sentiment Bias in Language Models via Counterfactual Evaluation","abstract":"Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper aims to quantify and reduce a particular type of bias exhibited by language models: bias in the sentiment of generated text. Given a conditioning context (e.g., a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g., country names, occupations, genders) in the conditioning context using a form of counterfactual evaluation. We quantify sentiment bias by adopting individual and group fairness metrics from the fair machine learning literature, and demonstrate that large-scale models trained on two different corpora (news articles, and Wikipedia) exhibit considerable levels of bias. We then propose embedding and sentiment prediction-derived regularization on the language model's latent representations. The regularizations improve fairness metrics while retaining comparable levels of perplexity and semantic similarity.","source":"http:\/\/arxiv.org\/pdf\/1911.03064","authors":["Po-Sen Huang","Huan Zhang","Ray Jiang","Robert Stanforth","Johannes Welbl","Jack Rae","Vishal Maini","Dani Yogatama","Pushmeet Kohli"],"categories":["cs.CL","cs.CY","cs.LG"],"published":"20191108","updated":"20201008","primary_category":"cs.CL"}
{"arxiv_id":"2203.07814","title":"Competition-Level Code Generation with AlphaCode","abstract":"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.","source":"http:\/\/arxiv.org\/pdf\/2203.07814","authors":["Yujia Li","David Choi","Junyoung Chung","Nate Kushman","Julian Schrittwieser","R\u00e9mi Leblond","Tom Eccles","James Keeling","Felix Gimeno","Agustin Dal Lago","Thomas Hubert","Peter Choy","Cyprien de Masson d'Autume","Igor Babuschkin","Xinyun Chen","Po-Sen Huang","Johannes Welbl","Sven Gowal","Alexey Cherepanov","James Molloy","Daniel J. Mankowitz","Esme Sutherland Robson","Pushmeet Kohli","Nando de Freitas","Koray Kavukcuoglu","Oriol Vinyals"],"categories":["cs.PL","cs.AI","cs.LG"],"published":"20220208","updated":"20220208","primary_category":"cs.PL"}
{"arxiv_id":"2105.02732","title":"What's in the Box? A Preliminary Analysis of Undesirable Content in the Common Crawl Corpus","abstract":"Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis.","source":"http:\/\/arxiv.org\/pdf\/2105.02732","authors":["Alexandra Sasha Luccioni","Joseph D. Viviano"],"categories":["cs.CL"],"published":"20210506","updated":"20210531","primary_category":"cs.CL"}
{"arxiv_id":"1606.08415","title":"Gaussian Error Linear Units (GELUs)","abstract":"We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.","source":"http:\/\/arxiv.org\/pdf\/1606.08415","authors":["Dan Hendrycks","Kevin Gimpel"],"categories":["cs.LG"],"published":"20160627","updated":"20230606","primary_category":"cs.LG"}
{"arxiv_id":"1703.04009","title":"Automated Hate Speech Detection and the Problem of Offensive Language","abstract":"A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.","source":"http:\/\/arxiv.org\/pdf\/1703.04009","authors":["Thomas Davidson","Dana Warmsley","Michael Macy","Ingmar Weber"],"categories":["cs.CL"],"published":"20170311","updated":"20170311","primary_category":"cs.CL"}
{"arxiv_id":"1801.06146","title":"Universal Language Model Fine-tuning for Text Classification","abstract":"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.","source":"http:\/\/arxiv.org\/pdf\/1801.06146","authors":["Jeremy Howard","Sebastian Ruder"],"categories":["cs.CL","cs.LG","stat.ML"],"published":"20180118","updated":"20180523","primary_category":"cs.CL"}
{"arxiv_id":"1606.06031","title":"The LAMBADA dataset: Word prediction requiring a broad discourse context","abstract":"We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.","source":"http:\/\/arxiv.org\/pdf\/1606.06031","authors":["Denis Paperno","Germ\u00e1n Kruszewski","Angeliki Lazaridou","Quan Ngoc Pham","Raffaella Bernardi","Sandro Pezzelle","Marco Baroni","Gemma Boleda","Raquel Fern\u00e1ndez"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20160620","updated":"20160620","primary_category":"cs.CL"}
{"arxiv_id":"2111.00364","title":"Sustainable AI: Environmental Implications, Challenges and Opportunities","abstract":"This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.","source":"http:\/\/arxiv.org\/pdf\/2111.00364","authors":["Carole-Jean Wu","Ramya Raghavendra","Udit Gupta","Bilge Acun","Newsha Ardalani","Kiwan Maeng","Gloria Chang","Fiona Aga Behram","James Huang","Charles Bai","Michael Gschwind","Anurag Gupta","Myle Ott","Anastasia Melnikov","Salvatore Candido","David Brooks","Geeta Chauhan","Benjamin Lee","Hsien-Hsin S. Lee","Bugra Akyildiz","Maximilian Balandat","Joe Spisak","Ravi Jain","Mike Rabbat","Kim Hazelwood"],"categories":["cs.LG","cs.AI","cs.AR"],"published":"20211030","updated":"20220109","primary_category":"cs.LG"}
{"arxiv_id":"1906.06669","title":"One Epoch Is All You Need","abstract":"In unsupervised learning, collecting more data is not always a costly process unlike the training. For example, it is not hard to enlarge the 40GB WebText used for training GPT-2 by modifying its sampling methodology considering how many webpages there are in the Internet. On the other hand, given that training on this dataset already costs tens of thousands of dollars, training on a larger dataset naively is not cost-wise feasible. In this paper, we suggest to train on a larger dataset for only one epoch unlike the current practice, in which the unsupervised models are trained for from tens to hundreds of epochs. Furthermore, we suggest to adjust the model size and the number of iterations to be performed appropriately. We show that the performance of Transformer language model becomes dramatically improved in this way, especially if the original number of epochs is greater. For example, by replacing the training for 10 epochs with the one epoch training, this translates to 1.9-3.3x speedup in wall-clock time in our settings and more if the original number of epochs is greater. Under one epoch training, no overfitting occurs, and regularization method does nothing but slows down the training. Also, the curve of test loss over iterations follows power-law extensively. We compare the wall-clock time of the training of models with different parameter budget under one epoch training, and we show that size\/iteration adjustment based on our proposed heuristics leads to 1-2.7x speedup in our cases. With the two methods combined, we achieve 3.3-5.1x speedup. Finally, we speculate various implications of one epoch training and size\/iteration adjustment. In particular, based on our analysis we believe that we can reduce the cost to train the state-of-the-art models as BERT and GPT-2 dramatically, maybe even by the factor of 10.","source":"http:\/\/arxiv.org\/pdf\/1906.06669","authors":["Aran Komatsuzaki"],"categories":["cs.LG","stat.ML"],"published":"20190616","updated":"20190616","primary_category":"cs.LG"}
{"arxiv_id":"2008.02754","title":"Discovering and Categorising Language Biases in Reddit","abstract":"We present a data-driven approach using word embeddings to discover and categorise language biases on the discussion platform Reddit. As spaces for isolated user communities, platforms such as Reddit are increasingly connected to issues of racism, sexism and other forms of discrimination. Hence, there is a need to monitor the language of these groups. One of the most promising AI approaches to trace linguistic biases in large textual datasets involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. Yet, previous studies require predefined sets of potential biases to study, e.g., whether gender is more or less associated with particular types of jobs. This makes these approaches unfit to deal with smaller and community-centric datasets such as those on Reddit, which contain smaller vocabularies and slang, as well as biases that may be particular to that community. This paper proposes a data-driven approach to automatically discover language biases encoded in the vocabulary of online discourse communities on Reddit. In our approach, protected attributes are connected to evaluative words found in the data, which are then categorised through a semantic analysis system. We verify the effectiveness of our method by comparing the biases we discover in the Google News dataset with those found in previous literature. We then successfully discover gender bias, religion bias, and ethnic bias in different Reddit communities. We conclude by discussing potential application scenarios and limitations of this data-driven bias discovery method.","source":"http:\/\/arxiv.org\/pdf\/2008.02754","authors":["Xavier Ferrer","Tom van Nuenen","Jose M. Such","Natalia Criado"],"categories":["cs.CL","cs.AI","cs.CY","cs.LG","cs.SI","68T50, 68T09, 91D30"],"published":"20200806","updated":"20200813","primary_category":"cs.CL"}
{"arxiv_id":"2209.07686","title":"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango","abstract":"The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.","source":"http:\/\/arxiv.org\/pdf\/2209.07686","authors":["Aman Madaan","Amir Yazdanbakhsh"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20220916","updated":"20221013","primary_category":"cs.CL"}
{"arxiv_id":"2210.03629","title":"ReAct: Synergizing Reasoning and Acting in Language Models","abstract":"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https:\/\/react-lm.github.io","source":"http:\/\/arxiv.org\/pdf\/2210.03629","authors":["Shunyu Yao","Jeffrey Zhao","Dian Yu","Nan Du","Izhak Shafran","Karthik Narasimhan","Yuan Cao"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20221006","updated":"20230310","primary_category":"cs.CL"}
{"arxiv_id":"2009.14395","title":"Can Automatic Post-Editing Improve NMT?","abstract":"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https:\/\/github.com\/shamilcm\/pedra.","source":"http:\/\/arxiv.org\/pdf\/2009.14395","authors":["Shamil Chollampatt","Raymond Hendy Susanto","Liling Tan","Ewa Szymanska"],"categories":["cs.CL"],"published":"20200930","updated":"20200930","primary_category":"cs.CL"}
{"arxiv_id":"2112.02721","title":"NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation","abstract":"Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Python-based natural language augmentation framework which supports the creation of both transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of natural language tasks. We demonstrate the efficacy of NL-Augmenter by using several of its transformations to analyze the robustness of popular natural language models. The infrastructure, datacards and robustness analysis results are available publicly on the NL-Augmenter repository (https:\/\/github.com\/GEM-benchmark\/NL-Augmenter).","source":"http:\/\/arxiv.org\/pdf\/2112.02721","authors":["Kaustubh D. Dhole","Varun Gangal","Sebastian Gehrmann","Aadesh Gupta","Zhenhao Li","Saad Mahamood","Abinaya Mahendiran","Simon Mille","Ashish Shrivastava","Samson Tan","Tongshuang Wu","Jascha Sohl-Dickstein","Jinho D. Choi","Eduard Hovy","Ondrej Dusek","Sebastian Ruder","Sajant Anand","Nagender Aneja","Rabin Banjade","Lisa Barthe","Hanna Behnke","Ian Berlot-Attwell","Connor Boyle","Caroline Brun","Marco Antonio Sobrevilla Cabezudo","Samuel Cahyawijaya","Emile Chapuis","Wanxiang Che","Mukund Choudhary","Christian Clauss","Pierre Colombo","Filip Cornell","Gautier Dagan","Mayukh Das","Tanay Dixit","Thomas Dopierre","Paul-Alexis Dray","Suchitra Dubey","Tatiana Ekeinhor","Marco Di Giovanni","Tanya Goyal","Rishabh Gupta","Rishabh Gupta","Louanes Hamla","Sang Han","Fabrice Harel-Canada","Antoine Honore","Ishan Jindal","Przemyslaw K. Joniak","Denis Kleyko","Venelin Kovatchev","Kalpesh Krishna","Ashutosh Kumar","Stefan Langer","Seungjae Ryan Lee","Corey James Levinson","Hualou Liang","Kaizhao Liang","Zhexiong Liu","Andrey Lukyanenko","Vukosi Marivate","Gerard de Melo","Simon Meoni","Maxime Meyer","Afnan Mir","Nafise Sadat Moosavi","Niklas Muennighoff","Timothy Sum Hon Mun","Kenton Murray","Marcin Namysl","Maria Obedkova","Priti Oli","Nivranshu Pasricha","Jan Pfister","Richard Plant","Vinay Prabhu","Vasile Pais","Libo Qin","Shahab Raji","Pawan Kumar Rajpoot","Vikas Raunak","Roy Rinberg","Nicolas Roberts","Juan Diego Rodriguez","Claude Roux","Vasconcellos P. H. S.","Ananya B. Sai","Robin M. Schmidt","Thomas Scialom","Tshephisho Sefara","Saqib N. Shamsi","Xudong Shen","Haoyue Shi","Yiwen Shi","Anna Shvets","Nick Siegel","Damien Sileo","Jamie Simon","Chandan Singh","Roman Sitelew","Priyank Soni","Taylor Sorensen","William Soto","Aman Srivastava","KV Aditya Srivatsa","Tony Sun","Mukund Varma T","A Tabassum","Fiona Anting Tan","Ryan Teehan","Mo Tiwari","Marie Tolkiehn","Athena Wang","Zijian Wang","Gloria Wang","Zijie J. Wang","Fuxuan Wei","Bryan Wilie","Genta Indra Winata","Xinyi Wu","Witold Wydma\u0144ski","Tianbao Xie","Usama Yaseen","Michael A. Yee","Jing Zhang","Yue Zhang"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20211206","updated":"20221011","primary_category":"cs.CL"}
{"arxiv_id":"2104.07838","title":"Investigating Failures of Automatic Translation in the Case of Unambiguous Gender","abstract":"Transformer based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by transformer based models with regards to translating from a language that doesn't mark gender on nouns into others that do. We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no transformer based model we tested was able to accurately gender occupation nouns systematically. We release an evaluation scheme and dataset for measuring the ability of transformer based NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences. Our dataset translates from an English source into 20 languages from several different language families. With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.","source":"http:\/\/arxiv.org\/pdf\/2104.07838","authors":["Adithya Renduchintala","Adina Williams"],"categories":["cs.CL"],"published":"20210416","updated":"20210416","primary_category":"cs.CL"}
{"arxiv_id":"1510.00149","title":"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding","abstract":"Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.","source":"http:\/\/arxiv.org\/pdf\/1510.00149","authors":["Song Han","Huizi Mao","William J. Dally"],"categories":["cs.CV","cs.NE"],"published":"20151001","updated":"20160215","primary_category":"cs.CV"}
{"arxiv_id":"2004.14224","title":"Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning","abstract":"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective which is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmark datasets, including question answering and knowledge base completion tasks.","source":"http:\/\/arxiv.org\/pdf\/2004.14224","authors":["Tao Shen","Yi Mao","Pengcheng He","Guodong Long","Adam Trischler","Weizhu Chen"],"categories":["cs.CL","cs.LG"],"published":"20200429","updated":"20200429","primary_category":"cs.CL"}
{"arxiv_id":"1911.03842","title":"Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation","abstract":"Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.","source":"http:\/\/arxiv.org\/pdf\/1911.03842","authors":["Emily Dinan","Angela Fan","Adina Williams","Jack Urbanek","Douwe Kiela","Jason Weston"],"categories":["cs.CL"],"published":"20191110","updated":"20200416","primary_category":"cs.CL"}
{"arxiv_id":"1901.02860","title":"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context","abstract":"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc\/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.","source":"http:\/\/arxiv.org\/pdf\/1901.02860","authors":["Zihang Dai","Zhilin Yang","Yiming Yang","Jaime Carbonell","Quoc V. Le","Ruslan Salakhutdinov"],"categories":["cs.LG","cs.CL","stat.ML"],"published":"20190109","updated":"20190602","primary_category":"cs.LG"}
{"arxiv_id":"2203.04806","title":"One-Shot Learning from a Demonstration with Hierarchical Latent Language","abstract":"Humans have the capability, aided by the expressive compositionality of their language, to learn quickly by demonstration. They are able to describe unseen task-performing procedures and generalize their execution to other contexts. In this work, we introduce DescribeWorld, an environment designed to test this sort of generalization skill in grounded agents, where tasks are linguistically and procedurally composed of elementary concepts. The agent observes a single task demonstration in a Minecraft-like grid world, and is then asked to carry out the same task in a new map. To enable such a level of generalization, we propose a neural agent infused with hierarchical latent language--both at the level of task inference and subtask planning. Our agent first generates a textual description of the demonstrated unseen task, then leverages this description to replicate it. Through multiple evaluation scenarios and a suite of generalization tests, we find that agents that perform text-based inference are better equipped for the challenge under a random split of tasks.","source":"http:\/\/arxiv.org\/pdf\/2203.04806","authors":["Nathaniel Weir","Xingdi Yuan","Marc-Alexandre C\u00f4t\u00e9","Matthew Hausknecht","Romain Laroche","Ida Momennejad","Harm Van Seijen","Benjamin Van Durme"],"categories":["cs.CL"],"published":"20220309","updated":"20220309","primary_category":"cs.CL"}
{"arxiv_id":"2301.12867","title":"Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity","abstract":"Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT\\footnote{In this paper, ChatGPT refers to the version released on Dec 15th.} to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) \\textit{Bias} 2) \\textit{Reliability} 3) \\textit{Robustness} 4) \\textit{Toxicity}. In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our findings on AI ethics and harmal behaviors of ChatGPT, as well as future problems and practical design considerations for responsible LLMs. We believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.","source":"http:\/\/arxiv.org\/pdf\/2301.12867","authors":["Terry Yue Zhuo","Yujin Huang","Chunyang Chen","Zhenchang Xing"],"categories":["cs.CL","cs.SE"],"published":"20230130","updated":"20230529","primary_category":"cs.CL"}
{"arxiv_id":"1912.06872","title":"Towards Robust Toxic Content Classification","abstract":"Toxic content detection aims to identify content that can offend or harm its recipients. Automated classifiers of toxic content need to be robust against adversaries who deliberately try to bypass filters. We propose a method of generating realistic model-agnostic attacks using a lexicon of toxic tokens, which attempts to mislead toxicity classifiers by diluting the toxicity signal either by obfuscating toxic tokens through character-level perturbations, or by injecting non-toxic distractor tokens. We show that these realistic attacks reduce the detection recall of state-of-the-art neural toxicity detectors, including those using ELMo and BERT, by more than 50% in some cases. We explore two approaches for defending against such attacks. First, we examine the effect of training on synthetically noised data. Second, we propose the Contextual Denoising Autoencoder (CDAE): a method for learning robust representations that uses character-level and contextual information to denoise perturbed tokens. We show that the two approaches are complementary, improving robustness to both character-level perturbations and distractors, recovering a considerable portion of the lost accuracy. Finally, we analyze the robustness characteristics of the most competitive methods and outline practical considerations for improving toxicity detectors.","source":"http:\/\/arxiv.org\/pdf\/1912.06872","authors":["Keita Kurita","Anna Belova","Antonios Anastasopoulos"],"categories":["cs.CL"],"published":"20191214","updated":"20191214","primary_category":"cs.CL"}
{"arxiv_id":"2108.07258","title":"On the Opportunities and Risks of Foundation Models","abstract":"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.","source":"http:\/\/arxiv.org\/pdf\/2108.07258","authors":["Rishi Bommasani","Drew A. Hudson","Ehsan Adeli","Russ Altman","Simran Arora","Sydney von Arx","Michael S. Bernstein","Jeannette Bohg","Antoine Bosselut","Emma Brunskill","Erik Brynjolfsson","Shyamal Buch","Dallas Card","Rodrigo Castellon","Niladri Chatterji","Annie Chen","Kathleen Creel","Jared Quincy Davis","Dora Demszky","Chris Donahue","Moussa Doumbouya","Esin Durmus","Stefano Ermon","John Etchemendy","Kawin Ethayarajh","Li Fei-Fei","Chelsea Finn","Trevor Gale","Lauren Gillespie","Karan Goel","Noah Goodman","Shelby Grossman","Neel Guha","Tatsunori Hashimoto","Peter Henderson","John Hewitt","Daniel E. Ho","Jenny Hong","Kyle Hsu","Jing Huang","Thomas Icard","Saahil Jain","Dan Jurafsky","Pratyusha Kalluri","Siddharth Karamcheti","Geoff Keeling","Fereshte Khani","Omar Khattab","Pang Wei Koh","Mark Krass","Ranjay Krishna","Rohith Kuditipudi","Ananya Kumar","Faisal Ladhak","Mina Lee","Tony Lee","Jure Leskovec","Isabelle Levent","Xiang Lisa Li","Xuechen Li","Tengyu Ma","Ali Malik","Christopher D. Manning","Suvir Mirchandani","Eric Mitchell","Zanele Munyikwa","Suraj Nair","Avanika Narayan","Deepak Narayanan","Ben Newman","Allen Nie","Juan Carlos Niebles","Hamed Nilforoshan","Julian Nyarko","Giray Ogut","Laurel Orr","Isabel Papadimitriou","Joon Sung Park","Chris Piech","Eva Portelance","Christopher Potts","Aditi Raghunathan","Rob Reich","Hongyu Ren","Frieda Rong","Yusuf Roohani","Camilo Ruiz","Jack Ryan","Christopher R\u00e9","Dorsa Sadigh","Shiori Sagawa","Keshav Santhanam","Andy Shih","Krishnan Srinivasan","Alex Tamkin","Rohan Taori","Armin W. Thomas","Florian Tram\u00e8r","Rose E. Wang","William Wang","Bohan Wu","Jiajun Wu","Yuhuai Wu","Sang Michael Xie","Michihiro Yasunaga","Jiaxuan You","Matei Zaharia","Michael Zhang","Tianyi Zhang","Xikun Zhang","Yuhui Zhang","Lucia Zheng","Kaitlyn Zhou","Percy Liang"],"categories":["cs.LG","cs.AI","cs.CY"],"published":"20210816","updated":"20220712","primary_category":"cs.LG"}
{"arxiv_id":"1809.02922","title":"Transforming Question Answering Datasets Into Natural Language Inference Datasets","abstract":"Existing datasets for natural language inference (NLI) have propelled research on language understanding. We propose a new method for automatically deriving NLI datasets from the growing abundance of large-scale question answering datasets. Our approach hinges on learning a sentence transformation model which converts question-answer pairs into their declarative forms. Despite being primarily trained on a single QA dataset, we show that it can be successfully applied to a variety of other QA resources. Using this system, we automatically derive a new freely available dataset of over 500k NLI examples (QA-NLI), and show that it exhibits a wide range of inference phenomena rarely seen in previous NLI datasets.","source":"http:\/\/arxiv.org\/pdf\/1809.02922","authors":["Dorottya Demszky","Kelvin Guu","Percy Liang"],"categories":["cs.CL"],"published":"20180909","updated":"20180911","primary_category":"cs.CL"}
{"arxiv_id":"2205.01068","title":"OPT: Open Pre-trained Transformer Language Models","abstract":"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1\/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.","source":"http:\/\/arxiv.org\/pdf\/2205.01068","authors":["Susan Zhang","Stephen Roller","Naman Goyal","Mikel Artetxe","Moya Chen","Shuohui Chen","Christopher Dewan","Mona Diab","Xian Li","Xi Victoria Lin","Todor Mihaylov","Myle Ott","Sam Shleifer","Kurt Shuster","Daniel Simig","Punit Singh Koura","Anjali Sridhar","Tianlu Wang","Luke Zettlemoyer"],"categories":["cs.CL","cs.LG"],"published":"20220502","updated":"20220621","primary_category":"cs.CL"}
{"arxiv_id":"2011.10208","title":"Collaborative Storytelling with Large-scale Neural Language Models","abstract":"Storytelling plays a central role in human socializing and entertainment. However, much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction. In this paper, we introduce the task of collaborative storytelling, where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it. We present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far. We constructed the storytelling system by tuning a publicly-available large scale language model on a dataset of writing prompts and their accompanying fictional works. We identify generating sufficiently human-like utterances to be an important technical issue and propose a sample-and-rank approach to improve utterance quality. Quantitative evaluation shows that our approach outperforms a baseline, and we present qualitative evaluation of our system's capabilities.","source":"http:\/\/arxiv.org\/pdf\/2011.10208","authors":["Eric Nichols","Leo Gao","Randy Gomez"],"categories":["cs.CL","cs.AI","cs.LG","cs.NE"],"published":"20201120","updated":"20201120","primary_category":"cs.CL"}
{"arxiv_id":"2006.03955","title":"Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases","abstract":"With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that appear in particular contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD and EIBD achieve high accuracy when detecting the intersectional and emergent biases of African American females and Mexican American females. Our results indicate that biases at the intersection of race and gender associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.","source":"http:\/\/arxiv.org\/pdf\/2006.03955","authors":["Wei Guo","Aylin Caliskan"],"categories":["cs.CY","cs.AI","cs.CL"],"published":"20200606","updated":"20210519","primary_category":"cs.CY"}
{"arxiv_id":"1604.04562","title":"A Network-based End-to-End Trainable Task-oriented Dialogue System","abstract":"Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.","source":"http:\/\/arxiv.org\/pdf\/1604.04562","authors":["Tsung-Hsien Wen","David Vandyke","Nikola Mrksic","Milica Gasic","Lina M. Rojas-Barahona","Pei-Hao Su","Stefan Ultes","Steve Young"],"categories":["cs.CL","cs.AI","cs.NE","stat.ML"],"published":"20160415","updated":"20170424","primary_category":"cs.CL"}
{"arxiv_id":"2204.02311","title":"PaLM: Scaling Language Modeling with Pathways","abstract":"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.","source":"http:\/\/arxiv.org\/pdf\/2204.02311","authors":["Aakanksha Chowdhery","Sharan Narang","Jacob Devlin","Maarten Bosma","Gaurav Mishra","Adam Roberts","Paul Barham","Hyung Won Chung","Charles Sutton","Sebastian Gehrmann","Parker Schuh","Kensen Shi","Sasha Tsvyashchenko","Joshua Maynez","Abhishek Rao","Parker Barnes","Yi Tay","Noam Shazeer","Vinodkumar Prabhakaran","Emily Reif","Nan Du","Ben Hutchinson","Reiner Pope","James Bradbury","Jacob Austin","Michael Isard","Guy Gur-Ari","Pengcheng Yin","Toju Duke","Anselm Levskaya","Sanjay Ghemawat","Sunipa Dev","Henryk Michalewski","Xavier Garcia","Vedant Misra","Kevin Robinson","Liam Fedus","Denny Zhou","Daphne Ippolito","David Luan","Hyeontaek Lim","Barret Zoph","Alexander Spiridonov","Ryan Sepassi","David Dohan","Shivani Agrawal","Mark Omernick","Andrew M. Dai","Thanumalayan Sankaranarayana Pillai","Marie Pellat","Aitor Lewkowycz","Erica Moreira","Rewon Child","Oleksandr Polozov","Katherine Lee","Zongwei Zhou","Xuezhi Wang","Brennan Saeta","Mark Diaz","Orhan Firat","Michele Catasta","Jason Wei","Kathy Meier-Hellstern","Douglas Eck","Jeff Dean","Slav Petrov","Noah Fiedel"],"categories":["cs.CL"],"published":"20220405","updated":"20221005","primary_category":"cs.CL"}
{"arxiv_id":"2206.05802","title":"Self-critiquing models for assisting human evaluators","abstract":"We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.","source":"http:\/\/arxiv.org\/pdf\/2206.05802","authors":["William Saunders","Catherine Yeh","Jeff Wu","Steven Bills","Long Ouyang","Jonathan Ward","Jan Leike"],"categories":["cs.CL","cs.LG"],"published":"20220612","updated":"20220614","primary_category":"cs.CL"}
{"arxiv_id":"2301.13688","title":"The Flan Collection: Designing Data and Methods for Effective Instruction Tuning","abstract":"We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https:\/\/github.com\/google-research\/FLAN\/tree\/main\/flan\/v2.","source":"http:\/\/arxiv.org\/pdf\/2301.13688","authors":["Shayne Longpre","Le Hou","Tu Vu","Albert Webson","Hyung Won Chung","Yi Tay","Denny Zhou","Quoc V. Le","Barret Zoph","Jason Wei","Adam Roberts"],"categories":["cs.AI","cs.CL","cs.LG"],"published":"20230131","updated":"20230214","primary_category":"cs.AI"}
{"arxiv_id":"2210.07316","title":"MTEB: Massive Text Embedding Benchmark","abstract":"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https:\/\/github.com\/embeddings-benchmark\/mteb.","source":"http:\/\/arxiv.org\/pdf\/2210.07316","authors":["Niklas Muennighoff","Nouamane Tazi","Lo\u00efc Magne","Nils Reimers"],"categories":["cs.CL","cs.IR","cs.LG"],"published":"20221013","updated":"20230319","primary_category":"cs.CL"}
{"arxiv_id":"1702.08608","title":"Towards A Rigorous Science of Interpretable Machine Learning","abstract":"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","source":"http:\/\/arxiv.org\/pdf\/1702.08608","authors":["Finale Doshi-Velez","Been Kim"],"categories":["stat.ML","cs.AI","cs.LG"],"published":"20170228","updated":"20170302","primary_category":"cs.AI"}
{"arxiv_id":"1802.07569","title":"Continual Lifelong Learning with Neural Networks: A Review","abstract":"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.","source":"http:\/\/arxiv.org\/pdf\/1802.07569","authors":["German I. Parisi","Ronald Kemker","Jose L. Part","Christopher Kanan","Stefan Wermter"],"categories":["cs.LG","q-bio.NC","stat.ML"],"published":"20180221","updated":"20190211","primary_category":"cs.LG"}
{"arxiv_id":"1811.11206","title":"Partitioned Variational Inference: A unified framework encompassing federated and continual learning","abstract":"Variational inference (VI) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference (PVI), that explicitly acknowledges these algorithmic dimensions of VI, unifies disparate literature, and provides guidance on usage. Crucially, the proposed PVI framework allows us to identify new ways of performing VI that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard VI.","source":"http:\/\/arxiv.org\/pdf\/1811.11206","authors":["Thang D. Bui","Cuong V. Nguyen","Siddharth Swaroop","Richard E. Turner"],"categories":["stat.ML","cs.AI","cs.LG"],"published":"20181127","updated":"20181127","primary_category":"cs.AI"}
{"arxiv_id":"2201.11903","title":"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models","abstract":"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.","source":"http:\/\/arxiv.org\/pdf\/2201.11903","authors":["Jason Wei","Xuezhi Wang","Dale Schuurmans","Maarten Bosma","Brian Ichter","Fei Xia","Ed Chi","Quoc Le","Denny Zhou"],"categories":["cs.CL","cs.AI"],"published":"20220128","updated":"20230110","primary_category":"cs.CL"}
{"arxiv_id":"1709.02349","title":"A Deep Reinforcement Learning Chatbot","abstract":"We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A\/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.","source":"http:\/\/arxiv.org\/pdf\/1709.02349","authors":["Iulian V. Serban","Chinnadhurai Sankar","Mathieu Germain","Saizheng Zhang","Zhouhan Lin","Sandeep Subramanian","Taesup Kim","Michael Pieper","Sarath Chandar","Nan Rosemary Ke","Sai Rajeshwar","Alexandre de Brebisson","Jose M. R. Sotelo","Dendi Suhubdy","Vincent Michalski","Alexandre Nguyen","Joelle Pineau","Yoshua Bengio"],"categories":["cs.CL","cs.AI","cs.LG","cs.NE","stat.ML","I.5.1; I.2.7"],"published":"20170907","updated":"20171105","primary_category":"cs.CL"}
{"arxiv_id":"2304.01373","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling","abstract":"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https:\/\/github.com\/EleutherAI\/pythia}.","source":"http:\/\/arxiv.org\/pdf\/2304.01373","authors":["Stella Biderman","Hailey Schoelkopf","Quentin Anthony","Herbie Bradley","Kyle O'Brien","Eric Hallahan","Mohammad Aflah Khan","Shivanshu Purohit","USVSN Sai Prashanth","Edward Raff","Aviya Skowron","Lintang Sutawika","Oskar van der Wal"],"categories":["cs.CL"],"published":"20230403","updated":"20230531","primary_category":"cs.CL"}
{"arxiv_id":"1707.07012","title":"Learning Transferable Architectures for Scalable Image Recognition","abstract":"Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.","source":"http:\/\/arxiv.org\/pdf\/1707.07012","authors":["Barret Zoph","Vijay Vasudevan","Jonathon Shlens","Quoc V. Le"],"categories":["cs.CV","cs.LG","stat.ML"],"published":"20170721","updated":"20180411","primary_category":"cs.CV"}
{"arxiv_id":"2109.07684","title":"Language Models are Few-shot Multilingual Learners","abstract":"General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.","source":"http:\/\/arxiv.org\/pdf\/2109.07684","authors":["Genta Indra Winata","Andrea Madotto","Zhaojiang Lin","Rosanne Liu","Jason Yosinski","Pascale Fung"],"categories":["cs.CL","cs.AI"],"published":"20210916","updated":"20210916","primary_category":"cs.CL"}
{"arxiv_id":"2011.01314","title":"Automatic Detection of Machine Generated Text: A Critical Survey","abstract":"Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.","source":"http:\/\/arxiv.org\/pdf\/2011.01314","authors":["Ganesh Jawahar","Muhammad Abdul-Mageed","Laks V. S. Lakshmanan"],"categories":["cs.CL","cs.AI"],"published":"20201102","updated":"20201102","primary_category":"cs.CL"}
{"arxiv_id":"2303.17651","title":"Self-Refine: Iterative Refinement with Self-Feedback","abstract":"Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.","source":"http:\/\/arxiv.org\/pdf\/2303.17651","authors":["Aman Madaan","Niket Tandon","Prakhar Gupta","Skyler Hallinan","Luyu Gao","Sarah Wiegreffe","Uri Alon","Nouha Dziri","Shrimai Prabhumoye","Yiming Yang","Shashank Gupta","Bodhisattwa Prasad Majumder","Katherine Hermann","Sean Welleck","Amir Yazdanbakhsh","Peter Clark"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20230330","updated":"20230525","primary_category":"cs.CL"}
{"arxiv_id":"2203.13474","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis","abstract":"Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https:\/\/github.com\/salesforce\/CodeGen.","source":"http:\/\/arxiv.org\/pdf\/2203.13474","authors":["Erik Nijkamp","Bo Pang","Hiroaki Hayashi","Lifu Tu","Huan Wang","Yingbo Zhou","Silvio Savarese","Caiming Xiong"],"categories":["cs.LG","cs.CL","cs.PL"],"published":"20220325","updated":"20230227","primary_category":"cs.LG"}
{"arxiv_id":"1811.03600","title":"Measuring the Effects of Data Parallelism on Neural Network Training","abstract":"Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.","source":"http:\/\/arxiv.org\/pdf\/1811.03600","authors":["Christopher J. Shallue","Jaehoon Lee","Joseph Antognini","Jascha Sohl-Dickstein","Roy Frostig","George E. Dahl"],"categories":["cs.LG","stat.ML"],"published":"20181108","updated":"20190719","primary_category":"cs.LG"}
{"arxiv_id":"1802.03268","title":"Efficient Neural Architecture Search via Parameter Sharing","abstract":"We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.","source":"http:\/\/arxiv.org\/pdf\/1802.03268","authors":["Hieu Pham","Melody Y. Guan","Barret Zoph","Quoc V. Le","Jeff Dean"],"categories":["cs.LG","cs.CL","cs.CV","cs.NE","stat.ML"],"published":"20180209","updated":"20180212","primary_category":"cs.LG"}
{"arxiv_id":"2209.03143","title":"AudioLM: a Language Modeling Approach to Audio Generation","abstract":"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.","source":"http:\/\/arxiv.org\/pdf\/2209.03143","authors":["Zal\u00e1n Borsos","Rapha\u00ebl Marinier","Damien Vincent","Eugene Kharitonov","Olivier Pietquin","Matt Sharifi","Dominik Roblek","Olivier Teboul","David Grangier","Marco Tagliasacchi","Neil Zeghidour"],"categories":["cs.SD","cs.LG","eess.AS"],"published":"20220907","updated":"20230726","primary_category":"cs.SD"}
{"arxiv_id":"2008.02637","title":"Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets","abstract":"Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 60-70% of test-time answers are also present somewhere in the training sets. We also find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can actually generalize, and what drives their overall performance. We find that all models perform dramatically worse on questions that cannot be memorized from training sets, with a mean absolute performance difference of 63% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models out-perform a BART closed-book QA model, further highlighting the role that training set memorization plays in these benchmarks","source":"http:\/\/arxiv.org\/pdf\/2008.02637","authors":["Patrick Lewis","Pontus Stenetorp","Sebastian Riedel"],"categories":["cs.CL","cs.AI"],"published":"20200806","updated":"20200806","primary_category":"cs.CL"}
{"arxiv_id":"2208.02294","title":"Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning","abstract":"Despite recent advances in natural language understanding and generation, and decades of research on the development of conversational bots, building automated agents that can carry on rich open-ended conversations with humans \"in the wild\" remains a formidable challenge. In this work we develop a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot's conversational skill at scale. Our work pairs the succinct embedding of the conversation state generated using SOTA (supervised) language models with RL techniques that are particularly suited to a dynamic action space that changes as the conversation progresses. Trained using crowd-sourced data, our novel system is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant.","source":"http:\/\/arxiv.org\/pdf\/2208.02294","authors":["Deborah Cohen","Moonkyung Ryu","Yinlam Chow","Orgad Keller","Ido Greenberg","Avinatan Hassidim","Michael Fink","Yossi Matias","Idan Szpektor","Craig Boutilier","Gal Elidan"],"categories":["cs.CL","cs.LG"],"published":"20220725","updated":"20220725","primary_category":"cs.CL"}
{"arxiv_id":"2104.08758","title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus","abstract":"Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.","source":"http:\/\/arxiv.org\/pdf\/2104.08758","authors":["Jesse Dodge","Maarten Sap","Ana Marasovi\u0107","William Agnew","Gabriel Ilharco","Dirk Groeneveld","Margaret Mitchell","Matt Gardner"],"categories":["cs.CL","cs.AI"],"published":"20210418","updated":"20210930","primary_category":"cs.CL"}
{"arxiv_id":"2302.04166","title":"GPTScore: Evaluate as You Desire","abstract":"Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation--how to achieve customized, multi-faceted evaluation without the need for annotated samples. We make our code publicly available at https:\/\/github.com\/jinlanfu\/GPTScore.","source":"http:\/\/arxiv.org\/pdf\/2302.04166","authors":["Jinlan Fu","See-Kiong Ng","Zhengbao Jiang","Pengfei Liu"],"categories":["cs.CL"],"published":"20230208","updated":"20230213","primary_category":"cs.CL"}
{"arxiv_id":"2103.01498","title":"A Survey On Universal Adversarial Attack","abstract":"The intriguing phenomenon of adversarial examples has attracted significant attention in machine learning and what might be more surprising to the community is the existence of universal adversarial perturbations (UAPs), i.e. a single perturbation to fool the target DNN for most images. With the focus on UAP against deep classifiers, this survey summarizes the recent progress on universal adversarial attacks, discussing the challenges from both the attack and defense sides, as well as the reason for the existence of UAP. We aim to extend this work as a dynamic survey that will regularly update its content to follow new works regarding UAP or universal attack in a wide range of domains, such as image, audio, video, text, etc. Relevant updates will be discussed at: https:\/\/bit.ly\/2SbQlLG. We welcome authors of future works in this field to contact us for including your new finding.","source":"http:\/\/arxiv.org\/pdf\/2103.01498","authors":["Chaoning Zhang","Philipp Benz","Chenguo Lin","Adil Karjauv","Jing Wu","In So Kweon"],"categories":["cs.LG","cs.CV"],"published":"20210302","updated":"20220104","primary_category":"cs.LG"}
{"arxiv_id":"2203.09509","title":"ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection","abstract":"Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset. Our code and data can be found at https:\/\/github.com\/microsoft\/ToxiGen.","source":"http:\/\/arxiv.org\/pdf\/2203.09509","authors":["Thomas Hartvigsen","Saadia Gabriel","Hamid Palangi","Maarten Sap","Dipankar Ray","Ece Kamar"],"categories":["cs.CL"],"published":"20220317","updated":"20220714","primary_category":"cs.CL"}
{"arxiv_id":"2104.08678","title":"Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation","abstract":"Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of adversarial attacks. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make question answering models more robust to human adversaries. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or re-labels them to improve quality. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation to show that our models are considerably more robust to new human-written adversarial examples: crowdworkers can fool our model only 8.8% of the time on average, compared to 17.6% for a model trained without synthetic data.","source":"http:\/\/arxiv.org\/pdf\/2104.08678","authors":["Max Bartolo","Tristan Thrush","Robin Jia","Sebastian Riedel","Pontus Stenetorp","Douwe Kiela"],"categories":["cs.CL","cs.LG"],"published":"20210418","updated":"20220315","primary_category":"cs.CL"}
{"arxiv_id":"1903.02134","title":"Negative Training for Neural Dialogue Response Generation","abstract":"Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. In this work, we propose a framework named \"Negative Training\" to minimize such behaviors. Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model. Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity.","source":"http:\/\/arxiv.org\/pdf\/1903.02134","authors":["Tianxing He","James Glass"],"categories":["cs.CL","cs.LG"],"published":"20190306","updated":"20200818","primary_category":"cs.CL"}
{"arxiv_id":"1909.08072","title":"Adversarial Attacks and Defenses in Images, Graphs and Text: A Review","abstract":"Deep neural networks (DNN) have achieved unprecedented success in numerous machine learning tasks in various domains. However, the existence of adversarial examples has raised concerns about applying deep learning to safety-critical applications. As a result, we have witnessed increasing interests in studying attack and defense mechanisms for DNN models on different data types, such as images, graphs and text. Thus, it is necessary to provide a systematic and comprehensive overview of the main threats of attacks and the success of corresponding countermeasures. In this survey, we review the state of the art algorithms for generating adversarial examples and the countermeasures against adversarial examples, for the three popular data types, i.e., images, graphs and text.","source":"http:\/\/arxiv.org\/pdf\/1909.08072","authors":["Han Xu","Yao Ma","Haochen Liu","Debayan Deb","Hui Liu","Jiliang Tang","Anil K. Jain"],"categories":["cs.LG","cs.CR","stat.ML"],"published":"20190917","updated":"20191009","primary_category":"cs.LG"}
{"arxiv_id":"2112.07867","title":"Interscript: A dataset for interactive learning of scripts through error feedback","abstract":"How can an end-user provide feedback if a deployed structured prediction model generates inconsistent output, ignoring the structural complexity of human language? This is an emerging topic with recent progress in synthetic or constrained settings, and the next big leap would require testing and tuning models in real-world settings. We present a new dataset, Interscript, containing user feedback on a deployed model that generates complex everyday tasks. Interscript contains 8,466 data points -- the input is a possibly erroneous script and a user feedback, and the output is a modified script. We posit two use-cases of \\ours that might significantly advance the state-of-the-art in interactive learning. The dataset is available at: https:\/\/github.com\/allenai\/interscript.","source":"http:\/\/arxiv.org\/pdf\/2112.07867","authors":["Niket Tandon","Aman Madaan","Peter Clark","Keisuke Sakaguchi","Yiming Yang"],"categories":["cs.AI"],"published":"20211215","updated":"20211216","primary_category":"cs.AI"}
{"arxiv_id":"2101.06060","title":"The Challenge of Value Alignment: from Fairer Algorithms to AI Safety","abstract":"This paper addresses the question of how to align AI systems with human values and situates it within a wider body of thought regarding technology and value. Far from existing in a vacuum, there has long been an interest in the ability of technology to 'lock-in' different value systems. There has also been considerable thought about how to align technologies with specific social values, including through participatory design-processes. In this paper we look more closely at the question of AI value alignment and suggest that the power and autonomy of AI systems gives rise to opportunities and challenges in the domain of value that have not been encountered before. Drawing important continuities between the work of the fairness, accountability, transparency and ethics community, and work being done by technical AI safety researchers, we suggest that more attention needs to be paid to the question of 'social value alignment' - that is, how to align AI systems with the plurality of values endorsed by groups of people, especially on the global level.","source":"http:\/\/arxiv.org\/pdf\/2101.06060","authors":["Iason Gabriel","Vafa Ghazavi"],"categories":["cs.CY"],"published":"20210115","updated":"20210118","primary_category":"cs.CY"}
{"arxiv_id":"2101.00027","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling","abstract":"Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.","source":"http:\/\/arxiv.org\/pdf\/2101.00027","authors":["Leo Gao","Stella Biderman","Sid Black","Laurence Golding","Travis Hoppe","Charles Foster","Jason Phang","Horace He","Anish Thite","Noa Nabeshima","Shawn Presser","Connor Leahy"],"categories":["cs.CL"],"published":"20201231","updated":"20201231","primary_category":"cs.CL"}
{"arxiv_id":"2107.08720","title":"Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech","abstract":"Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. Thus, some NLP studies have started addressing the task of counter narrative generation. Although such studies have made an effort to build hate speech \/ counter narrative (HS\/CN) datasets for neural generation, they fall short in reaching either high-quality and\/or high-quantity. In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and\/or post-edit. Our experiments comprised several loops including dynamic variations. Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection. To our knowledge, the resulting dataset is the only expert-based multi-target HS\/CN dataset available to the community.","source":"http:\/\/arxiv.org\/pdf\/2107.08720","authors":["Margherita Fanton","Helena Bonaldi","Serra Sinem Tekiroglu","Marco Guerini"],"categories":["cs.CL","cs.CY"],"published":"20210719","updated":"20210719","primary_category":"cs.CL"}
{"arxiv_id":"2209.15003","title":"Compositional Semantic Parsing with Large Language Models","abstract":"Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.","source":"http:\/\/arxiv.org\/pdf\/2209.15003","authors":["Andrew Drozdov","Nathanael Sch\u00e4rli","Ekin Aky\u00fcrek","Nathan Scales","Xinying Song","Xinyun Chen","Olivier Bousquet","Denny Zhou"],"categories":["cs.CL","cs.AI"],"published":"20220929","updated":"20220930","primary_category":"cs.CL"}
{"arxiv_id":"1904.09751","title":"The Curious Case of Neural Text Degeneration","abstract":"Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.","source":"http:\/\/arxiv.org\/pdf\/1904.09751","authors":["Ari Holtzman","Jan Buys","Li Du","Maxwell Forbes","Yejin Choi"],"categories":["cs.CL"],"published":"20190422","updated":"20200214","primary_category":"cs.CL"}
{"arxiv_id":"2202.01771","title":"Pre-Trained Language Models for Interactive Decision-Making","abstract":"Language model (LM) pre-training is useful in many language processing tasks. But can pre-trained LMs be further leveraged for more general machine learning problems? We propose an approach for using LMs to scaffold learning and generalization in general sequential decision-making problems. In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action. We demonstrate that this framework enables effective combinatorial generalization across different environments and supervisory modalities. We begin by assuming access to a set of expert demonstrations, and show that initializing policies with LMs and fine-tuning them via behavior cloning improves task completion rates by 43.6% in the VirtualHome environment. Next, we integrate an active data gathering procedure in which agents iteratively interact with the environment, relabel past \"failed\" experiences with new goals, and update their policies in a self-supervised loop. Active data gathering further improves combinatorial generalization, outperforming the best baseline by 25.1%. Finally, we explain these results by investigating three possible factors underlying the effectiveness of the LM-based policy. We find that sequential input representations (vs. fixed-dimensional feature vectors) and LM-based weight initialization are both important for generalization. Surprisingly, however, the format of the policy inputs encoding (e.g. as a natural language string vs. an arbitrary sequential encoding) has little influence. Together, these results suggest that language modeling induces representations that are useful for modeling not just language, but also goals and plans; these representations can aid learning and generalization even outside of language processing.","source":"http:\/\/arxiv.org\/pdf\/2202.01771","authors":["Shuang Li","Xavier Puig","Chris Paxton","Yilun Du","Clinton Wang","Linxi Fan","Tao Chen","De-An Huang","Ekin Aky\u00fcrek","Anima Anandkumar","Jacob Andreas","Igor Mordatch","Antonio Torralba","Yuke Zhu"],"categories":["cs.LG","cs.CL"],"published":"20220203","updated":"20221029","primary_category":"cs.LG"}
{"arxiv_id":"1812.06162","title":"An Empirical Model of Large-Batch Training","abstract":"In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.","source":"http:\/\/arxiv.org\/pdf\/1812.06162","authors":["Sam McCandlish","Jared Kaplan","Dario Amodei","OpenAI Dota Team"],"categories":["cs.LG","stat.ML"],"published":"20181214","updated":"20181214","primary_category":"cs.LG"}
{"arxiv_id":"2208.14271","title":"Faithful Reasoning Using Large Language Models","abstract":"Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.","source":"http:\/\/arxiv.org\/pdf\/2208.14271","authors":["Antonia Creswell","Murray Shanahan"],"categories":["cs.AI","cs.CL"],"published":"20220830","updated":"20220830","primary_category":"cs.AI"}
{"arxiv_id":"2201.07207","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents","abstract":"Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. \"make breakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https:\/\/huangwl18.github.io\/language-planner","source":"http:\/\/arxiv.org\/pdf\/2201.07207","authors":["Wenlong Huang","Pieter Abbeel","Deepak Pathak","Igor Mordatch"],"categories":["cs.LG","cs.AI","cs.CL","cs.CV","cs.RO"],"published":"20220118","updated":"20220308","primary_category":"cs.LG"}
{"arxiv_id":"1506.06724","title":"Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books","abstract":"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie\/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.","source":"http:\/\/arxiv.org\/pdf\/1506.06724","authors":["Yukun Zhu","Ryan Kiros","Richard Zemel","Ruslan Salakhutdinov","Raquel Urtasun","Antonio Torralba","Sanja Fidler"],"categories":["cs.CV","cs.CL"],"published":"20150622","updated":"20150622","primary_category":"cs.CV"}
{"arxiv_id":"1801.10198","title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.","source":"http:\/\/arxiv.org\/pdf\/1801.10198","authors":["Peter J. Liu","Mohammad Saleh","Etienne Pot","Ben Goodrich","Ryan Sepassi","Lukasz Kaiser","Noam Shazeer"],"categories":["cs.CL"],"published":"20180130","updated":"20180130","primary_category":"cs.CL"}
{"arxiv_id":"2206.14858","title":"Solving Quantitative Reasoning Problems with Language Models","abstract":"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.","source":"http:\/\/arxiv.org\/pdf\/2206.14858","authors":["Aitor Lewkowycz","Anders Andreassen","David Dohan","Ethan Dyer","Henryk Michalewski","Vinay Ramasesh","Ambrose Slone","Cem Anil","Imanol Schlag","Theo Gutman-Solo","Yuhuai Wu","Behnam Neyshabur","Guy Gur-Ari","Vedant Misra"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20220629","updated":"20220701","primary_category":"cs.CL"}
{"arxiv_id":"1905.00537","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems","abstract":"In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.","source":"http:\/\/arxiv.org\/pdf\/1905.00537","authors":["Alex Wang","Yada Pruksachatkun","Nikita Nangia","Amanpreet Singh","Julian Michael","Felix Hill","Omer Levy","Samuel R. Bowman"],"categories":["cs.CL","cs.AI"],"published":"20190502","updated":"20200213","primary_category":"cs.CL"}
{"arxiv_id":"2001.08435","title":"The Pushshift Reddit Dataset","abstract":"Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves. Reddit, the so called \"front page of the Internet,\" in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit's millions of subreddits, hundreds of millions of users, and hundreds of billions of comments are at the same time relatively accessible, but time consuming to collect and analyze systematically. In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift's Reddit dataset is updated in real-time, and includes historical data back to Reddit's inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregating, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection, cleaning, and storage phases of their projects.","source":"http:\/\/arxiv.org\/pdf\/2001.08435","authors":["Jason Baumgartner","Savvas Zannettou","Brian Keegan","Megan Squire","Jeremy Blackburn"],"categories":["cs.SI","cs.CY"],"published":"20200123","updated":"20200123","primary_category":"cs.SI"}
{"arxiv_id":"1910.09700","title":"Quantifying the Carbon Emissions of Machine Learning","abstract":"From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.","source":"http:\/\/arxiv.org\/pdf\/1910.09700","authors":["Alexandre Lacoste","Alexandra Luccioni","Victor Schmidt","Thomas Dandres"],"categories":["cs.CY","cs.LG"],"published":"20191021","updated":"20191104","primary_category":"cs.CY"}
{"arxiv_id":"2210.05359","title":"Mind's Eye: Grounded Language Model Reasoning through Simulation","abstract":"Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.","source":"http:\/\/arxiv.org\/pdf\/2210.05359","authors":["Ruibo Liu","Jason Wei","Shixiang Shane Gu","Te-Yen Wu","Soroush Vosoughi","Claire Cui","Denny Zhou","Andrew M. Dai"],"categories":["cs.CL","cs.AI"],"published":"20221011","updated":"20221011","primary_category":"cs.CL"}
{"arxiv_id":"1810.12885","title":"ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension","abstract":"We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http:\/\/nlp.jhu.edu\/record.","source":"http:\/\/arxiv.org\/pdf\/1810.12885","authors":["Sheng Zhang","Xiaodong Liu","Jingjing Liu","Jianfeng Gao","Kevin Duh","Benjamin Van Durme"],"categories":["cs.CL"],"published":"20181030","updated":"20181030","primary_category":"cs.CL"}
{"arxiv_id":"2109.03300","title":"Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models","abstract":"All AI models are susceptible to learning biases in data that they are trained on. For generative dialogue models, being trained on real human conversations containing unbalanced gender and race\/ethnicity references can lead to models that display learned biases, which we define here broadly as any measurable differences in the distributions of words or semantic content of conversations based on demographic groups. We measure the strength of such biases by producing artificial conversations between two copies of a dialogue model, conditioning one conversational partner to state a name commonly associated with a certain gender and\/or race\/ethnicity. We find that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender. We show that several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation, including on a downstream conversational task. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races\/ethnicities.","source":"http:\/\/arxiv.org\/pdf\/2109.03300","authors":["Eric Michael Smith","Adina Williams"],"categories":["cs.CL"],"published":"20210907","updated":"20210907","primary_category":"cs.CL"}
{"arxiv_id":"2002.05651","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning","abstract":"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","source":"http:\/\/arxiv.org\/pdf\/2002.05651","authors":["Peter Henderson","Jieru Hu","Joshua Romoff","Emma Brunskill","Dan Jurafsky","Joelle Pineau"],"categories":["cs.CY","cs.LG"],"published":"20200131","updated":"20221129","primary_category":"cs.CY"}
{"arxiv_id":"1610.02413","title":"Equality of Opportunity in Supervised Learning","abstract":"We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores.","source":"http:\/\/arxiv.org\/pdf\/1610.02413","authors":["Moritz Hardt","Eric Price","Nathan Srebro"],"categories":["cs.LG"],"published":"20161007","updated":"20161007","primary_category":"cs.LG"}
{"arxiv_id":"2105.12655","title":"CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks","abstract":"Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled numerous breakthroughs, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of AI for Code has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.","source":"http:\/\/arxiv.org\/pdf\/2105.12655","authors":["Ruchir Puri","David S. Kung","Geert Janssen","Wei Zhang","Giacomo Domeniconi","Vladimir Zolotov","Julian Dolby","Jie Chen","Mihir Choudhury","Lindsey Decker","Veronika Thost","Luca Buratti","Saurabh Pujar","Shyam Ramji","Ulrich Finkler","Susan Malaika","Frederick Reiss"],"categories":["cs.SE","cs.AI"],"published":"20210525","updated":"20210829","primary_category":"cs.SE"}
{"arxiv_id":"1906.00300","title":"Latent Retrieval for Weakly Supervised Open Domain Question Answering","abstract":"Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and\/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.","source":"http:\/\/arxiv.org\/pdf\/1906.00300","authors":["Kenton Lee","Ming-Wei Chang","Kristina Toutanova"],"categories":["cs.CL"],"published":"20190601","updated":"20190627","primary_category":"cs.CL"}
{"arxiv_id":"2010.07079","title":"Recipes for Safety in Open-domain Chatbots","abstract":"Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.","source":"http:\/\/arxiv.org\/pdf\/2010.07079","authors":["Jing Xu","Da Ju","Margaret Li","Y-Lan Boureau","Jason Weston","Emily Dinan"],"categories":["cs.CL","cs.AI"],"published":"20201014","updated":"20210804","primary_category":"cs.CL"}
{"arxiv_id":"2304.12298","title":"BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor Attacks to InstructGPT","abstract":"Recently, ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., InstructGPT. In this study, we propose BadGPT, the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated text through BadGPT.","source":"http:\/\/arxiv.org\/pdf\/2304.12298","authors":["Jiawen Shi","Yixin Liu","Pan Zhou","Lichao Sun"],"categories":["cs.CR","cs.AI"],"published":"20230221","updated":"20230221","primary_category":"cs.CR"}
{"arxiv_id":"2108.13349","title":"On the Multilingual Capabilities of Very Large-Scale English Language Models","abstract":"Generative Pre-trained Transformers (GPTs) have recently been scaled to unprecedented sizes in the history of machine learning. These models, solely trained on the language modeling objective, have been shown to exhibit outstanding few-shot learning capabilities in a number of different tasks. Nevertheless, aside from anecdotal experiences, little is known regarding their multilingual capabilities, given the fact that the pre-training corpus is almost entirely composed of English text. In this work, we investigate the multilingual skills of GPT-3, focusing on one language that barely appears in the pre-training corpus, Catalan, which makes the results especially meaningful; we assume that our results may be relevant for other languages as well. We find that the model shows an outstanding performance, particularly in generative tasks, with predictable limitations mostly in language understanding tasks but still with remarkable results given the zero-shot scenario. We investigate its potential and limits in extractive question-answering and natural language generation, as well as the effect of scale in terms of model size.","source":"http:\/\/arxiv.org\/pdf\/2108.13349","authors":["Jordi Armengol-Estap\u00e9","Ona de Gibert Bonet","Maite Melero"],"categories":["cs.CL","cs.AI"],"published":"20210830","updated":"20210830","primary_category":"cs.CL"}
{"arxiv_id":"2009.01325","title":"Learning to summarize from human feedback","abstract":"As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN\/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.","source":"http:\/\/arxiv.org\/pdf\/2009.01325","authors":["Nisan Stiennon","Long Ouyang","Jeff Wu","Daniel M. Ziegler","Ryan Lowe","Chelsea Voss","Alec Radford","Dario Amodei","Paul Christiano"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20200902","updated":"20220215","primary_category":"cs.CL"}
{"arxiv_id":"1901.07291","title":"Cross-lingual Language Model Pretraining","abstract":"Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.","source":"http:\/\/arxiv.org\/pdf\/1901.07291","authors":["Guillaume Lample","Alexis Conneau"],"categories":["cs.CL"],"published":"20190122","updated":"20190122","primary_category":"cs.CL"}
{"arxiv_id":"2203.15556","title":"Training Compute-Optimal Large Language Models","abstract":"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.","source":"http:\/\/arxiv.org\/pdf\/2203.15556","authors":["Jordan Hoffmann","Sebastian Borgeaud","Arthur Mensch","Elena Buchatskaya","Trevor Cai","Eliza Rutherford","Diego de Las Casas","Lisa Anne Hendricks","Johannes Welbl","Aidan Clark","Tom Hennigan","Eric Noland","Katie Millican","George van den Driessche","Bogdan Damoc","Aurelia Guy","Simon Osindero","Karen Simonyan","Erich Elsen","Jack W. Rae","Oriol Vinyals","Laurent Sifre"],"categories":["cs.CL","cs.LG"],"published":"20220329","updated":"20220329","primary_category":"cs.CL"}
{"arxiv_id":"2211.11736","title":"Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models","abstract":"In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.","source":"http:\/\/arxiv.org\/pdf\/2211.11736","authors":["Ted Xiao","Harris Chan","Pierre Sermanet","Ayzaan Wahid","Anthony Brohan","Karol Hausman","Sergey Levine","Jonathan Tompson"],"categories":["cs.RO","cs.AI","cs.LG"],"published":"20221121","updated":"20230701","primary_category":"cs.RO"}
{"arxiv_id":"2304.14178","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality","abstract":"Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https:\/\/github.com\/X-PLUG\/mPLUG-Owl. The online demo is available at https:\/\/www.modelscope.cn\/studios\/damo\/mPLUG-Owl.","source":"http:\/\/arxiv.org\/pdf\/2304.14178","authors":["Qinghao Ye","Haiyang Xu","Guohai Xu","Jiabo Ye","Ming Yan","Yiyang Zhou","Junyang Wang","Anwen Hu","Pengcheng Shi","Yaya Shi","Chenliang Li","Yuanhong Xu","Hehong Chen","Junfeng Tian","Qian Qi","Ji Zhang","Fei Huang"],"categories":["cs.CL","cs.CV","cs.LG"],"published":"20230427","updated":"20230427","primary_category":"cs.CL"}
{"arxiv_id":"2212.08073","title":"Constitutional AI: Harmlessness from AI Feedback","abstract":"As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.","source":"http:\/\/arxiv.org\/pdf\/2212.08073","authors":["Yuntao Bai","Saurav Kadavath","Sandipan Kundu","Amanda Askell","Jackson Kernion","Andy Jones","Anna Chen","Anna Goldie","Azalia Mirhoseini","Cameron McKinnon","Carol Chen","Catherine Olsson","Christopher Olah","Danny Hernandez","Dawn Drain","Deep Ganguli","Dustin Li","Eli Tran-Johnson","Ethan Perez","Jamie Kerr","Jared Mueller","Jeffrey Ladish","Joshua Landau","Kamal Ndousse","Kamile Lukosuite","Liane Lovitt","Michael Sellitto","Nelson Elhage","Nicholas Schiefer","Noemi Mercado","Nova DasSarma","Robert Lasenby","Robin Larson","Sam Ringer","Scott Johnston","Shauna Kravec","Sheer El Showk","Stanislav Fort","Tamera Lanham","Timothy Telleen-Lawton","Tom Conerly","Tom Henighan","Tristan Hume","Samuel R. Bowman","Zac Hatfield-Dodds","Ben Mann","Dario Amodei","Nicholas Joseph","Sam McCandlish","Tom Brown","Jared Kaplan"],"categories":["cs.CL","cs.AI"],"published":"20221215","updated":"20221215","primary_category":"cs.CL"}
{"arxiv_id":"2205.12688","title":"ProsocialDialog: A Prosocial Backbone for Conversational Agents","abstract":"Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue safety labels accompanied by free-form rationales. With this dataset, we introduce a dialogue safety detection module, Canary, capable of generating RoTs given conversational context, and a socially-informed dialogue agent, Prost. Empirical results show that Prost generates more socially acceptable dialogues compared to other state-of-the-art language and dialogue models in both in-domain and out-of-domain settings. Additionally, Canary effectively guides conversational agents and off-the-shelf language models to generate significantly more prosocial responses. Our work highlights the promise and importance of creating and steering conversational AI to be socially responsible.","source":"http:\/\/arxiv.org\/pdf\/2205.12688","authors":["Hyunwoo Kim","Youngjae Yu","Liwei Jiang","Ximing Lu","Daniel Khashabi","Gunhee Kim","Yejin Choi","Maarten Sap"],"categories":["cs.CL"],"published":"20220525","updated":"20221025","primary_category":"cs.CL"}
{"arxiv_id":"1707.02286","title":"Emergence of Locomotion Behaviours in Rich Environments","abstract":"The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https:\/\/youtu.be\/hx_bgoTF7bs .","source":"http:\/\/arxiv.org\/pdf\/1707.02286","authors":["Nicolas Heess","Dhruva TB","Srinivasan Sriram","Jay Lemmon","Josh Merel","Greg Wayne","Yuval Tassa","Tom Erez","Ziyu Wang","S. M. Ali Eslami","Martin Riedmiller","David Silver"],"categories":["cs.AI"],"published":"20170707","updated":"20170710","primary_category":"cs.AI"}
{"arxiv_id":"1910.13461","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","abstract":"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.","source":"http:\/\/arxiv.org\/pdf\/1910.13461","authors":["Mike Lewis","Yinhan Liu","Naman Goyal","Marjan Ghazvininejad","Abdelrahman Mohamed","Omer Levy","Ves Stoyanov","Luke Zettlemoyer"],"categories":["cs.CL","cs.LG","stat.ML"],"published":"20191029","updated":"20191029","primary_category":"cs.CL"}
{"arxiv_id":"1806.03822","title":"Know What You Don't Know: Unanswerable Questions for SQuAD","abstract":"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.","source":"http:\/\/arxiv.org\/pdf\/1806.03822","authors":["Pranav Rajpurkar","Robin Jia","Percy Liang"],"categories":["cs.CL"],"published":"20180611","updated":"20180611","primary_category":"cs.CL"}
{"arxiv_id":"2205.11503","title":"Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models","abstract":"We propose a method for arbitrary textual style transfer (TST)--the task of transforming a text into any given style--utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Specifically, our method first uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks these candidates according to a combination of the three components above. Empirically, our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while consuming two orders of magnitude less compute and memory. Finally, we conduct a systematic investigation of the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets.","source":"http:\/\/arxiv.org\/pdf\/2205.11503","authors":["Mirac Suzgun","Luke Melas-Kyriazi","Dan Jurafsky"],"categories":["cs.CL"],"published":"20220523","updated":"20220523","primary_category":"cs.CL"}
{"arxiv_id":"2206.04615","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models","abstract":"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.","source":"http:\/\/arxiv.org\/pdf\/2206.04615","authors":["Aarohi Srivastava","Abhinav Rastogi","Abhishek Rao","Abu Awal Md Shoeb","Abubakar Abid","Adam Fisch","Adam R. Brown","Adam Santoro","Aditya Gupta","Adri\u00e0 Garriga-Alonso","Agnieszka Kluska","Aitor Lewkowycz","Akshat Agarwal","Alethea Power","Alex Ray","Alex Warstadt","Alexander W. Kocurek","Ali Safaya","Ali Tazarv","Alice Xiang","Alicia Parrish","Allen Nie","Aman Hussain","Amanda Askell","Amanda Dsouza","Ambrose Slone","Ameet Rahane","Anantharaman S. Iyer","Anders Andreassen","Andrea Madotto","Andrea Santilli","Andreas Stuhlm\u00fcller","Andrew Dai","Andrew La","Andrew Lampinen","Andy Zou","Angela Jiang","Angelica Chen","Anh Vuong","Animesh Gupta","Anna Gottardi","Antonio Norelli","Anu Venkatesh","Arash Gholamidavoodi","Arfa Tabassum","Arul Menezes","Arun Kirubarajan","Asher Mullokandov","Ashish Sabharwal","Austin Herrick","Avia Efrat","Aykut Erdem","Ayla Karaka\u015f","B. Ryan Roberts","Bao Sheng Loe","Barret Zoph","Bart\u0142omiej Bojanowski","Batuhan \u00d6zyurt","Behnam Hedayatnia","Behnam Neyshabur","Benjamin Inden","Benno Stein","Berk Ekmekci","Bill Yuchen Lin","Blake Howald","Bryan Orinion","Cameron Diao","Cameron Dour","Catherine Stinson","Cedrick Argueta","C\u00e9sar Ferri Ram\u00edrez","Chandan Singh","Charles Rathkopf","Chenlin Meng","Chitta Baral","Chiyu Wu","Chris Callison-Burch","Chris Waites","Christian Voigt","Christopher D. Manning","Christopher Potts","Cindy Ramirez","Clara E. Rivera","Clemencia Siro","Colin Raffel","Courtney Ashcraft","Cristina Garbacea","Damien Sileo","Dan Garrette","Dan Hendrycks","Dan Kilman","Dan Roth","Daniel Freeman","Daniel Khashabi","Daniel Levy","Daniel Mosegu\u00ed Gonz\u00e1lez","Danielle Perszyk","Danny Hernandez","Danqi Chen","Daphne Ippolito","Dar Gilboa","David Dohan","David Drakard","David Jurgens","Debajyoti Datta","Deep Ganguli","Denis Emelin","Denis Kleyko","Deniz Yuret","Derek Chen","Derek Tam","Dieuwke Hupkes","Diganta Misra","Dilyar Buzan","Dimitri Coelho Mollo","Diyi Yang","Dong-Ho Lee","Dylan Schrader","Ekaterina Shutova","Ekin Dogus Cubuk","Elad Segal","Eleanor Hagerman","Elizabeth Barnes","Elizabeth Donoway","Ellie Pavlick","Emanuele Rodola","Emma Lam","Eric Chu","Eric Tang","Erkut Erdem","Ernie Chang","Ethan A. Chi","Ethan Dyer","Ethan Jerzak","Ethan Kim","Eunice Engefu Manyasi","Evgenii Zheltonozhskii","Fanyue Xia","Fatemeh Siar","Fernando Mart\u00ednez-Plumed","Francesca Happ\u00e9","Francois Chollet","Frieda Rong","Gaurav Mishra","Genta Indra Winata","Gerard de Melo","Germ\u00e1n Kruszewski","Giambattista Parascandolo","Giorgio Mariani","Gloria Wang","Gonzalo Jaimovitch-L\u00f3pez","Gregor Betz","Guy Gur-Ari","Hana Galijasevic","Hannah Kim","Hannah Rashkin","Hannaneh Hajishirzi","Harsh Mehta","Hayden Bogar","Henry Shevlin","Hinrich Sch\u00fctze","Hiromu Yakura","Hongming Zhang","Hugh Mee Wong","Ian Ng","Isaac Noble","Jaap Jumelet","Jack Geissinger","Jackson Kernion","Jacob Hilton","Jaehoon Lee","Jaime Fern\u00e1ndez Fisac","James B. Simon","James Koppel","James Zheng","James Zou","Jan Koco\u0144","Jana Thompson","Janelle Wingfield","Jared Kaplan","Jarema Radom","Jascha Sohl-Dickstein","Jason Phang","Jason Wei","Jason Yosinski","Jekaterina Novikova","Jelle Bosscher","Jennifer Marsh","Jeremy Kim","Jeroen Taal","Jesse Engel","Jesujoba Alabi","Jiacheng Xu","Jiaming Song","Jillian Tang","Joan Waweru","John Burden","John Miller","John U. Balis","Jonathan Batchelder","Jonathan Berant","J\u00f6rg Frohberg","Jos Rozen","Jose Hernandez-Orallo","Joseph Boudeman","Joseph Guerr","Joseph Jones","Joshua B. Tenenbaum","Joshua S. Rule","Joyce Chua","Kamil Kanclerz","Karen Livescu","Karl Krauth","Karthik Gopalakrishnan","Katerina Ignatyeva","Katja Markert","Kaustubh D. Dhole","Kevin Gimpel","Kevin Omondi","Kory Mathewson","Kristen Chiafullo","Ksenia Shkaruta","Kumar Shridhar","Kyle McDonell","Kyle Richardson","Laria Reynolds","Leo Gao","Li Zhang","Liam Dugan","Lianhui Qin","Lidia Contreras-Ochando","Louis-Philippe Morency","Luca Moschella","Lucas Lam","Lucy Noble","Ludwig Schmidt","Luheng He","Luis Oliveros Col\u00f3n","Luke Metz","L\u00fctfi Kerem \u015eenel","Maarten Bosma","Maarten Sap","Maartje ter Hoeve","Maheen Farooqi","Manaal Faruqui","Mantas Mazeika","Marco Baturan","Marco Marelli","Marco Maru","Maria Jose Ram\u00edrez Quintana","Marie Tolkiehn","Mario Giulianelli","Martha Lewis","Martin Potthast","Matthew L. Leavitt","Matthias Hagen","M\u00e1ty\u00e1s Schubert","Medina Orduna Baitemirova","Melody Arnaud","Melvin McElrath","Michael A. Yee","Michael Cohen","Michael Gu","Michael Ivanitskiy","Michael Starritt","Michael Strube","Micha\u0142 Sw\u0119drowski","Michele Bevilacqua","Michihiro Yasunaga","Mihir Kale","Mike Cain","Mimee Xu","Mirac Suzgun","Mitch Walker","Mo Tiwari","Mohit Bansal","Moin Aminnaseri","Mor Geva","Mozhdeh Gheini","Mukund Varma T","Nanyun Peng","Nathan A. Chi","Nayeon Lee","Neta Gur-Ari Krakover","Nicholas Cameron","Nicholas Roberts","Nick Doiron","Nicole Martinez","Nikita Nangia","Niklas Deckers","Niklas Muennighoff","Nitish Shirish Keskar","Niveditha S. Iyer","Noah Constant","Noah Fiedel","Nuan Wen","Oliver Zhang","Omar Agha","Omar Elbaghdadi","Omer Levy","Owain Evans","Pablo Antonio Moreno Casares","Parth Doshi","Pascale Fung","Paul Pu Liang","Paul Vicol","Pegah Alipoormolabashi","Peiyuan Liao","Percy Liang","Peter Chang","Peter Eckersley","Phu Mon Htut","Pinyu Hwang","Piotr Mi\u0142kowski","Piyush Patil","Pouya Pezeshkpour","Priti Oli","Qiaozhu Mei","Qing Lyu","Qinlang Chen","Rabin Banjade","Rachel Etta Rudolph","Raefer Gabriel","Rahel Habacker","Ramon Risco","Rapha\u00ebl Milli\u00e8re","Rhythm Garg","Richard Barnes","Rif A. Saurous","Riku Arakawa","Robbe Raymaekers","Robert Frank","Rohan Sikand","Roman Novak","Roman Sitelew","Ronan LeBras","Rosanne Liu","Rowan Jacobs","Rui Zhang","Ruslan Salakhutdinov","Ryan Chi","Ryan Lee","Ryan Stovall","Ryan Teehan","Rylan Yang","Sahib Singh","Saif M. Mohammad","Sajant Anand","Sam Dillavou","Sam Shleifer","Sam Wiseman","Samuel Gruetter","Samuel R. Bowman","Samuel S. Schoenholz","Sanghyun Han","Sanjeev Kwatra","Sarah A. Rous","Sarik Ghazarian","Sayan Ghosh","Sean Casey","Sebastian Bischoff","Sebastian Gehrmann","Sebastian Schuster","Sepideh Sadeghi","Shadi Hamdan","Sharon Zhou","Shashank Srivastava","Sherry Shi","Shikhar Singh","Shima Asaadi","Shixiang Shane Gu","Shubh Pachchigar","Shubham Toshniwal","Shyam Upadhyay","Shyamolima","Debnath","Siamak Shakeri","Simon Thormeyer","Simone Melzi","Siva Reddy","Sneha Priscilla Makini","Soo-Hwan Lee","Spencer Torene","Sriharsha Hatwar","Stanislas Dehaene","Stefan Divic","Stefano Ermon","Stella Biderman","Stephanie Lin","Stephen Prasad","Steven T. Piantadosi","Stuart M. Shieber","Summer Misherghi","Svetlana Kiritchenko","Swaroop Mishra","Tal Linzen","Tal Schuster","Tao Li","Tao Yu","Tariq Ali","Tatsu Hashimoto","Te-Lin Wu","Th\u00e9o Desbordes","Theodore Rothschild","Thomas Phan","Tianle Wang","Tiberius Nkinyili","Timo Schick","Timofei Kornev","Titus Tunduny","Tobias Gerstenberg","Trenton Chang","Trishala Neeraj","Tushar Khot","Tyler Shultz","Uri Shaham","Vedant Misra","Vera Demberg","Victoria Nyamai","Vikas Raunak","Vinay Ramasesh","Vinay Uday Prabhu","Vishakh Padmakumar","Vivek Srikumar","William Fedus","William Saunders","William Zhang","Wout Vossen","Xiang Ren","Xiaoyu Tong","Xinran Zhao","Xinyi Wu","Xudong Shen","Yadollah Yaghoobzadeh","Yair Lakretz","Yangqiu Song","Yasaman Bahri","Yejin Choi","Yichi Yang","Yiding Hao","Yifu Chen","Yonatan Belinkov","Yu Hou","Yufang Hou","Yuntao Bai","Zachary Seid","Zhuoye Zhao","Zijian Wang","Zijie J. Wang","Zirui Wang","Ziyi Wu"],"categories":["cs.CL","cs.AI","cs.CY","cs.LG","stat.ML"],"published":"20220609","updated":"20230612","primary_category":"cs.CL"}
{"arxiv_id":"2305.11206","title":"LIMA: Less Is More for Alignment","abstract":"Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.","source":"http:\/\/arxiv.org\/pdf\/2305.11206","authors":["Chunting Zhou","Pengfei Liu","Puxin Xu","Srini Iyer","Jiao Sun","Yuning Mao","Xuezhe Ma","Avia Efrat","Ping Yu","Lili Yu","Susan Zhang","Gargi Ghosh","Mike Lewis","Luke Zettlemoyer","Omer Levy"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20230518","updated":"20230518","primary_category":"cs.CL"}
{"arxiv_id":"2211.10435","title":"PAL: Program-aided Language Models","abstract":"Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as \"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http:\/\/reasonwithpal.com\/ .","source":"http:\/\/arxiv.org\/pdf\/2211.10435","authors":["Luyu Gao","Aman Madaan","Shuyan Zhou","Uri Alon","Pengfei Liu","Yiming Yang","Jamie Callan","Graham Neubig"],"categories":["cs.CL","cs.AI"],"published":"20221118","updated":"20230127","primary_category":"cs.CL"}
{"arxiv_id":"1805.09733","title":"Towards Robust Evaluations of Continual Learning","abstract":"Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.","source":"http:\/\/arxiv.org\/pdf\/1805.09733","authors":["Sebastian Farquhar","Yarin Gal"],"categories":["stat.ML","cs.LG"],"published":"20180524","updated":"20190626","primary_category":"cs.LG"}
{"arxiv_id":"2205.11916","title":"Large Language Models are Zero-Shot Reasoners","abstract":"Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.","source":"http:\/\/arxiv.org\/pdf\/2205.11916","authors":["Takeshi Kojima","Shixiang Shane Gu","Machel Reid","Yutaka Matsuo","Yusuke Iwasawa"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20220524","updated":"20230129","primary_category":"cs.CL"}
{"arxiv_id":"1701.08118","title":"Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis","abstract":"Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.","source":"http:\/\/arxiv.org\/pdf\/1701.08118","authors":["Bj\u00f6rn Ross","Michael Rist","Guillermo Carbonell","Benjamin Cabrera","Nils Kurowsky","Michael Wojatzki"],"categories":["cs.CL"],"published":"20170127","updated":"20170127","primary_category":"cs.CL"}
{"arxiv_id":"1511.06434","title":"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks","abstract":"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.","source":"http:\/\/arxiv.org\/pdf\/1511.06434","authors":["Alec Radford","Luke Metz","Soumith Chintala"],"categories":["cs.LG","cs.CV"],"published":"20151119","updated":"20160107","primary_category":"cs.LG"}
{"arxiv_id":"1709.04546","title":"Normalized Direction-preserving Adam","abstract":"Adaptive optimization algorithms, such as Adam and RMSprop, have shown better optimization performance than stochastic gradient descent (SGD) in some scenarios. However, recent studies show that they often lead to worse generalization performance than SGD, especially for training deep neural networks (DNNs). In this work, we identify the reasons that Adam generalizes worse than SGD, and develop a variant of Adam to eliminate the generalization gap. The proposed method, normalized direction-preserving Adam (ND-Adam), enables more precise control of the direction and step size for updating weight vectors, leading to significantly improved generalization performance. Following a similar rationale, we further improve the generalization performance in classification tasks by regularizing the softmax logits. By bridging the gap between SGD and Adam, we also hope to shed light on why certain optimization algorithms generalize better than others.","source":"http:\/\/arxiv.org\/pdf\/1709.04546","authors":["Zijun Zhang","Lin Ma","Zongpeng Li","Chuan Wu"],"categories":["cs.LG","stat.ML"],"published":"20170913","updated":"20180918","primary_category":"cs.LG"}
{"arxiv_id":"2006.12442","title":"Open-Domain Conversational Agents: Current Progress, Open Problems, and Future Directions","abstract":"We present our view of what is necessary to build an engaging open-domain conversational agent: covering the qualities of such an agent, the pieces of the puzzle that have been built so far, and the gaping holes we have not filled yet. We present a biased view, focusing on work done by our own group, while citing related work in each area. In particular, we discuss in detail the properties of continual learning, providing engaging content, and being well-behaved -- and how to measure success in providing them. We end with a discussion of our experience and learnings, and our recommendations to the community.","source":"http:\/\/arxiv.org\/pdf\/2006.12442","authors":["Stephen Roller","Y-Lan Boureau","Jason Weston","Antoine Bordes","Emily Dinan","Angela Fan","David Gunning","Da Ju","Margaret Li","Spencer Poff","Pratik Ringshia","Kurt Shuster","Eric Michael Smith","Arthur Szlam","Jack Urbanek","Mary Williamson"],"categories":["cs.CL","cs.AI"],"published":"20200622","updated":"20200713","primary_category":"cs.CL"}
{"arxiv_id":"2107.12808","title":"Open-Ended Learning Leads to Generally Capable Agents","abstract":"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.","source":"http:\/\/arxiv.org\/pdf\/2107.12808","authors":["Open Ended Learning Team","Adam Stooke","Anuj Mahajan","Catarina Barros","Charlie Deck","Jakob Bauer","Jakub Sygnowski","Maja Trebacz","Max Jaderberg","Michael Mathieu","Nat McAleese","Nathalie Bradley-Schmieg","Nathaniel Wong","Nicolas Porcel","Roberta Raileanu","Steph Hughes-Fitt","Valentin Dalibard","Wojciech Marian Czarnecki"],"categories":["cs.LG","cs.AI","cs.MA"],"published":"20210727","updated":"20210731","primary_category":"cs.LG"}
{"arxiv_id":"2004.13637","title":"Recipes for building an open-domain chatbot","abstract":"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.","source":"http:\/\/arxiv.org\/pdf\/2004.13637","authors":["Stephen Roller","Emily Dinan","Naman Goyal","Da Ju","Mary Williamson","Yinhan Liu","Jing Xu","Myle Ott","Kurt Shuster","Eric M. Smith","Y-Lan Boureau","Jason Weston"],"categories":["cs.CL","cs.AI"],"published":"20200428","updated":"20200430","primary_category":"cs.CL"}
{"arxiv_id":"1908.01091","title":"Toward Understanding Catastrophic Forgetting in Continual Learning","abstract":"We study the relationship between catastrophic forgetting and properties of task sequences. In particular, given a sequence of tasks, we would like to understand which properties of this sequence influence the error rates of continual learning algorithms trained on the sequence. To this end, we propose a new procedure that makes use of recent developments in task space modeling as well as correlation analysis to specify and analyze the properties we are interested in. As an application, we apply our procedure to study two properties of a task sequence: (1) total complexity and (2) sequential heterogeneity. We show that error rates are strongly and positively correlated to a task sequence's total complexity for some state-of-the-art algorithms. We also show that, surprisingly, the error rates have no or even negative correlations in some cases to sequential heterogeneity. Our findings suggest directions for improving continual learning benchmarks and methods.","source":"http:\/\/arxiv.org\/pdf\/1908.01091","authors":["Cuong V. Nguyen","Alessandro Achille","Michael Lam","Tal Hassner","Vijay Mahadevan","Stefano Soatto"],"categories":["cs.LG","cs.CV","stat.ML"],"published":"20190802","updated":"20190802","primary_category":"cs.LG"}
{"arxiv_id":"1902.06720","title":"Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent","abstract":"A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.","source":"http:\/\/arxiv.org\/pdf\/1902.06720","authors":["Jaehoon Lee","Lechao Xiao","Samuel S. Schoenholz","Yasaman Bahri","Roman Novak","Jascha Sohl-Dickstein","Jeffrey Pennington"],"categories":["stat.ML","cs.LG"],"published":"20190218","updated":"20191208","primary_category":"cs.LG"}
{"arxiv_id":"1912.02164","title":"Plug and Play Language Models: A Simple Approach to Controlled Text Generation","abstract":"Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.","source":"http:\/\/arxiv.org\/pdf\/1912.02164","authors":["Sumanth Dathathri","Andrea Madotto","Janice Lan","Jane Hung","Eric Frank","Piero Molino","Jason Yosinski","Rosanne Liu"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20191204","updated":"20200303","primary_category":"cs.CL"}
{"arxiv_id":"2303.15056","title":"ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks","abstract":"Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.","source":"http:\/\/arxiv.org\/pdf\/2303.15056","authors":["Fabrizio Gilardi","Meysam Alizadeh","Ma\u00ebl Kubli"],"categories":["cs.CL","cs.CY"],"published":"20230327","updated":"20230719","primary_category":"cs.CL"}
{"arxiv_id":"1906.02738","title":"Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading","abstract":"Although neural conversation models are effective in learning how to produce fluent responses, their primary challenge lies in knowing what to say to make the conversation contentful and non-vacuous. We present a new end-to-end approach to contentful neural conversation that jointly models response generation and on-demand machine reading. The key idea is to provide the conversation model with relevant long-form text on the fly as a source of external knowledge. The model performs QA-style reading comprehension on this text in response to each conversational turn, thereby allowing for more focused integration of external knowledge than has been possible in prior approaches. To support further research on knowledge-grounded conversation, we introduce a new large-scale conversation dataset grounded in external web pages (2.8M turns, 7.4M sentences of grounding). Both human evaluation and automated metrics show that our approach results in more contentful responses compared to a variety of previous methods, improving both the informativeness and diversity of generated output.","source":"http:\/\/arxiv.org\/pdf\/1906.02738","authors":["Lianhui Qin","Michel Galley","Chris Brockett","Xiaodong Liu","Xiang Gao","Bill Dolan","Yejin Choi","Jianfeng Gao"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20190606","updated":"20190607","primary_category":"cs.CL"}
{"arxiv_id":"1803.05457","title":"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge","abstract":"We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.","source":"http:\/\/arxiv.org\/pdf\/1803.05457","authors":["Peter Clark","Isaac Cowhey","Oren Etzioni","Tushar Khot","Ashish Sabharwal","Carissa Schoenick","Oyvind Tafjord"],"categories":["cs.AI","cs.CL","cs.IR"],"published":"20180314","updated":"20180314","primary_category":"cs.AI"}
{"arxiv_id":"2108.07732","title":"Program Synthesis with Large Language Models","abstract":"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.","source":"http:\/\/arxiv.org\/pdf\/2108.07732","authors":["Jacob Austin","Augustus Odena","Maxwell Nye","Maarten Bosma","Henryk Michalewski","David Dohan","Ellen Jiang","Carrie Cai","Michael Terry","Quoc Le","Charles Sutton"],"categories":["cs.PL","cs.LG"],"published":"20210816","updated":"20210816","primary_category":"cs.PL"}
{"arxiv_id":"2210.07700","title":"Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey","abstract":"Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works' taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks\/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different mitigation strategies' motivations, their limitations, and open problems for future research.","source":"http:\/\/arxiv.org\/pdf\/2210.07700","authors":["Sachin Kumar","Vidhisha Balachandran","Lucille Njoo","Antonios Anastasopoulos","Yulia Tsvetkov"],"categories":["cs.CL"],"published":"20221014","updated":"20230221","primary_category":"cs.CL"}
{"arxiv_id":"2112.05682","title":"Self-attention Does Not Need $O(n^2)$ Memory","abstract":"We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.","source":"http:\/\/arxiv.org\/pdf\/2112.05682","authors":["Markus N. Rabe","Charles Staats"],"categories":["cs.LG"],"published":"20211210","updated":"20221010","primary_category":"cs.LG"}
{"arxiv_id":"1707.08819","title":"A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets","abstract":"The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32$\\times$32 (and its variants ImageNet64$\\times$64 and ImageNet16$\\times$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32$\\times$32 pixels per image (64$\\times$64 and 16$\\times$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http:\/\/image-net.org\/download-images and https:\/\/github.com\/PatrykChrabaszcz\/Imagenet32_Scripts","source":"http:\/\/arxiv.org\/pdf\/1707.08819","authors":["Patryk Chrabaszcz","Ilya Loshchilov","Frank Hutter"],"categories":["cs.CV","cs.LG"],"published":"20170727","updated":"20170823","primary_category":"cs.CV"}
{"arxiv_id":"2009.06367","title":"GeDi: Generative Discriminator Guided Sequence Generation","abstract":"While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.","source":"http:\/\/arxiv.org\/pdf\/2009.06367","authors":["Ben Krause","Akhilesh Deepak Gotmare","Bryan McCann","Nitish Shirish Keskar","Shafiq Joty","Richard Socher","Nazneen Fatema Rajani"],"categories":["cs.CL","cs.LG"],"published":"20200914","updated":"20201022","primary_category":"cs.CL"}
{"arxiv_id":"2010.11982","title":"The Turking Test: Can Language Models Understand Instructions?","abstract":"Supervised machine learning provides the learner with a set of input-output examples of the target task. Humans, however, can also learn to perform new tasks from instructions in natural language. Can machines learn to understand instructions as well? We present the Turking Test, which examines a model's ability to follow natural language instructions of varying complexity. These range from simple tasks, like retrieving the nth word of a sentence, to ones that require creativity, such as generating examples for SNLI and SQuAD in place of human intelligence workers (\"turkers\"). Despite our lenient evaluation methodology, we observe that a large pretrained language model performs poorly across all tasks. Analyzing the model's error patterns reveals that the model tends to ignore explicit instructions and often generates outputs that cannot be construed as an attempt to solve the task. While it is not yet clear whether instruction understanding can be captured by traditional language models, the sheer expressivity of instruction understanding makes it an appealing alternative to the rising few-shot inference paradigm.","source":"http:\/\/arxiv.org\/pdf\/2010.11982","authors":["Avia Efrat","Omer Levy"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20201022","updated":"20201022","primary_category":"cs.CL"}
{"arxiv_id":"1909.08053","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism","abstract":"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).","source":"http:\/\/arxiv.org\/pdf\/1909.08053","authors":["Mohammad Shoeybi","Mostofa Patwary","Raul Puri","Patrick LeGresley","Jared Casper","Bryan Catanzaro"],"categories":["cs.CL"],"published":"20190917","updated":"20200313","primary_category":"cs.CL"}
{"arxiv_id":"2102.04351","title":"Generating Fake Cyber Threat Intelligence Using Transformer-Based Models","abstract":"Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and\/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as training input to subvert cyber defense systems, forcing the model to learn incorrect inputs to serve their malicious needs. In this paper, we automatically generate fake CTI text descriptions using transformers. We show that given an initial prompt sentence, a public language model like GPT-2 with fine-tuning, can generate plausible CTI text with the ability of corrupting cyber-defense systems. We utilize the generated fake CTI text to perform a data poisoning attack on a Cybersecurity Knowledge Graph (CKG) and a cybersecurity corpus. The poisoning attack introduced adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. We evaluate with traditional approaches and conduct a human evaluation study with cybersecurity professionals and threat hunters. Based on the study, professional threat hunters were equally likely to consider our fake generated CTI as true.","source":"http:\/\/arxiv.org\/pdf\/2102.04351","authors":["Priyanka Ranade","Aritran Piplai","Sudip Mittal","Anupam Joshi","Tim Finin"],"categories":["cs.CR","cs.AI"],"published":"20210208","updated":"20210618","primary_category":"cs.CR"}
{"arxiv_id":"1801.04871","title":"Building a Conversational Agent Overnight with Dialogue Self-Play","abstract":"We propose Machines Talking To Machines (M2M), a framework combining automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents for goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with just a task schema and an API client from the dialogue system developer, but it is also customizable to cater to task-specific interactions. Compared to the Wizard-of-Oz approach for data collection, M2M achieves greater diversity and coverage of salient dialogue flows while maintaining the naturalness of individual utterances. In the first phase, a simulated user bot and a domain-agnostic system bot converse to exhaustively generate dialogue \"outlines\", i.e. sequences of template utterances and their semantic parses. In the second phase, crowd workers provide contextual rewrites of the dialogues to make the utterances more natural while preserving their meaning. The entire process can finish within a few hours. We propose a new corpus of 3,000 dialogues spanning 2 domains collected with M2M, and present comparisons with popular dialogue datasets on the quality and diversity of the surface forms and dialogue flows.","source":"http:\/\/arxiv.org\/pdf\/1801.04871","authors":["Pararth Shah","Dilek Hakkani-T\u00fcr","Gokhan T\u00fcr","Abhinav Rastogi","Ankur Bapna","Neha Nayak","Larry Heck"],"categories":["cs.AI","cs.CL"],"published":"20180115","updated":"20180115","primary_category":"cs.AI"}
{"arxiv_id":"2207.10551","title":"Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?","abstract":"There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.","source":"http:\/\/arxiv.org\/pdf\/2207.10551","authors":["Yi Tay","Mostafa Dehghani","Samira Abnar","Hyung Won Chung","William Fedus","Jinfeng Rao","Sharan Narang","Vinh Q. Tran","Dani Yogatama","Donald Metzler"],"categories":["cs.LG","cs.CL"],"published":"20220721","updated":"20220721","primary_category":"cs.LG"}
{"arxiv_id":"2112.10752","title":"High-Resolution Image Synthesis with Latent Diffusion Models","abstract":"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https:\/\/github.com\/CompVis\/latent-diffusion .","source":"http:\/\/arxiv.org\/pdf\/2112.10752","authors":["Robin Rombach","Andreas Blattmann","Dominik Lorenz","Patrick Esser","Bj\u00f6rn Ommer"],"categories":["cs.CV"],"published":"20211220","updated":"20220413","primary_category":"cs.CV"}
{"arxiv_id":"1905.03197","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation","abstract":"This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN\/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https:\/\/github.com\/microsoft\/unilm.","source":"http:\/\/arxiv.org\/pdf\/1905.03197","authors":["Li Dong","Nan Yang","Wenhui Wang","Furu Wei","Xiaodong Liu","Yu Wang","Jianfeng Gao","Ming Zhou","Hsiao-Wuen Hon"],"categories":["cs.CL"],"published":"20190508","updated":"20191015","primary_category":"cs.CL"}
{"arxiv_id":"2005.13170","title":"Chat as Expected: Learning to Manipulate Black-box Neural Dialogue Models","abstract":"Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.","source":"http:\/\/arxiv.org\/pdf\/2005.13170","authors":["Haochen Liu","Zhiwei Wang","Tyler Derr","Jiliang Tang"],"categories":["cs.CL","cs.AI"],"published":"20200527","updated":"20200527","primary_category":"cs.CL"}
{"arxiv_id":"2210.03493","title":"Automatic Chain of Thought Prompting in Large Language Models","abstract":"Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like \"Let's think step by step\" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the \"Let's think step by step\" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https:\/\/github.com\/amazon-research\/auto-cot","source":"http:\/\/arxiv.org\/pdf\/2210.03493","authors":["Zhuosheng Zhang","Aston Zhang","Mu Li","Alex Smola"],"categories":["cs.CL","cs.AI"],"published":"20221007","updated":"20221007","primary_category":"cs.CL"}
{"arxiv_id":"1806.08730","title":"The Natural Language Decathlon: Multitask Learning as Question Answering","abstract":"Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting. MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.","source":"http:\/\/arxiv.org\/pdf\/1806.08730","authors":["Bryan McCann","Nitish Shirish Keskar","Caiming Xiong","Richard Socher"],"categories":["cs.CL","cs.AI","cs.LG","stat.ML"],"published":"20180620","updated":"20180620","primary_category":"cs.CL"}
{"arxiv_id":"1908.04577","title":"StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding","abstract":"Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.","source":"http:\/\/arxiv.org\/pdf\/1908.04577","authors":["Wei Wang","Bin Bi","Ming Yan","Chen Wu","Zuyi Bao","Jiangnan Xia","Liwei Peng","Luo Si"],"categories":["cs.CL"],"published":"20190813","updated":"20190927","primary_category":"cs.CL"}
{"arxiv_id":"1607.06450","title":"Layer Normalization","abstract":"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.","source":"http:\/\/arxiv.org\/pdf\/1607.06450","authors":["Jimmy Lei Ba","Jamie Ryan Kiros","Geoffrey E. Hinton"],"categories":["stat.ML","cs.LG"],"published":"20160721","updated":"20160721","primary_category":"cs.LG"}
{"arxiv_id":"2011.02839","title":"Chasing Carbon: The Elusive Environmental Footprint of Computing","abstract":"Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.","source":"http:\/\/arxiv.org\/pdf\/2011.02839","authors":["Udit Gupta","Young Geun Kim","Sylvia Lee","Jordan Tse","Hsien-Hsin S. Lee","Gu-Yeon Wei","David Brooks","Carole-Jean Wu"],"categories":["cs.AR","cs.CY"],"published":"20201028","updated":"20201028","primary_category":"cs.AR"}
{"arxiv_id":"2302.07459","title":"The Capacity for Moral Self-Correction in Large Language Models","abstract":"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.","source":"http:\/\/arxiv.org\/pdf\/2302.07459","authors":["Deep Ganguli","Amanda Askell","Nicholas Schiefer","Thomas I. Liao","Kamil\u0117 Luko\u0161i\u016bt\u0117","Anna Chen","Anna Goldie","Azalia Mirhoseini","Catherine Olsson","Danny Hernandez","Dawn Drain","Dustin Li","Eli Tran-Johnson","Ethan Perez","Jackson Kernion","Jamie Kerr","Jared Mueller","Joshua Landau","Kamal Ndousse","Karina Nguyen","Liane Lovitt","Michael Sellitto","Nelson Elhage","Noemi Mercado","Nova DasSarma","Oliver Rausch","Robert Lasenby","Robin Larson","Sam Ringer","Sandipan Kundu","Saurav Kadavath","Scott Johnston","Shauna Kravec","Sheer El Showk","Tamera Lanham","Timothy Telleen-Lawton","Tom Henighan","Tristan Hume","Yuntao Bai","Zac Hatfield-Dodds","Ben Mann","Dario Amodei","Nicholas Joseph","Sam McCandlish","Tom Brown","Christopher Olah","Jack Clark","Samuel R. Bowman","Jared Kaplan"],"categories":["cs.CL"],"published":"20230215","updated":"20230218","primary_category":"cs.CL"}
{"arxiv_id":"2202.07206","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning","abstract":"Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above $70\\%$ (absolute) more accurate on the top 10\\% frequent terms in comparison to the bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.","source":"http:\/\/arxiv.org\/pdf\/2202.07206","authors":["Yasaman Razeghi","Robert L. Logan IV","Matt Gardner","Sameer Singh"],"categories":["cs.CL","cs.LG"],"published":"20220215","updated":"20220524","primary_category":"cs.CL"}
{"arxiv_id":"2106.15590","title":"The Values Encoded in Machine Learning Research","abstract":"Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15\\%) and far fewer discuss negative potential (1\\%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power.Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.","source":"http:\/\/arxiv.org\/pdf\/2106.15590","authors":["Abeba Birhane","Pratyusha Kalluri","Dallas Card","William Agnew","Ravit Dotan","Michelle Bao"],"categories":["cs.LG","cs.AI","cs.CY"],"published":"20210629","updated":"20220621","primary_category":"cs.LG"}
{"arxiv_id":"1806.10293","title":"QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation","abstract":"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.","source":"http:\/\/arxiv.org\/pdf\/1806.10293","authors":["Dmitry Kalashnikov","Alex Irpan","Peter Pastor","Julian Ibarz","Alexander Herzog","Eric Jang","Deirdre Quillen","Ethan Holly","Mrinal Kalakrishnan","Vincent Vanhoucke","Sergey Levine"],"categories":["cs.LG","cs.AI","cs.CV","cs.RO","stat.ML"],"published":"20180627","updated":"20181128","primary_category":"cs.LG"}
{"arxiv_id":"2101.05783","title":"Persistent Anti-Muslim Bias in Large Language Models","abstract":"It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, \"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is mapped to \"money\" in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for \"Muslims\" from 66% to 20%, but which is still higher than for other religious groups.","source":"http:\/\/arxiv.org\/pdf\/2101.05783","authors":["Abubakar Abid","Maheen Farooqi","James Zou"],"categories":["cs.CL","cs.LG"],"published":"20210114","updated":"20210118","primary_category":"cs.CL"}
{"arxiv_id":"2205.12673","title":"InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning","abstract":"Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.","source":"http:\/\/arxiv.org\/pdf\/2205.12673","authors":["Prakhar Gupta","Cathy Jiao","Yi-Ting Yeh","Shikib Mehri","Maxine Eskenazi","Jeffrey P. Bigham"],"categories":["cs.CL"],"published":"20220525","updated":"20221026","primary_category":"cs.CL"}
{"arxiv_id":"2110.08514","title":"Analyzing Dynamic Adversarial Training Data in the Limit","abstract":"To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.","source":"http:\/\/arxiv.org\/pdf\/2110.08514","authors":["Eric Wallace","Adina Williams","Robin Jia","Douwe Kiela"],"categories":["cs.CL","cs.LG"],"published":"20211016","updated":"20220926","primary_category":"cs.CL"}
{"arxiv_id":"2005.00614","title":"Multi-Dimensional Gender Bias Classification","abstract":"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites. Distinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers. We show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models, detecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.","source":"http:\/\/arxiv.org\/pdf\/2005.00614","authors":["Emily Dinan","Angela Fan","Ledell Wu","Jason Weston","Douwe Kiela","Adina Williams"],"categories":["cs.CL"],"published":"20200501","updated":"20200501","primary_category":"cs.CL"}
{"arxiv_id":"2203.09161","title":"How Many Data Samples is an Additional Instruction Worth?","abstract":"Recently introduced instruction-paradigm empowers non-expert users to leverage NLP resources by defining a new task in natural language. Instruction-tuned models have significantly outperformed multitask learning models (without instruction); however they are far from state-of-the-art task-specific models. Conventional approaches to improve model performance via creating datasets with large number of task instances or architectural changes in the model may not be feasible for non-expert users. However, they can write alternate instructions to represent an instruction task. Is Instruction-augmentation helpful? We augment a subset of tasks in the expanded version of NATURAL INSTRUCTIONS with additional instructions and find that it significantly improves model performance (up to 35%), especially in the low-data regime. Our results indicate that an additional instruction can be equivalent to ~200 data samples on average across tasks.","source":"http:\/\/arxiv.org\/pdf\/2203.09161","authors":["Ravsehaj Singh Puri","Swaroop Mishra","Mihir Parmar","Chitta Baral"],"categories":["cs.CL","cs.AI","cs.CV","cs.LG"],"published":"20220317","updated":"20230213","primary_category":"cs.CL"}
{"arxiv_id":"2210.09261","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them","abstract":"BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.","source":"http:\/\/arxiv.org\/pdf\/2210.09261","authors":["Mirac Suzgun","Nathan Scales","Nathanael Sch\u00e4rli","Sebastian Gehrmann","Yi Tay","Hyung Won Chung","Aakanksha Chowdhery","Quoc V. Le","Ed H. Chi","Denny Zhou","Jason Wei"],"categories":["cs.CL","cs.AI"],"published":"20221017","updated":"20221017","primary_category":"cs.CL"}
{"arxiv_id":"1708.07747","title":"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms","abstract":"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https:\/\/github.com\/zalandoresearch\/fashion-mnist","source":"http:\/\/arxiv.org\/pdf\/1708.07747","authors":["Han Xiao","Kashif Rasul","Roland Vollgraf"],"categories":["cs.LG","cs.CV","stat.ML"],"published":"20170825","updated":"20170915","primary_category":"cs.LG"}
{"arxiv_id":"2106.13219","title":"Towards Understanding and Mitigating Social Biases in Language Models","abstract":"As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.","source":"http:\/\/arxiv.org\/pdf\/2106.13219","authors":["Paul Pu Liang","Chiyu Wu","Louis-Philippe Morency","Ruslan Salakhutdinov"],"categories":["cs.CL","cs.AI","cs.CY","cs.LG"],"published":"20210624","updated":"20210624","primary_category":"cs.CL"}
{"arxiv_id":"2203.13224","title":"Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion","abstract":"Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.","source":"http:\/\/arxiv.org\/pdf\/2203.13224","authors":["Kurt Shuster","Mojtaba Komeili","Leonard Adolphs","Stephen Roller","Arthur Szlam","Jason Weston"],"categories":["cs.CL","cs.AI"],"published":"20220324","updated":"20220329","primary_category":"cs.CL"}
{"arxiv_id":"2301.08653","title":"An Analysis of the Automatic Bug Fixing Performance of ChatGPT","abstract":"To support software developers in finding and fixing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to find test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compare the performance with the results of several other approaches reported in the literature. We find that ChatGPT's bug fixing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, ChatGPT offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to ChatGPT, its success rate can be further increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.","source":"http:\/\/arxiv.org\/pdf\/2301.08653","authors":["Dominik Sobania","Martin Briesch","Carol Hanna","Justyna Petke"],"categories":["cs.SE"],"published":"20230120","updated":"20230120","primary_category":"cs.SE"}
{"arxiv_id":"2103.11811","title":"MasakhaNER: Named Entity Recognition for African Languages","abstract":"We take a step towards addressing the under-representation of the African continent in NLP research by creating the first large publicly available high-quality dataset for named entity recognition (NER) in ten African languages, bringing together a variety of stakeholders. We detail characteristics of the languages to help researchers understand the challenges that these languages pose for NER. We analyze our datasets and conduct an extensive empirical evaluation of state-of-the-art methods across both supervised and transfer learning settings. We release the data, code, and models in order to inspire future research on African NLP.","source":"http:\/\/arxiv.org\/pdf\/2103.11811","authors":["David Ifeoluwa Adelani","Jade Abbott","Graham Neubig","Daniel D'souza","Julia Kreutzer","Constantine Lignos","Chester Palen-Michel","Happy Buzaaba","Shruti Rijhwani","Sebastian Ruder","Stephen Mayhew","Israel Abebe Azime","Shamsuddeen Muhammad","Chris Chinenye Emezue","Joyce Nakatumba-Nabende","Perez Ogayo","Anuoluwapo Aremu","Catherine Gitau","Derguene Mbaye","Jesujoba Alabi","Seid Muhie Yimam","Tajuddeen Gwadabe","Ignatius Ezeani","Rubungo Andre Niyongabo","Jonathan Mukiibi","Verrah Otiende","Iroro Orife","Davis David","Samba Ngom","Tosin Adewumi","Paul Rayson","Mofetoluwa Adeyemi","Gerald Muriuki","Emmanuel Anebi","Chiamaka Chukwuneke","Nkiruka Odu","Eric Peter Wairagala","Samuel Oyerinde","Clemencia Siro","Tobius Saul Bateesa","Temilola Oloyede","Yvonne Wambui","Victor Akinode","Deborah Nabagereka","Maurice Katusiime","Ayodele Awokoya","Mouhamadane MBOUP","Dibora Gebreyohannes","Henok Tilaye","Kelechi Nwaike","Degaga Wolde","Abdoulaye Faye","Blessing Sibanda","Orevaoghene Ahia","Bonaventure F. P. Dossou","Kelechi Ogueji","Thierno Ibrahima DIOP","Abdoulaye Diallo","Adewale Akinfaderin","Tendai Marengereke","Salomey Osei"],"categories":["cs.CL","cs.AI"],"published":"20210322","updated":"20210705","primary_category":"cs.CL"}
{"arxiv_id":"1901.11117","title":"The Evolved Transformer","abstract":"Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original \"big\" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.","source":"http:\/\/arxiv.org\/pdf\/1901.11117","authors":["David R. So","Chen Liang","Quoc V. Le"],"categories":["cs.LG","cs.CL","cs.NE","stat.ML"],"published":"20190130","updated":"20190517","primary_category":"cs.LG"}
{"arxiv_id":"1905.10044","title":"BoolQ: Exploring the Surprising Difficulty of Natural Yes\/No Questions","abstract":"In this paper we study yes\/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.","source":"http:\/\/arxiv.org\/pdf\/1905.10044","authors":["Christopher Clark","Kenton Lee","Ming-Wei Chang","Tom Kwiatkowski","Michael Collins","Kristina Toutanova"],"categories":["cs.CL"],"published":"20190524","updated":"20190524","primary_category":"cs.CL"}
{"arxiv_id":"2103.01991","title":"Adversarial Environment Generation for Learning to Navigate the Web","abstract":"Learning to autonomously navigate the web is a difficult sequential decision making task. The state and action spaces are large and combinatorial in nature, and websites are dynamic environments consisting of several pages. One of the bottlenecks of training web navigation agents is providing a learnable curriculum of training environments that can cover the large variety of real-world websites. Therefore, we propose using Adversarial Environment Generation (AEG) to generate challenging web environments in which to train reinforcement learning (RL) agents. We provide a new benchmarking environment, gMiniWoB, which enables an RL adversary to use compositional primitives to learn to generate arbitrarily complex websites. To train the adversary, we propose a new technique for maximizing regret using the difference in the scores obtained by a pair of navigator agents. Our results show that our approach significantly outperforms prior methods for minimax regret AEG. The regret objective trains the adversary to design a curriculum of environments that are \"just-the-right-challenge\" for the navigator agents; our results show that over time, the adversary learns to generate increasingly complex web navigation tasks. The navigator agents trained with our technique learn to complete challenging, high-dimensional web navigation tasks, such as form filling, booking a flight etc. We show that the navigator agent trained with our proposed Flexible b-PAIRED technique significantly outperforms competitive automatic curriculum generation baselines -- including a state-of-the-art RL web navigation approach -- on a set of challenging unseen test environments, and achieves more than 80% success rate on some tasks.","source":"http:\/\/arxiv.org\/pdf\/2103.01991","authors":["Izzeddin Gur","Natasha Jaques","Kevin Malta","Manoj Tiwari","Honglak Lee","Aleksandra Faust"],"categories":["cs.LG","cs.AI","cs.MA"],"published":"20210302","updated":"20210302","primary_category":"cs.LG"}
{"arxiv_id":"1611.09830","title":"NewsQA: A Machine Comprehension Dataset","abstract":"We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https:\/\/datasets.maluuba.com\/NewsQA.","source":"http:\/\/arxiv.org\/pdf\/1611.09830","authors":["Adam Trischler","Tong Wang","Xingdi Yuan","Justin Harris","Alessandro Sordoni","Philip Bachman","Kaheer Suleman"],"categories":["cs.CL","cs.AI"],"published":"20161129","updated":"20170207","primary_category":"cs.CL"}
{"arxiv_id":"1502.05698","title":"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks","abstract":"One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.","source":"http:\/\/arxiv.org\/pdf\/1502.05698","authors":["Jason Weston","Antoine Bordes","Sumit Chopra","Alexander M. Rush","Bart van Merri\u00ebnboer","Armand Joulin","Tomas Mikolov"],"categories":["cs.AI","cs.CL","stat.ML"],"published":"20150219","updated":"20151231","primary_category":"cs.AI"}
{"arxiv_id":"2001.00973","title":"Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing","abstract":"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development lifecycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.","source":"http:\/\/arxiv.org\/pdf\/2001.00973","authors":["Inioluwa Deborah Raji","Andrew Smart","Rebecca N. White","Margaret Mitchell","Timnit Gebru","Ben Hutchinson","Jamila Smith-Loud","Daniel Theron","Parker Barnes"],"categories":["cs.CY"],"published":"20200103","updated":"20200103","primary_category":"cs.CY"}
{"arxiv_id":"1910.07475","title":"MLQA: Evaluating Cross-lingual Extractive Question Answering","abstract":"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making training QA systems in other languages challenging. An alternative to building large monolingual training datasets is to develop cross-lingual systems which can transfer to a target language without requiring training data in that language. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. It consists of over 12K QA instances in English and 5K in each other language, with each QA instance being parallel between 4 languages on average. MLQA is built using a novel alignment context strategy on Wikipedia articles, and serves as a cross-lingual extension to existing extractive QA datasets. We evaluate current state-of-the-art cross-lingual representations on MLQA, and also provide machine-translation-based baselines. In all cases, transfer results are shown to be significantly behind training-language performance.","source":"http:\/\/arxiv.org\/pdf\/1910.07475","authors":["Patrick Lewis","Barlas O\u011fuz","Ruty Rinott","Sebastian Riedel","Holger Schwenk"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20191016","updated":"20200503","primary_category":"cs.CL"}
{"arxiv_id":"2106.09022","title":"A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection","abstract":"Mahalanobis distance (MD) is a simple and popular post-processing method for detecting out-of-distribution (OOD) inputs in neural networks. We analyze its failure modes for near-OOD detection and propose a simple fix called relative Mahalanobis distance (RMD) which improves performance and is more robust to hyperparameter choice. On a wide selection of challenging vision, language, and biology OOD benchmarks (CIFAR-100 vs CIFAR-10, CLINC OOD intent detection, Genomics OOD), we show that RMD meaningfully improves upon MD performance (by up to 15% AUROC on genomics OOD).","source":"http:\/\/arxiv.org\/pdf\/2106.09022","authors":["Jie Ren","Stanislav Fort","Jeremiah Liu","Abhijit Guha Roy","Shreyas Padhy","Balaji Lakshminarayanan"],"categories":["cs.LG"],"published":"20210616","updated":"20210616","primary_category":"cs.LG"}
{"arxiv_id":"2206.06336","title":"Language Models are General-Purpose Interfaces","abstract":"Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.","source":"http:\/\/arxiv.org\/pdf\/2206.06336","authors":["Yaru Hao","Haoyu Song","Li Dong","Shaohan Huang","Zewen Chi","Wenhui Wang","Shuming Ma","Furu Wei"],"categories":["cs.CL"],"published":"20220613","updated":"20220613","primary_category":"cs.CL"}
{"arxiv_id":"1703.04933","title":"Sharp Minima Can Generalize For Deep Nets","abstract":"Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.","source":"http:\/\/arxiv.org\/pdf\/1703.04933","authors":["Laurent Dinh","Razvan Pascanu","Samy Bengio","Yoshua Bengio"],"categories":["cs.LG"],"published":"20170315","updated":"20170515","primary_category":"cs.LG"}
{"arxiv_id":"2112.00861","title":"A General Language Assistant as a Laboratory for Alignment","abstract":"Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.","source":"http:\/\/arxiv.org\/pdf\/2112.00861","authors":["Amanda Askell","Yuntao Bai","Anna Chen","Dawn Drain","Deep Ganguli","Tom Henighan","Andy Jones","Nicholas Joseph","Ben Mann","Nova DasSarma","Nelson Elhage","Zac Hatfield-Dodds","Danny Hernandez","Jackson Kernion","Kamal Ndousse","Catherine Olsson","Dario Amodei","Tom Brown","Jack Clark","Sam McCandlish","Chris Olah","Jared Kaplan"],"categories":["cs.CL","cs.LG"],"published":"20211201","updated":"20211209","primary_category":"cs.CL"}
{"arxiv_id":"1905.11946","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks","abstract":"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https:\/\/github.com\/tensorflow\/tpu\/tree\/master\/models\/official\/efficientnet.","source":"http:\/\/arxiv.org\/pdf\/1905.11946","authors":["Mingxing Tan","Quoc V. Le"],"categories":["cs.LG","cs.CV","stat.ML"],"published":"20190528","updated":"20200911","primary_category":"cs.LG"}
{"arxiv_id":"1907.11274","title":"Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning","abstract":"The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the use of ML to create \"synthetic media\" (e.g. to generate or manipulate audio, video, images, and text), and the question of what publication and release processes around such research might look like, though many of the considerations discussed will apply to ML research more broadly. We are not arguing for any specific approach on when or how research should be distributed, but instead try to lay out some useful tools, analogies, and options for thinking about these issues. We begin with some background on the idea that ML research might be misused in harmful ways, and why advances in synthetic media, in particular, are raising concerns. We then outline in more detail some of the different paths to harm from ML research, before reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the ML and synthetic media research communities. Next, we outline some important dimensions of disagreement on these issues which risk polarizing conversations. Finally, we conclude with recommendations, suggesting that the machine learning community might benefit from: working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies; building a community and norms around understanding the impacts of ML research, e.g. through regular workshops at major conferences; and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone.","source":"http:\/\/arxiv.org\/pdf\/1907.11274","authors":["Aviv Ovadya","Jess Whittlestone"],"categories":["cs.CY","cs.LG"],"published":"20190725","updated":"20190729","primary_category":"cs.CY"}
{"arxiv_id":"1809.02789","title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering","abstract":"We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic---in the context of common knowledge---and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.","source":"http:\/\/arxiv.org\/pdf\/1809.02789","authors":["Todor Mihaylov","Peter Clark","Tushar Khot","Ashish Sabharwal"],"categories":["cs.CL"],"published":"20180908","updated":"20180908","primary_category":"cs.CL"}
{"arxiv_id":"2212.12017","title":"OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization","abstract":"Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.","source":"http:\/\/arxiv.org\/pdf\/2212.12017","authors":["Srinivasan Iyer","Xi Victoria Lin","Ramakanth Pasunuru","Todor Mihaylov","Daniel Simig","Ping Yu","Kurt Shuster","Tianlu Wang","Qing Liu","Punit Singh Koura","Xian Li","Brian O'Horo","Gabriel Pereyra","Jeff Wang","Christopher Dewan","Asli Celikyilmaz","Luke Zettlemoyer","Ves Stoyanov"],"categories":["cs.CL"],"published":"20221222","updated":"20230130","primary_category":"cs.CL"}
{"arxiv_id":"1910.06611","title":"Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving","abstract":"We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.","source":"http:\/\/arxiv.org\/pdf\/1910.06611","authors":["Imanol Schlag","Paul Smolensky","Roland Fernandez","Nebojsa Jojic","J\u00fcrgen Schmidhuber","Jianfeng Gao"],"categories":["cs.LG","stat.ML"],"published":"20191015","updated":"20201104","primary_category":"cs.LG"}
{"arxiv_id":"1810.06683","title":"FlowQA: Grasping Flow in History for Conversational Machine Comprehension","abstract":"Conversational machine comprehension requires the understanding of the conversation history, such as previous question\/answer pairs, the document context, and the current question. To enable traditional, single-turn models to encode the history comprehensively, we introduce Flow, a mechanism that can incorporate intermediate representations generated during the process of answering previous questions, through an alternating parallel processing structure. Compared to approaches that concatenate previous questions\/answers as input, Flow integrates the latent semantics of the conversation history more deeply. Our model, FlowQA, shows superior performance on two recently proposed conversational challenges (+7.2% F1 on CoQA and +4.0% on QuAC). The effectiveness of Flow also shows in other tasks. By reducing sequential instruction understanding to conversational machine comprehension, FlowQA outperforms the best models on all three domains in SCONE, with +1.8% to +4.4% improvement in accuracy.","source":"http:\/\/arxiv.org\/pdf\/1810.06683","authors":["Hsin-Yuan Huang","Eunsol Choi","Wen-tau Yih"],"categories":["cs.CL","cs.AI"],"published":"20181006","updated":"20190416","primary_category":"cs.CL"}
{"arxiv_id":"2210.02414","title":"GLM-130B: An Open Bilingual Pre-trained Model","abstract":"We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https:\/\/github.com\/THUDM\/GLM-130B .","source":"http:\/\/arxiv.org\/pdf\/2210.02414","authors":["Aohan Zeng","Xiao Liu","Zhengxiao Du","Zihan Wang","Hanyu Lai","Ming Ding","Zhuoyi Yang","Yifan Xu","Wendi Zheng","Xiao Xia","Weng Lam Tam","Zixuan Ma","Yufei Xue","Jidong Zhai","Wenguang Chen","Peng Zhang","Yuxiao Dong","Jie Tang"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20221005","updated":"20221005","primary_category":"cs.CL"}
{"arxiv_id":"1909.05858","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation","abstract":"Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https:\/\/github.com\/salesforce\/ctrl.","source":"http:\/\/arxiv.org\/pdf\/1909.05858","authors":["Nitish Shirish Keskar","Bryan McCann","Lav R. Varshney","Caiming Xiong","Richard Socher"],"categories":["cs.CL"],"published":"20190911","updated":"20190920","primary_category":"cs.CL"}
{"arxiv_id":"2104.08773","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions","abstract":"Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.","source":"http:\/\/arxiv.org\/pdf\/2104.08773","authors":["Swaroop Mishra","Daniel Khashabi","Chitta Baral","Hannaneh Hajishirzi"],"categories":["cs.CL","cs.AI","cs.CV","cs.LG"],"published":"20210418","updated":"20220314","primary_category":"cs.CL"}
{"arxiv_id":"2104.07567","title":"Retrieval Augmentation Reduces Hallucination in Conversation","abstract":"Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots.","source":"http:\/\/arxiv.org\/pdf\/2104.07567","authors":["Kurt Shuster","Spencer Poff","Moya Chen","Douwe Kiela","Jason Weston"],"categories":["cs.CL","cs.AI"],"published":"20210415","updated":"20210415","primary_category":"cs.CL"}
{"arxiv_id":"2109.09193","title":"Towards Zero-Label Language Learning","abstract":"This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of few-shot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages few-shot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-specific models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark.","source":"http:\/\/arxiv.org\/pdf\/2109.09193","authors":["Zirui Wang","Adams Wei Yu","Orhan Firat","Yuan Cao"],"categories":["cs.CL","cs.LG"],"published":"20210919","updated":"20210919","primary_category":"cs.CL"}
{"arxiv_id":"1710.10368","title":"Deep Generative Dual Memory Network for Continual Learning","abstract":"Despite advances in deep learning, neural networks can only learn multiple tasks when trained on them jointly. When tasks arrive sequentially, they lose performance on previously learnt tasks. This phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our contributions are: (i) a dual memory architecture emulating the complementary learning systems (hippocampus and the neocortex) in the human brain, (ii) memory consolidation via generative replay of past experiences, (iii) demonstrating advantages of generative replay and dual memories via experiments, and (iv) improved performance retention on challenging tasks even for low capacity models. Our architecture displays many characteristics of the mammalian memory and provides insights on the connection between sleep and learning.","source":"http:\/\/arxiv.org\/pdf\/1710.10368","authors":["Nitin Kamra","Umang Gupta","Yan Liu"],"categories":["cs.LG"],"published":"20171028","updated":"20180525","primary_category":"cs.LG"}
{"arxiv_id":"2102.09690","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models","abstract":"GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N\/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.","source":"http:\/\/arxiv.org\/pdf\/2102.09690","authors":["Tony Z. Zhao","Eric Wallace","Shi Feng","Dan Klein","Sameer Singh"],"categories":["cs.CL","cs.LG"],"published":"20210219","updated":"20210610","primary_category":"cs.CL"}
{"arxiv_id":"1811.00720","title":"Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems","abstract":"Solving math word problems is a challenging task that requires accurate natural language understanding to bridge natural language texts and math expressions. Motivated by the intuition about how human generates the equations given the problem texts, this paper presents a neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in texts. This paper views the process of generating equation as a bridge between the semantic world and the symbolic world, where the proposed neural math solver is based on an encoder-decoder framework. In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next. The preliminary experiments are conducted in a dataset Math23K, and our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10% accuracy, demonstrating the effectiveness of bridging the symbolic and semantic worlds from math word problems.","source":"http:\/\/arxiv.org\/pdf\/1811.00720","authors":["Ting-Rui Chiang","Yun-Nung Chen"],"categories":["cs.CL"],"published":"20181102","updated":"20190609","primary_category":"cs.CL"}
{"arxiv_id":"1808.05326","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","abstract":"Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.","source":"http:\/\/arxiv.org\/pdf\/1808.05326","authors":["Rowan Zellers","Yonatan Bisk","Roy Schwartz","Yejin Choi"],"categories":["cs.CL"],"published":"20180816","updated":"20180816","primary_category":"cs.CL"}
{"arxiv_id":"2212.10560","title":"Self-Instruct: Aligning Language Models with Self-Generated Instructions","abstract":"Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https:\/\/github.com\/yizhongw\/self-instruct.","source":"http:\/\/arxiv.org\/pdf\/2212.10560","authors":["Yizhong Wang","Yeganeh Kordi","Swaroop Mishra","Alisa Liu","Noah A. Smith","Daniel Khashabi","Hannaneh Hajishirzi"],"categories":["cs.CL","cs.AI"],"published":"20221220","updated":"20230525","primary_category":"cs.CL"}
{"arxiv_id":"1905.12616","title":"Defending Against Neural Fake News","abstract":"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.","source":"http:\/\/arxiv.org\/pdf\/1905.12616","authors":["Rowan Zellers","Ari Holtzman","Hannah Rashkin","Yonatan Bisk","Ali Farhadi","Franziska Roesner","Yejin Choi"],"categories":["cs.CL","cs.CY"],"published":"20190529","updated":"20201211","primary_category":"cs.CL"}
{"arxiv_id":"2211.09260","title":"Task-aware Retrieval with Instructions","abstract":"We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.","source":"http:\/\/arxiv.org\/pdf\/2211.09260","authors":["Akari Asai","Timo Schick","Patrick Lewis","Xilun Chen","Gautier Izacard","Sebastian Riedel","Hannaneh Hajishirzi","Wen-tau Yih"],"categories":["cs.CL"],"published":"20221116","updated":"20221219","primary_category":"cs.CL"}
{"arxiv_id":"1906.01604","title":"KERMIT: Generative Insertion-Based Modeling for Sequences","abstract":"We present KERMIT, a simple insertion-based approach to generative modeling for sequences and sequence pairs. KERMIT models the joint distribution and its decompositions (i.e., marginals and conditionals) using a single neural network and, unlike much prior work, does not rely on a prespecified factorization of the data distribution. During training, one can feed KERMIT paired data $(x, y)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired data $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we have access to the conditionals $p(x \\mid y)$ and $p(y \\mid x)$ in both directions. We can also sample from the joint distribution or the marginals. The model supports both serial fully autoregressive decoding and parallel partially autoregressive decoding, with the latter exhibiting an empirically logarithmic runtime. We demonstrate through experiments in machine translation, representation learning, and zero-shot cloze question answering that our unified approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems across a wide range of tasks without the need for problem-specific architectural adaptation.","source":"http:\/\/arxiv.org\/pdf\/1906.01604","authors":["William Chan","Nikita Kitaev","Kelvin Guu","Mitchell Stern","Jakob Uszkoreit"],"categories":["cs.CL","cs.LG","stat.ML"],"published":"20190604","updated":"20190604","primary_category":"cs.CL"}
{"arxiv_id":"2304.03277","title":"Instruction Tuning with GPT-4","abstract":"Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.","source":"http:\/\/arxiv.org\/pdf\/2304.03277","authors":["Baolin Peng","Chunyuan Li","Pengcheng He","Michel Galley","Jianfeng Gao"],"categories":["cs.CL","cs.AI"],"published":"20230406","updated":"20230406","primary_category":"cs.CL"}
{"arxiv_id":"1904.00962","title":"Large Batch Optimization for Deep Learning: Training BERT in 76 minutes","abstract":"Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/optimizers\/lamb.py","source":"http:\/\/arxiv.org\/pdf\/1904.00962","authors":["Yang You","Jing Li","Sashank Reddi","Jonathan Hseu","Sanjiv Kumar","Srinadh Bhojanapalli","Xiaodan Song","James Demmel","Kurt Keutzer","Cho-Jui Hsieh"],"categories":["cs.LG","cs.AI","cs.CL","stat.ML"],"published":"20190401","updated":"20200103","primary_category":"cs.LG"}
{"arxiv_id":"1810.08810","title":"The Frontiers of Fairness in Machine Learning","abstract":"The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.","source":"http:\/\/arxiv.org\/pdf\/1810.08810","authors":["Alexandra Chouldechova","Aaron Roth"],"categories":["cs.LG","cs.DS","cs.GT","stat.ML"],"published":"20181020","updated":"20181020","primary_category":"cs.LG"}
{"arxiv_id":"1905.07830","title":"HellaSwag: Can a Machine Really Finish Your Sentence?","abstract":"Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.","source":"http:\/\/arxiv.org\/pdf\/1905.07830","authors":["Rowan Zellers","Ari Holtzman","Yonatan Bisk","Ali Farhadi","Yejin Choi"],"categories":["cs.CL"],"published":"20190519","updated":"20190519","primary_category":"cs.CL"}
{"arxiv_id":"2009.11462","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models","abstract":"Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning \"bad\" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.","source":"http:\/\/arxiv.org\/pdf\/2009.11462","authors":["Samuel Gehman","Suchin Gururangan","Maarten Sap","Yejin Choi","Noah A. Smith"],"categories":["cs.CL"],"published":"20200924","updated":"20200925","primary_category":"cs.CL"}
{"arxiv_id":"2109.03034","title":"Generate & Rank: A Multi-task Framework for Math Word Problems","abstract":"Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% $\\rightarrow$ 85.4%) higher than the state-of-the-art.","source":"http:\/\/arxiv.org\/pdf\/2109.03034","authors":["Jianhao Shen","Yichun Yin","Lin Li","Lifeng Shang","Xin Jiang","Ming Zhang","Qun Liu"],"categories":["cs.CL","cs.AI"],"published":"20210907","updated":"20210907","primary_category":"cs.CL"}
{"arxiv_id":"1910.01108","title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","abstract":"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and\/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.","source":"http:\/\/arxiv.org\/pdf\/1910.01108","authors":["Victor Sanh","Lysandre Debut","Julien Chaumond","Thomas Wolf"],"categories":["cs.CL"],"published":"20191002","updated":"20200301","primary_category":"cs.CL"}
{"arxiv_id":"2109.10862","title":"Recursively Summarizing Books with Human Feedback","abstract":"A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\\sim5\\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.","source":"http:\/\/arxiv.org\/pdf\/2109.10862","authors":["Jeff Wu","Long Ouyang","Daniel M. Ziegler","Nisan Stiennon","Ryan Lowe","Jan Leike","Paul Christiano"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20210922","updated":"20210927","primary_category":"cs.CL"}
{"arxiv_id":"2005.04305","title":"Measuring the Algorithmic Efficiency of Neural Networks","abstract":"Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.","source":"http:\/\/arxiv.org\/pdf\/2005.04305","authors":["Danny Hernandez","Tom B. Brown"],"categories":["cs.LG","cs.CV","stat.ML"],"published":"20200508","updated":"20200508","primary_category":"cs.LG"}
{"arxiv_id":"2103.12407","title":"Detecting Hate Speech with GPT-3","abstract":"Sophisticated language models such as OpenAI's GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model's accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.","source":"http:\/\/arxiv.org\/pdf\/2103.12407","authors":["Ke-Li Chiu","Annie Collins","Rohan Alexander"],"categories":["cs.CL"],"published":"20210323","updated":"20220324","primary_category":"cs.CL"}
{"arxiv_id":"1506.05869","title":"A Neural Conversational Model","abstract":"Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.","source":"http:\/\/arxiv.org\/pdf\/1506.05869","authors":["Oriol Vinyals","Quoc Le"],"categories":["cs.CL"],"published":"20150619","updated":"20150722","primary_category":"cs.CL"}
{"arxiv_id":"2302.08582","title":"Pretraining Language Models with Human Preferences","abstract":"Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.","source":"http:\/\/arxiv.org\/pdf\/2302.08582","authors":["Tomasz Korbak","Kejian Shi","Angelica Chen","Rasika Bhalerao","Christopher L. Buckley","Jason Phang","Samuel R. Bowman","Ethan Perez"],"categories":["cs.CL","cs.LG"],"published":"20230216","updated":"20230614","primary_category":"cs.CL"}
{"arxiv_id":"2010.13002","title":"Pre-trained Summarization Distillation","abstract":"Recent state-of-the-art approaches to summarization utilize large pre-trained Transformer models. Distilling these models to smaller student models has become critically important for practical use; however there are many different distillation methods proposed by the NLP literature. Recent work on distilling BERT for classification and regression tasks shows strong performance using direct knowledge distillation. Alternatively, machine translation practitioners distill using pseudo-labeling, where a small model is trained on the translations of a larger model. A third, simpler approach is to 'shrink and fine-tune' (SFT), which avoids any explicit distillation by copying parameters to a smaller student model and then fine-tuning. We compare these three approaches for distillation of Pegasus and BART, the current and former state of the art, pre-trained summarization models, and find that SFT outperforms knowledge distillation and pseudo-labeling on the CNN\/DailyMail dataset, but under-performs pseudo-labeling on the more abstractive XSUM dataset. PyTorch Code and checkpoints of different sizes are available through Hugging Face transformers here http:\/\/tiny.cc\/4iy0tz.","source":"http:\/\/arxiv.org\/pdf\/2010.13002","authors":["Sam Shleifer","Alexander M. Rush"],"categories":["cs.CL","cs.AI"],"published":"20201024","updated":"20201028","primary_category":"cs.CL"}
{"arxiv_id":"1712.00409","title":"Deep Learning Scaling is Predictable, Empirically","abstract":"Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.","source":"http:\/\/arxiv.org\/pdf\/1712.00409","authors":["Joel Hestness","Sharan Narang","Newsha Ardalani","Gregory Diamos","Heewoo Jun","Hassan Kianinejad","Md. Mostofa Ali Patwary","Yang Yang","Yanqi Zhou"],"categories":["cs.LG","stat.ML"],"published":"20171201","updated":"20171201","primary_category":"cs.LG"}
{"arxiv_id":"2212.09689","title":"Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor","abstract":"Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.","source":"http:\/\/arxiv.org\/pdf\/2212.09689","authors":["Or Honovich","Thomas Scialom","Omer Levy","Timo Schick"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20221219","updated":"20221219","primary_category":"cs.CL"}
{"arxiv_id":"2204.07705","title":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks","abstract":"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.","source":"http:\/\/arxiv.org\/pdf\/2204.07705","authors":["Yizhong Wang","Swaroop Mishra","Pegah Alipoormolabashi","Yeganeh Kordi","Amirreza Mirzaei","Anjana Arunkumar","Arjun Ashok","Arut Selvan Dhanasekaran","Atharva Naik","David Stap","Eshaan Pathak","Giannis Karamanolakis","Haizhi Gary Lai","Ishan Purohit","Ishani Mondal","Jacob Anderson","Kirby Kuznia","Krima Doshi","Maitreya Patel","Kuntal Kumar Pal","Mehrad Moradshahi","Mihir Parmar","Mirali Purohit","Neeraj Varshney","Phani Rohitha Kaza","Pulkit Verma","Ravsehaj Singh Puri","Rushang Karia","Shailaja Keyur Sampat","Savan Doshi","Siddhartha Mishra","Sujan Reddy","Sumanta Patro","Tanay Dixit","Xudong Shen","Chitta Baral","Yejin Choi","Noah A. Smith","Hannaneh Hajishirzi","Daniel Khashabi"],"categories":["cs.CL","cs.AI"],"published":"20220416","updated":"20221024","primary_category":"cs.CL"}
{"arxiv_id":"2211.01910","title":"Large Language Models Are Human-Level Prompt Engineers","abstract":"By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19\/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and\/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https:\/\/sites.google.com\/view\/automatic-prompt-engineer.","source":"http:\/\/arxiv.org\/pdf\/2211.01910","authors":["Yongchao Zhou","Andrei Ioan Muresanu","Ziwen Han","Keiran Paster","Silviu Pitis","Harris Chan","Jimmy Ba"],"categories":["cs.LG","cs.AI","cs.CL"],"published":"20221103","updated":"20230310","primary_category":"cs.LG"}
{"arxiv_id":"2302.07842","title":"Augmented Language Models: a Survey","abstract":"This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.","source":"http:\/\/arxiv.org\/pdf\/2302.07842","authors":["Gr\u00e9goire Mialon","Roberto Dess\u00ec","Maria Lomeli","Christoforos Nalmpantis","Ram Pasunuru","Roberta Raileanu","Baptiste Rozi\u00e8re","Timo Schick","Jane Dwivedi-Yu","Asli Celikyilmaz","Edouard Grave","Yann LeCun","Thomas Scialom"],"categories":["cs.CL"],"published":"20230215","updated":"20230215","primary_category":"cs.CL"}
{"arxiv_id":"2208.03299","title":"Atlas: Few-shot Learning with Retrieval Augmented Language Models","abstract":"Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.","source":"http:\/\/arxiv.org\/pdf\/2208.03299","authors":["Gautier Izacard","Patrick Lewis","Maria Lomeli","Lucas Hosseini","Fabio Petroni","Timo Schick","Jane Dwivedi-Yu","Armand Joulin","Sebastian Riedel","Edouard Grave"],"categories":["cs.CL"],"published":"20220805","updated":"20221116","primary_category":"cs.CL"}
{"arxiv_id":"2012.11635","title":"A Distributional Approach to Controlled Text Generation","abstract":"We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both \"pointwise\" and \"distributional\" constraints over the target LM -- to our knowledge, the first model with such generality -- while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. (Code available at https:\/\/github.com\/naver\/gdc)","source":"http:\/\/arxiv.org\/pdf\/2012.11635","authors":["Muhammad Khalifa","Hady Elsahar","Marc Dymetman"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20201221","updated":"20210506","primary_category":"cs.CL"}
{"arxiv_id":"2112.11446","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher","abstract":"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.","source":"http:\/\/arxiv.org\/pdf\/2112.11446","authors":["Jack W. Rae","Sebastian Borgeaud","Trevor Cai","Katie Millican","Jordan Hoffmann","Francis Song","John Aslanides","Sarah Henderson","Roman Ring","Susannah Young","Eliza Rutherford","Tom Hennigan","Jacob Menick","Albin Cassirer","Richard Powell","George van den Driessche","Lisa Anne Hendricks","Maribeth Rauh","Po-Sen Huang","Amelia Glaese","Johannes Welbl","Sumanth Dathathri","Saffron Huang","Jonathan Uesato","John Mellor","Irina Higgins","Antonia Creswell","Nat McAleese","Amy Wu","Erich Elsen","Siddhant Jayakumar","Elena Buchatskaya","David Budden","Esme Sutherland","Karen Simonyan","Michela Paganini","Laurent Sifre","Lena Martens","Xiang Lorraine Li","Adhiguna Kuncoro","Aida Nematzadeh","Elena Gribovskaya","Domenic Donato","Angeliki Lazaridou","Arthur Mensch","Jean-Baptiste Lespiau","Maria Tsimpoukelli","Nikolai Grigorev","Doug Fritz","Thibault Sottiaux","Mantas Pajarskas","Toby Pohlen","Zhitao Gong","Daniel Toyama","Cyprien de Masson d'Autume","Yujia Li","Tayfun Terzi","Vladimir Mikulik","Igor Babuschkin","Aidan Clark","Diego de Las Casas","Aurelia Guy","Chris Jones","James Bradbury","Matthew Johnson","Blake Hechtman","Laura Weidinger","Iason Gabriel","William Isaac","Ed Lockhart","Simon Osindero","Laura Rimell","Chris Dyer","Oriol Vinyals","Kareem Ayoub","Jeff Stanway","Lorrayne Bennett","Demis Hassabis","Koray Kavukcuoglu","Geoffrey Irving"],"categories":["cs.CL","cs.AI"],"published":"20211208","updated":"20220121","primary_category":"cs.CL"}
{"arxiv_id":"2105.04054","title":"Societal Biases in Language Generation: Progress and Challenges","abstract":"Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.","source":"http:\/\/arxiv.org\/pdf\/2105.04054","authors":["Emily Sheng","Kai-Wei Chang","Premkumar Natarajan","Nanyun Peng"],"categories":["cs.CL"],"published":"20210510","updated":"20210622","primary_category":"cs.CL"}
{"arxiv_id":"1909.12673","title":"A Constructive Prediction of the Generalization Error Across Scales","abstract":"The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model\/data scales. Our construction follows insights obtained from observations conducted over a range of model\/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.","source":"http:\/\/arxiv.org\/pdf\/1909.12673","authors":["Jonathan S. Rosenfeld","Amir Rosenfeld","Yonatan Belinkov","Nir Shavit"],"categories":["cs.LG","cs.CL","cs.CV","stat.ML"],"published":"20190927","updated":"20191220","primary_category":"cs.LG"}
{"arxiv_id":"2201.11990","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model","abstract":"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.","source":"http:\/\/arxiv.org\/pdf\/2201.11990","authors":["Shaden Smith","Mostofa Patwary","Brandon Norick","Patrick LeGresley","Samyam Rajbhandari","Jared Casper","Zhun Liu","Shrimai Prabhumoye","George Zerveas","Vijay Korthikanti","Elton Zhang","Rewon Child","Reza Yazdani Aminabadi","Julie Bernauer","Xia Song","Mohammad Shoeybi","Yuxiong He","Michael Houston","Saurabh Tiwary","Bryan Catanzaro"],"categories":["cs.CL"],"published":"20220128","updated":"20220204","primary_category":"cs.CL"}
{"arxiv_id":"2212.08410","title":"Teaching Small Language Models to Reason","abstract":"Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.","source":"http:\/\/arxiv.org\/pdf\/2212.08410","authors":["Lucie Charlotte Magister","Jonathan Mallinson","Jakub Adamek","Eric Malmi","Aliaksei Severyn"],"categories":["cs.CL","cs.LG"],"published":"20221216","updated":"20230601","primary_category":"cs.CL"}
{"arxiv_id":"2111.10952","title":"ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning","abstract":"Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.","source":"http:\/\/arxiv.org\/pdf\/2111.10952","authors":["Vamsi Aribandi","Yi Tay","Tal Schuster","Jinfeng Rao","Huaixiu Steven Zheng","Sanket Vaibhav Mehta","Honglei Zhuang","Vinh Q. Tran","Dara Bahri","Jianmo Ni","Jai Gupta","Kai Hui","Sebastian Ruder","Donald Metzler"],"categories":["cs.CL","cs.LG"],"published":"20211122","updated":"20220129","primary_category":"cs.CL"}
{"arxiv_id":"2107.03451","title":"Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling","abstract":"Over the last several years, end-to-end neural conversational agents have vastly improved in their ability to carry a chit-chat conversation with humans. However, these models are often trained on large datasets from the internet, and as a result, may learn undesirable behaviors from this data, such as toxic or otherwise harmful language. Researchers must thus wrestle with the issue of how and when to release these models. In this paper, we survey the problem landscape for safety for end-to-end conversational AI and discuss recent and related work. We highlight tensions between values, potential positive impact and potential harms, and provide a framework for making decisions about whether and how to release these models, following the tenets of value-sensitive design. We additionally provide a suite of tools to enable researchers to make better-informed decisions about training and releasing end-to-end conversational AI models.","source":"http:\/\/arxiv.org\/pdf\/2107.03451","authors":["Emily Dinan","Gavin Abercrombie","A. Stevie Bergman","Shannon Spruit","Dirk Hovy","Y-Lan Boureau","Verena Rieser"],"categories":["cs.CL","cs.AI"],"published":"20210707","updated":"20210723","primary_category":"cs.CL"}
{"arxiv_id":"2012.15562","title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts","abstract":"Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model's embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT's and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.","source":"http:\/\/arxiv.org\/pdf\/2012.15562","authors":["Jonas Pfeiffer","Ivan Vuli\u0107","Iryna Gurevych","Sebastian Ruder"],"categories":["cs.CL"],"published":"20201231","updated":"20210910","primary_category":"cs.CL"}
{"arxiv_id":"1708.06733","title":"BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain","abstract":"Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \\emph{BadNet}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of {25}\\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.","source":"http:\/\/arxiv.org\/pdf\/1708.06733","authors":["Tianyu Gu","Brendan Dolan-Gavitt","Siddharth Garg"],"categories":["cs.CR","cs.LG"],"published":"20170822","updated":"20190311","primary_category":"cs.CR"}
{"arxiv_id":"2210.11416","title":"Scaling Instruction-Finetuned Language Models","abstract":"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.","source":"http:\/\/arxiv.org\/pdf\/2210.11416","authors":["Hyung Won Chung","Le Hou","Shayne Longpre","Barret Zoph","Yi Tay","William Fedus","Yunxuan Li","Xuezhi Wang","Mostafa Dehghani","Siddhartha Brahma","Albert Webson","Shixiang Shane Gu","Zhuyun Dai","Mirac Suzgun","Xinyun Chen","Aakanksha Chowdhery","Alex Castro-Ros","Marie Pellat","Kevin Robinson","Dasha Valter","Sharan Narang","Gaurav Mishra","Adams Yu","Vincent Zhao","Yanping Huang","Andrew Dai","Hongkun Yu","Slav Petrov","Ed H. Chi","Jeff Dean","Jacob Devlin","Adam Roberts","Denny Zhou","Quoc V. Le","Jason Wei"],"categories":["cs.LG","cs.CL"],"published":"20221020","updated":"20221206","primary_category":"cs.LG"}
{"arxiv_id":"2012.07805","title":"Extracting Training Data from Large Language Models","abstract":"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.","source":"http:\/\/arxiv.org\/pdf\/2012.07805","authors":["Nicholas Carlini","Florian Tramer","Eric Wallace","Matthew Jagielski","Ariel Herbert-Voss","Katherine Lee","Adam Roberts","Tom Brown","Dawn Song","Ulfar Erlingsson","Alina Oprea","Colin Raffel"],"categories":["cs.CR","cs.CL","cs.LG"],"published":"20201214","updated":"20210615","primary_category":"cs.CR"}
{"arxiv_id":"1808.04409","title":"Thou shalt not hate: Countering Online Hate Speech","abstract":"Hate content in social media is ever-increasing. While Facebook, Twitter, Google have attempted to take several steps to tackle the hateful content, they have mostly been unsuccessful. Counterspeech is seen as an effective way of tackling the online hate without any harm to the freedom of speech. Thus, an alternative strategy for these platforms could be to promote counterspeech as a defense against hate content. However, in order to have a successful promotion of such counterspeech, one has to have a deep understanding of its dynamics in the online world. Lack of carefully curated data largely inhibits such understanding. In this paper, we create and release the first ever dataset for counterspeech using comments from YouTube. The data contains 13,924 manually annotated comments where the labels indicate whether a comment is a counterspeech or not. This data allows us to perform a rigorous measurement study characterizing the linguistic structure of counterspeech for the first time. This analysis results in various interesting insights such as: the counterspeech comments receive much more likes as compared to the non-counterspeech comments, for certain communities majority of the non-counterspeech comments tend to be hate speech, the different types of counterspeech are not all equally effective and the language choice of users posting counterspeech is largely different from those posting non-counterspeech as revealed by a detailed psycholinguistic analysis. Finally, we build a set of machine learning models that are able to automatically detect counterspeech in YouTube videos with an F1-score of 0.71. We also build multilabel models that can detect different types of counterspeech in a comment with an F1-score of 0.60.","source":"http:\/\/arxiv.org\/pdf\/1808.04409","authors":["Binny Mathew","Punyajoy Saha","Hardik Tharad","Subham Rajgaria","Prajwal Singhania","Suman Kalyan Maity","Pawan Goyal","Animesh Mukherje"],"categories":["cs.SI"],"published":"20180813","updated":"20190404","primary_category":"cs.SI"}
{"arxiv_id":"2203.11147","title":"Teaching language models to support answers with verified quotes","abstract":"Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train \"open-book\" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.","source":"http:\/\/arxiv.org\/pdf\/2203.11147","authors":["Jacob Menick","Maja Trebacz","Vladimir Mikulik","John Aslanides","Francis Song","Martin Chadwick","Mia Glaese","Susannah Young","Lucy Campbell-Gillingham","Geoffrey Irving","Nat McAleese"],"categories":["cs.CL","cs.LG"],"published":"20220321","updated":"20220321","primary_category":"cs.CL"}
{"arxiv_id":"1710.06481","title":"Constructing Datasets for Multi-hop Reading Comprehension Across Documents","abstract":"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence - effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% - leaving ample room for improvement.","source":"http:\/\/arxiv.org\/pdf\/1710.06481","authors":["Johannes Welbl","Pontus Stenetorp","Sebastian Riedel"],"categories":["cs.CL","cs.AI"],"published":"20171017","updated":"20180611","primary_category":"cs.CL"}
{"arxiv_id":"2203.14465","title":"STaR: Bootstrapping Reasoning With Reasoning","abstract":"Generating step-by-step \"chain-of-thought\" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.","source":"http:\/\/arxiv.org\/pdf\/2203.14465","authors":["Eric Zelikman","Yuhuai Wu","Jesse Mu","Noah D. Goodman"],"categories":["cs.LG","cs.AI","cs.CL"],"published":"20220328","updated":"20220520","primary_category":"cs.LG"}
{"arxiv_id":"1612.03651","title":"FastText.zip: Compressing text classification models","abstract":"We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.","source":"http:\/\/arxiv.org\/pdf\/1612.03651","authors":["Armand Joulin","Edouard Grave","Piotr Bojanowski","Matthijs Douze","H\u00e9rve J\u00e9gou","Tomas Mikolov"],"categories":["cs.CL","cs.LG"],"published":"20161212","updated":"20161212","primary_category":"cs.CL"}
{"arxiv_id":"2102.05610","title":"Searching for Fast Model Families on Datacenter Accelerators","abstract":"Neural Architecture Search (NAS), together with model scaling, has shown remarkable progress in designing high accuracy and fast convolutional architecture families. However, as neither NAS nor model scaling considers sufficient hardware architecture details, they do not take full advantage of the emerging datacenter (DC) accelerators. In this paper, we search for fast and accurate CNN model families for efficient inference on DC accelerators. We first analyze DC accelerators and find that existing CNNs suffer from insufficient operational intensity, parallelism, and execution efficiency. These insights let us create a DC-accelerator-optimized search space, with space-to-depth, space-to-batch, hybrid fused convolution structures with vanilla and depthwise convolutions, and block-wise activation functions. On top of our DC accelerator optimized neural architecture search space, we further propose a latency-aware compound scaling (LACS), the first multi-objective compound scaling method optimizing both accuracy and latency. Our LACS discovers that network depth should grow much faster than image size and network width, which is quite different from previous compound scaling results. With the new search space and LACS, our search and scaling on datacenter accelerators results in a new model series named EfficientNet-X. EfficientNet-X is up to more than 2X faster than EfficientNet (a model series with state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with comparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet and ResNeSt on TPUv3 and GPUv100.","source":"http:\/\/arxiv.org\/pdf\/2102.05610","authors":["Sheng Li","Mingxing Tan","Ruoming Pang","Andrew Li","Liqun Cheng","Quoc Le","Norman P. Jouppi"],"categories":["cs.CV","eess.IV"],"published":"20210210","updated":"20210210","primary_category":"cs.CV"}
{"arxiv_id":"2103.14659","title":"Alignment of Language Agents","abstract":"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.","source":"http:\/\/arxiv.org\/pdf\/2103.14659","authors":["Zachary Kenton","Tom Everitt","Laura Weidinger","Iason Gabriel","Vladimir Mikulik","Geoffrey Irving"],"categories":["cs.AI","cs.LG"],"published":"20210326","updated":"20210326","primary_category":"cs.AI"}
{"arxiv_id":"2102.09692","title":"To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making","abstract":"People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.","source":"http:\/\/arxiv.org\/pdf\/2102.09692","authors":["Zana Bu\u00e7inca","Maja Barbara Malaya","Krzysztof Z. Gajos"],"categories":["cs.HC","cs.AI"],"published":"20210219","updated":"20210219","primary_category":"cs.HC"}
{"arxiv_id":"2304.07327","title":"OpenAssistant Conversations -- Democratizing Large Language Model Alignment","abstract":"Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7% respectively. We release our code and data under fully permissive licenses.","source":"http:\/\/arxiv.org\/pdf\/2304.07327","authors":["Andreas K\u00f6pf","Yannic Kilcher","Dimitri von R\u00fctte","Sotiris Anagnostidis","Zhi-Rui Tam","Keith Stevens","Abdullah Barhoum","Nguyen Minh Duc","Oliver Stanley","Rich\u00e1rd Nagyfi","Shahul ES","Sameer Suri","David Glushkov","Arnav Dantuluri","Andrew Maguire","Christoph Schuhmann","Huu Nguyen","Alexander Mattick"],"categories":["cs.CL","cs.AI","I.2"],"published":"20230414","updated":"20230414","primary_category":"cs.CL"}
{"arxiv_id":"2210.02875","title":"Binding Language Models in Symbolic Languages","abstract":"Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https:\/\/github.com\/HKUNLP\/Binder .","source":"http:\/\/arxiv.org\/pdf\/2210.02875","authors":["Zhoujun Cheng","Tianbao Xie","Peng Shi","Chengzu Li","Rahul Nadkarni","Yushi Hu","Caiming Xiong","Dragomir Radev","Mari Ostendorf","Luke Zettlemoyer","Noah A. Smith","Tao Yu"],"categories":["cs.CL"],"published":"20221006","updated":"20230301","primary_category":"cs.CL"}
{"arxiv_id":"2004.03705","title":"Deep Learning Based Text Classification: A Comprehensive Review","abstract":"Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.","source":"http:\/\/arxiv.org\/pdf\/2004.03705","authors":["Shervin Minaee","Nal Kalchbrenner","Erik Cambria","Narjes Nikzad","Meysam Chenaghlu","Jianfeng Gao"],"categories":["cs.CL","cs.LG","stat.ML"],"published":"20200406","updated":"20210104","primary_category":"cs.CL"}
{"arxiv_id":"2204.11117","title":"Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning","abstract":"Recent work has found that multi-task training with a large number of diverse tasks can uniformly improve downstream performance on unseen target tasks. In contrast, literature on task transferability has established that the choice of intermediate tasks can heavily affect downstream task performance. In this work, we aim to disentangle the effect of scale and relatedness of tasks in multi-task representation learning. We find that, on average, increasing the scale of multi-task learning, in terms of the number of tasks, indeed results in better learned representations than smaller multi-task setups. However, if the target tasks are known ahead of time, then training on a smaller set of related tasks is competitive to the large-scale multi-task training at a reduced computational cost.","source":"http:\/\/arxiv.org\/pdf\/2204.11117","authors":["Vishakh Padmakumar","Leonard Lausen","Miguel Ballesteros","Sheng Zha","He He","George Karypis"],"categories":["cs.CL","cs.LG"],"published":"20220423","updated":"20220712","primary_category":"cs.CL"}
{"arxiv_id":"1606.06737","title":"Criticality in Formal Languages and Statistical Physics","abstract":"We show that the mutual information between two symbols, as a function of the number of symbols between the two, decays exponentially in any probabilistic regular grammar, but can decay like a power law for a context-free grammar. This result about formal languages is closely related to a well-known result in classical statistical mechanics that there are no phase transitions in dimensions fewer than two. It is also related to the emergence of power-law correlations in turbulence and cosmological inflation through recursive generative processes. We elucidate these physics connections and comment on potential applications of our results to machine learning tasks like training artificial recurrent neural networks. Along the way, we introduce a useful quantity which we dub the rational mutual information and discuss generalizations of our claims involving more complicated Bayesian networks.","source":"http:\/\/arxiv.org\/pdf\/1606.06737","authors":["Henry W. Lin","Max Tegmark"],"categories":["cond-mat.dis-nn","cs.CL"],"published":"20160621","updated":"20170823","primary_category":"cs.CL"}
{"arxiv_id":"1706.07269","title":"Explanation in Artificial Intelligence: Insights from the Social Sciences","abstract":"There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology\/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.","source":"http:\/\/arxiv.org\/pdf\/1706.07269","authors":["Tim Miller"],"categories":["cs.AI"],"published":"20170622","updated":"20180815","primary_category":"cs.AI"}
{"arxiv_id":"2205.12393","title":"Fine-tuned Language Models are Continual Learners","abstract":"Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions. Language models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning we show that Language Models can be continual learners. We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn diverse new tasks, while still maintaining good performance on previous tasks, spanning remarkably through 70 datasets in total. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some compositionality.","source":"http:\/\/arxiv.org\/pdf\/2205.12393","authors":["Thomas Scialom","Tuhin Chakrabarty","Smaranda Muresan"],"categories":["cs.CL"],"published":"20220524","updated":"20221029","primary_category":"cs.CL"}
{"arxiv_id":"1506.02438","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation","abstract":"Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","source":"http:\/\/arxiv.org\/pdf\/1506.02438","authors":["John Schulman","Philipp Moritz","Sergey Levine","Michael Jordan","Pieter Abbeel"],"categories":["cs.LG","cs.RO","cs.SY"],"published":"20150608","updated":"20181020","primary_category":"cs.LG"}
{"arxiv_id":"1907.11692","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","abstract":"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.","source":"http:\/\/arxiv.org\/pdf\/1907.11692","authors":["Yinhan Liu","Myle Ott","Naman Goyal","Jingfei Du","Mandar Joshi","Danqi Chen","Omer Levy","Mike Lewis","Luke Zettlemoyer","Veselin Stoyanov"],"categories":["cs.CL"],"published":"20190726","updated":"20190726","primary_category":"cs.CL"}
{"arxiv_id":"2102.07988","title":"TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models","abstract":"Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https:\/\/github.com\/zhuohan123\/terapipe","source":"http:\/\/arxiv.org\/pdf\/2102.07988","authors":["Zhuohan Li","Siyuan Zhuang","Shiyuan Guo","Danyang Zhuo","Hao Zhang","Dawn Song","Ion Stoica"],"categories":["cs.LG","cs.CL","cs.DC"],"published":"20210216","updated":"20210928","primary_category":"cs.LG"}
{"arxiv_id":"2210.06245","title":"Back to the Future: On Potential Histories in NLP","abstract":"Machine learning and NLP require the construction of datasets to train and fine-tune models. In this context, previous work has demonstrated the sensitivity of these data sets. For instance, potential societal biases in this data are likely to be encoded and to be amplified in the models we deploy. In this work, we draw from developments in the field of history and take a novel perspective on these problems: considering datasets and models through the lens of historical fiction surfaces their political nature, and affords re-configuring how we view the past, such that marginalized discourses are surfaced. Building on such insights, we argue that contemporary methods for machine learning are prejudiced towards dominant and hegemonic histories. Employing the example of neopronouns, we show that by surfacing marginalized histories within contemporary conditions, we can create models that better represent the lived realities of traditionally marginalized and excluded communities.","source":"http:\/\/arxiv.org\/pdf\/2210.06245","authors":["Zeerak Talat","Anne Lauscher"],"categories":["cs.CL"],"published":"20221012","updated":"20221012","primary_category":"cs.CL"}
{"arxiv_id":"2211.00053","title":"Generating Sequences by Learning to Self-Correct","abstract":"Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.","source":"http:\/\/arxiv.org\/pdf\/2211.00053","authors":["Sean Welleck","Ximing Lu","Peter West","Faeze Brahman","Tianxiao Shen","Daniel Khashabi","Yejin Choi"],"categories":["cs.CL"],"published":"20221031","updated":"20221031","primary_category":"cs.CL"}
{"arxiv_id":"2205.05131","title":"UL2: Unifying Language Learning Paradigms","abstract":"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized & unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 & GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.","source":"http:\/\/arxiv.org\/pdf\/2205.05131","authors":["Yi Tay","Mostafa Dehghani","Vinh Q. Tran","Xavier Garcia","Jason Wei","Xuezhi Wang","Hyung Won Chung","Siamak Shakeri","Dara Bahri","Tal Schuster","Huaixiu Steven Zheng","Denny Zhou","Neil Houlsby","Donald Metzler"],"categories":["cs.CL"],"published":"20220510","updated":"20230228","primary_category":"cs.CL"}
{"arxiv_id":"2212.14882","title":"ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports","abstract":"The release of ChatGPT, a language model capable of generating text that appears human-like and authentic, has gained significant attention beyond the research community. We expect that the convincing performance of ChatGPT incentivizes users to apply it to a variety of downstream tasks, including prompting the model to simplify their own medical reports. To investigate this phenomenon, we conducted an exploratory case study. In a questionnaire, we asked 15 radiologists to assess the quality of radiology reports simplified by ChatGPT. Most radiologists agreed that the simplified reports were factually correct, complete, and not potentially harmful to the patient. Nevertheless, instances of incorrect statements, missed key medical findings, and potentially harmful passages were reported. While further studies are needed, the initial insights of this study indicate a great potential in using large language models like ChatGPT to improve patient-centered care in radiology and other medical domains.","source":"http:\/\/arxiv.org\/pdf\/2212.14882","authors":["Katharina Jeblick","Balthasar Schachtner","Jakob Dexl","Andreas Mittermeier","Anna Theresa St\u00fcber","Johanna Topalis","Tobias Weber","Philipp Wesp","Bastian Sabel","Jens Ricke","Michael Ingrisch"],"categories":["cs.CL","cs.LG"],"published":"20221230","updated":"20221230","primary_category":"cs.CL"}
{"arxiv_id":"2204.05999","title":"InCoder: A Generative Model for Code Infilling and Synthesis","abstract":"Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https:\/\/sites.google.com\/view\/incoder-code-models","source":"http:\/\/arxiv.org\/pdf\/2204.05999","authors":["Daniel Fried","Armen Aghajanyan","Jessy Lin","Sida Wang","Eric Wallace","Freda Shi","Ruiqi Zhong","Wen-tau Yih","Luke Zettlemoyer","Mike Lewis"],"categories":["cs.SE","cs.CL","cs.LG"],"published":"20220412","updated":"20230409","primary_category":"cs.SE"}
{"arxiv_id":"2302.09270","title":"Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey","abstract":"With the development of artificial intelligence, dialogue systems have been endowed with amazing chit-chat capabilities, and there is widespread interest and discussion about whether the generated contents are socially beneficial. In this paper, we present a new perspective of research scope towards building a safe, responsible, and modal dialogue system, including 1) abusive and toxic contents, 2) unfairness and discrimination, 3) ethics and morality issues, and 4) risk of misleading and privacy information. Besides, we review the mainstream methods for evaluating the safety of large models from the perspectives of exposure and detection of safety issues. The recent advances in methodologies for the safety improvement of both end-to-end dialogue systems and pipeline-based models are further introduced. Finally, we discussed six existing challenges towards responsible AI: explainable safety monitoring, continuous learning of safety issues, robustness against malicious attacks, multimodal information processing, unified research framework, and multidisciplinary theory integration. We hope this survey will inspire further research toward safer dialogue systems.","source":"http:\/\/arxiv.org\/pdf\/2302.09270","authors":["Jiawen Deng","Hao Sun","Zhexin Zhang","Jiale Cheng","Minlie Huang"],"categories":["cs.AI"],"published":"20230218","updated":"20230307","primary_category":"cs.AI"}
{"arxiv_id":"2007.02423","title":"Participation is not a Design Fix for Machine Learning","abstract":"This paper critically examines existing modes of participation in design practice and machine learning. Cautioning against 'participation-washing', it suggests that the ML community must become attuned to possibly exploitative and extractive forms of community involvement and shift away from the prerogatives of context-independent scalability.","source":"http:\/\/arxiv.org\/pdf\/2007.02423","authors":["Mona Sloane","Emanuel Moss","Olaitan Awomolo","Laura Forlano"],"categories":["cs.CY","cs.LG"],"published":"20200705","updated":"20200811","primary_category":"cs.CY"}
{"arxiv_id":"2301.11305","title":"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature","abstract":"The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https:\/\/ericmitchell.ai\/detectgpt for code, data, and other project information.","source":"http:\/\/arxiv.org\/pdf\/2301.11305","authors":["Eric Mitchell","Yoonho Lee","Alexander Khazatsky","Christopher D. Manning","Chelsea Finn"],"categories":["cs.CL","cs.AI"],"published":"20230126","updated":"20230723","primary_category":"cs.CL"}
{"arxiv_id":"2004.08994","title":"Adversarial Training for Large Neural Language Models","abstract":"Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https:\/\/github.com\/namisan\/mt-dnn.","source":"http:\/\/arxiv.org\/pdf\/2004.08994","authors":["Xiaodong Liu","Hao Cheng","Pengcheng He","Weizhu Chen","Yu Wang","Hoifung Poon","Jianfeng Gao"],"categories":["cs.CL"],"published":"20200420","updated":"20200429","primary_category":"cs.CL"}
{"arxiv_id":"1904.03310","title":"Gender Bias in Contextualized Word Embeddings","abstract":"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","source":"http:\/\/arxiv.org\/pdf\/1904.03310","authors":["Jieyu Zhao","Tianlu Wang","Mark Yatskar","Ryan Cotterell","Vicente Ordonez","Kai-Wei Chang"],"categories":["cs.CL"],"published":"20190405","updated":"20190405","primary_category":"cs.CL"}
{"arxiv_id":"2009.03300","title":"Measuring Massive Multitask Language Understanding","abstract":"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.","source":"http:\/\/arxiv.org\/pdf\/2009.03300","authors":["Dan Hendrycks","Collin Burns","Steven Basart","Andy Zou","Mantas Mazeika","Dawn Song","Jacob Steinhardt"],"categories":["cs.CY","cs.AI","cs.CL","cs.LG"],"published":"20200907","updated":"20210112","primary_category":"cs.CY"}
{"arxiv_id":"2212.10403","title":"Towards Reasoning in Large Language Models: A Survey","abstract":"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.","source":"http:\/\/arxiv.org\/pdf\/2212.10403","authors":["Jie Huang","Kevin Chen-Chuan Chang"],"categories":["cs.CL","cs.AI"],"published":"20221220","updated":"20230526","primary_category":"cs.CL"}
{"arxiv_id":"1712.09913","title":"Visualizing the Loss Landscape of Neural Nets","abstract":"Neural network training relies on our ability to find \"good\" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple \"filter normalization\" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.","source":"http:\/\/arxiv.org\/pdf\/1712.09913","authors":["Hao Li","Zheng Xu","Gavin Taylor","Christoph Studer","Tom Goldstein"],"categories":["cs.LG","cs.CV","stat.ML"],"published":"20171228","updated":"20181107","primary_category":"cs.LG"}
{"arxiv_id":"1605.09090","title":"Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention","abstract":"In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called \"Inner-Attention\" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of \"Inner-Attention\" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin.","source":"http:\/\/arxiv.org\/pdf\/1605.09090","authors":["Yang Liu","Chengjie Sun","Lei Lin","Xiaolong Wang"],"categories":["cs.CL"],"published":"20160530","updated":"20160530","primary_category":"cs.CL"}
{"arxiv_id":"2207.05221","title":"Language Models (Mostly) Know What They Know","abstract":"We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true\/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability \"P(True)\" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict \"P(IK)\", the probability that \"I know\" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.","source":"http:\/\/arxiv.org\/pdf\/2207.05221","authors":["Saurav Kadavath","Tom Conerly","Amanda Askell","Tom Henighan","Dawn Drain","Ethan Perez","Nicholas Schiefer","Zac Hatfield-Dodds","Nova DasSarma","Eli Tran-Johnson","Scott Johnston","Sheer El-Showk","Andy Jones","Nelson Elhage","Tristan Hume","Anna Chen","Yuntao Bai","Sam Bowman","Stanislav Fort","Deep Ganguli","Danny Hernandez","Josh Jacobson","Jackson Kernion","Shauna Kravec","Liane Lovitt","Kamal Ndousse","Catherine Olsson","Sam Ringer","Dario Amodei","Tom Brown","Jack Clark","Nicholas Joseph","Ben Mann","Sam McCandlish","Chris Olah","Jared Kaplan"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20220711","updated":"20221121","primary_category":"cs.CL"}
{"arxiv_id":"2206.02336","title":"Making Large Language Models Better Reasoners with Step-Aware Verifier","abstract":"Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DIVERSE on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).","source":"http:\/\/arxiv.org\/pdf\/2206.02336","authors":["Yifei Li","Zeqi Lin","Shizhuo Zhang","Qiang Fu","Bei Chen","Jian-Guang Lou","Weizhu Chen"],"categories":["cs.CL","cs.AI"],"published":"20220606","updated":"20230524","primary_category":"cs.CL"}
{"arxiv_id":"1710.10723","title":"Simple and Effective Multi-Paragraph Reading Comprehension","abstract":"We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.","source":"http:\/\/arxiv.org\/pdf\/1710.10723","authors":["Christopher Clark","Matt Gardner"],"categories":["cs.CL"],"published":"20171029","updated":"20171107","primary_category":"cs.CL"}
{"arxiv_id":"2103.03874","title":"Measuring Mathematical Problem Solving With the MATH Dataset","abstract":"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.","source":"http:\/\/arxiv.org\/pdf\/2103.03874","authors":["Dan Hendrycks","Collin Burns","Saurav Kadavath","Akul Arora","Steven Basart","Eric Tang","Dawn Song","Jacob Steinhardt"],"categories":["cs.LG","cs.AI","cs.CL"],"published":"20210305","updated":"20211108","primary_category":"cs.LG"}
{"arxiv_id":"2010.00133","title":"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models","abstract":"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.","source":"http:\/\/arxiv.org\/pdf\/2010.00133","authors":["Nikita Nangia","Clara Vania","Rasika Bhalerao","Samuel R. Bowman"],"categories":["cs.CL","cs.AI"],"published":"20200930","updated":"20200930","primary_category":"cs.CL"}
{"arxiv_id":"2101.03961","title":"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity","abstract":"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.","source":"http:\/\/arxiv.org\/pdf\/2101.03961","authors":["William Fedus","Barret Zoph","Noam Shazeer"],"categories":["cs.LG","cs.AI"],"published":"20210111","updated":"20220616","primary_category":"cs.LG"}
{"arxiv_id":"1705.04304","title":"A Deep Reinforced Model for Abstractive Summarization","abstract":"Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN\/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN\/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.","source":"http:\/\/arxiv.org\/pdf\/1705.04304","authors":["Romain Paulus","Caiming Xiong","Richard Socher"],"categories":["cs.CL"],"published":"20170511","updated":"20171113","primary_category":"cs.CL"}
{"arxiv_id":"2110.14168","title":"Training Verifiers to Solve Math Word Problems","abstract":"State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.","source":"http:\/\/arxiv.org\/pdf\/2110.14168","authors":["Karl Cobbe","Vineet Kosaraju","Mohammad Bavarian","Mark Chen","Heewoo Jun","Lukasz Kaiser","Matthias Plappert","Jerry Tworek","Jacob Hilton","Reiichiro Nakano","Christopher Hesse","John Schulman"],"categories":["cs.LG","cs.CL"],"published":"20211027","updated":"20211118","primary_category":"cs.LG"}
{"arxiv_id":"2204.10050","title":"SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding","abstract":"This paper presents the shared task on Multilingual Idiomaticity Detection and Sentence Embedding, which consists of two subtasks: (a) a binary classification task aimed at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.","source":"http:\/\/arxiv.org\/pdf\/2204.10050","authors":["Harish Tayyar Madabushi","Edward Gow-Smith","Marcos Garcia","Carolina Scarton","Marco Idiart","Aline Villavicencio"],"categories":["cs.CL"],"published":"20220421","updated":"20220530","primary_category":"cs.CL"}
{"arxiv_id":"2202.07785","title":"Predictability and Surprise in Large Generative Models","abstract":"Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their \"scaling laws\"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.","source":"http:\/\/arxiv.org\/pdf\/2202.07785","authors":["Deep Ganguli","Danny Hernandez","Liane Lovitt","Nova DasSarma","Tom Henighan","Andy Jones","Nicholas Joseph","Jackson Kernion","Ben Mann","Amanda Askell","Yuntao Bai","Anna Chen","Tom Conerly","Dawn Drain","Nelson Elhage","Sheer El Showk","Stanislav Fort","Zac Hatfield-Dodds","Scott Johnston","Shauna Kravec","Neel Nanda","Kamal Ndousse","Catherine Olsson","Daniela Amodei","Dario Amodei","Tom Brown","Jared Kaplan","Sam McCandlish","Chris Olah","Jack Clark"],"categories":["cs.CY"],"published":"20220215","updated":"20221003","primary_category":"cs.CY"}
{"arxiv_id":"2204.05862","title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback","abstract":"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.","source":"http:\/\/arxiv.org\/pdf\/2204.05862","authors":["Yuntao Bai","Andy Jones","Kamal Ndousse","Amanda Askell","Anna Chen","Nova DasSarma","Dawn Drain","Stanislav Fort","Deep Ganguli","Tom Henighan","Nicholas Joseph","Saurav Kadavath","Jackson Kernion","Tom Conerly","Sheer El-Showk","Nelson Elhage","Zac Hatfield-Dodds","Danny Hernandez","Tristan Hume","Scott Johnston","Shauna Kravec","Liane Lovitt","Neel Nanda","Catherine Olsson","Dario Amodei","Tom Brown","Jack Clark","Sam McCandlish","Chris Olah","Ben Mann","Jared Kaplan"],"categories":["cs.CL","cs.LG"],"published":"20220412","updated":"20220412","primary_category":"cs.CL"}
{"arxiv_id":"2007.01282","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering","abstract":"Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.","source":"http:\/\/arxiv.org\/pdf\/2007.01282","authors":["Gautier Izacard","Edouard Grave"],"categories":["cs.CL","cs.LG"],"published":"20200702","updated":"20210203","primary_category":"cs.CL"}
{"arxiv_id":"2205.12255","title":"TALM: Tool Augmented Language Models","abstract":"Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative \"self-play\" technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.","source":"http:\/\/arxiv.org\/pdf\/2205.12255","authors":["Aaron Parisi","Yao Zhao","Noah Fiedel"],"categories":["cs.CL","cs.AI"],"published":"20220524","updated":"20220524","primary_category":"cs.CL"}
{"arxiv_id":"1904.07734","title":"Three scenarios for continual learning","abstract":"Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and--in case it is not--whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.","source":"http:\/\/arxiv.org\/pdf\/1904.07734","authors":["Gido M. van de Ven","Andreas S. Tolias"],"categories":["cs.LG","cs.AI","cs.CV","stat.ML"],"published":"20190415","updated":"20190415","primary_category":"cs.LG"}
{"arxiv_id":"2112.04359","title":"Ethical and social risks of harm from Language Models","abstract":"This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.","source":"http:\/\/arxiv.org\/pdf\/2112.04359","authors":["Laura Weidinger","John Mellor","Maribeth Rauh","Conor Griffin","Jonathan Uesato","Po-Sen Huang","Myra Cheng","Mia Glaese","Borja Balle","Atoosa Kasirzadeh","Zac Kenton","Sasha Brown","Will Hawkins","Tom Stepleton","Courtney Biles","Abeba Birhane","Julia Haas","Laura Rimell","Lisa Anne Hendricks","William Isaac","Sean Legassick","Geoffrey Irving","Iason Gabriel"],"categories":["cs.CL","cs.AI","cs.CY"],"published":"20211208","updated":"20211208","primary_category":"cs.CL"}
{"arxiv_id":"1602.01783","title":"Asynchronous Methods for Deep Reinforcement Learning","abstract":"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","source":"http:\/\/arxiv.org\/pdf\/1602.01783","authors":["Volodymyr Mnih","Adri\u00e0 Puigdom\u00e8nech Badia","Mehdi Mirza","Alex Graves","Timothy P. Lillicrap","Tim Harley","David Silver","Koray Kavukcuoglu"],"categories":["cs.LG"],"published":"20160204","updated":"20160616","primary_category":"cs.LG"}
{"arxiv_id":"2110.03215","title":"Towards Continual Knowledge Learning of Language Models","abstract":"Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, evaluation script, and baseline code to reproduce our results are available at https:\/\/github.com\/joeljang\/continual-knowledge-learning.","source":"http:\/\/arxiv.org\/pdf\/2110.03215","authors":["Joel Jang","Seonghyeon Ye","Sohee Yang","Joongbo Shin","Janghoon Han","Gyeonghun Kim","Stanley Jungkyu Choi","Minjoon Seo"],"categories":["cs.CL","cs.LG"],"published":"20211007","updated":"20220524","primary_category":"cs.CL"}
{"arxiv_id":"1508.07909","title":"Neural Machine Translation of Rare Words with Subword Units","abstract":"Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.","source":"http:\/\/arxiv.org\/pdf\/1508.07909","authors":["Rico Sennrich","Barry Haddow","Alexandra Birch"],"categories":["cs.CL"],"published":"20150831","updated":"20160610","primary_category":"cs.CL"}
{"arxiv_id":"1911.00536","title":"DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation","abstract":"We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.","source":"http:\/\/arxiv.org\/pdf\/1911.00536","authors":["Yizhe Zhang","Siqi Sun","Michel Galley","Yen-Chun Chen","Chris Brockett","Xiang Gao","Jianfeng Gao","Jingjing Liu","Bill Dolan"],"categories":["cs.CL","cs.LG"],"published":"20191101","updated":"20200502","primary_category":"cs.CL"}
{"arxiv_id":"2210.03350","title":"Measuring and Narrowing the Compositionality Gap in Language Models","abstract":"We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.","source":"http:\/\/arxiv.org\/pdf\/2210.03350","authors":["Ofir Press","Muru Zhang","Sewon Min","Ludwig Schmidt","Noah A. Smith","Mike Lewis"],"categories":["cs.CL"],"published":"20221007","updated":"20230523","primary_category":"cs.CL"}
{"arxiv_id":"1910.10683","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","abstract":"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.","source":"http:\/\/arxiv.org\/pdf\/1910.10683","authors":["Colin Raffel","Noam Shazeer","Adam Roberts","Katherine Lee","Sharan Narang","Michael Matena","Yanqi Zhou","Wei Li","Peter J. Liu"],"categories":["cs.LG","cs.CL","stat.ML"],"published":"20191023","updated":"20230919","primary_category":"cs.LG"}
{"arxiv_id":"2103.00453","title":"Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP","abstract":"When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model's parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.","source":"http:\/\/arxiv.org\/pdf\/2103.00453","authors":["Timo Schick","Sahana Udupa","Hinrich Sch\u00fctze"],"categories":["cs.CL"],"published":"20210228","updated":"20210909","primary_category":"cs.CL"}
{"arxiv_id":"2305.15717","title":"The False Promise of Imitating Proprietary LLMs","abstract":"An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.","source":"http:\/\/arxiv.org\/pdf\/2305.15717","authors":["Arnav Gudibande","Eric Wallace","Charlie Snell","Xinyang Geng","Hao Liu","Pieter Abbeel","Sergey Levine","Dawn Song"],"categories":["cs.CL"],"published":"20230525","updated":"20230525","primary_category":"cs.CL"}
{"arxiv_id":"2012.15761","title":"Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection","abstract":"We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of ~40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes ~15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also perform better on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use. Accepted at ACL 2021.","source":"http:\/\/arxiv.org\/pdf\/2012.15761","authors":["Bertie Vidgen","Tristan Thrush","Zeerak Waseem","Douwe Kiela"],"categories":["cs.CL","cs.LG"],"published":"20201231","updated":"20210603","primary_category":"cs.CL"}
{"arxiv_id":"2007.05558","title":"The Computational Limits of Deep Learning","abstract":"Deep learning's recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image classification, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article catalogs the extent of this dependency, showing that progress across a wide variety of applications is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.","source":"http:\/\/arxiv.org\/pdf\/2007.05558","authors":["Neil C. Thompson","Kristjan Greenewald","Keeheon Lee","Gabriel F. Manso"],"categories":["cs.LG","stat.ML"],"published":"20200710","updated":"20220727","primary_category":"cs.LG"}
{"arxiv_id":"2009.10855","title":"Controlling Style in Generated Dialogue","abstract":"Open-domain conversation models have become good at generating natural-sounding dialogue, using very large architectures with billions of trainable parameters. The vast training data required to train these architectures aggregates many different styles, tones, and qualities. Using that data to train a single model makes it difficult to use the model as a consistent conversational agent, e.g. with a stable set of persona traits and a typical style of expression. Several architectures affording control mechanisms over generation architectures have been proposed, each with different trade-offs. However, it remains unclear whether their use in dialogue is viable, and what the trade-offs look like with the most recent state-of-the-art conversational architectures. In this work, we adapt three previously proposed controllable generation architectures to open-domain dialogue generation, controlling the style of the generation to match one among about 200 possible styles. We compare their respective performance and tradeoffs, and show how they can be used to provide insights into existing conversational datasets, and generate a varied set of styled conversation replies.","source":"http:\/\/arxiv.org\/pdf\/2009.10855","authors":["Eric Michael Smith","Diana Gonzalez-Rico","Emily Dinan","Y-Lan Boureau"],"categories":["cs.CL"],"published":"20200922","updated":"20200922","primary_category":"cs.CL"}
{"arxiv_id":"2304.06364","title":"AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models","abstract":"Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https:\/\/github.com\/ruixiangcui\/AGIEval.","source":"http:\/\/arxiv.org\/pdf\/2304.06364","authors":["Wanjun Zhong","Ruixiang Cui","Yiduo Guo","Yaobo Liang","Shuai Lu","Yanlin Wang","Amin Saied","Weizhu Chen","Nan Duan"],"categories":["cs.CL","cs.AI"],"published":"20230413","updated":"20230918","primary_category":"cs.CL"}
{"arxiv_id":"1705.04146","title":"Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems","abstract":"Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.","source":"http:\/\/arxiv.org\/pdf\/1705.04146","authors":["Wang Ling","Dani Yogatama","Chris Dyer","Phil Blunsom"],"categories":["cs.AI","cs.CL","cs.LG"],"published":"20170511","updated":"20171023","primary_category":"cs.AI"}
{"arxiv_id":"1908.08113","title":"X-SQL: reinforce schema representation with context","abstract":"In this work, we present X-SQL, a new network architecture for the problem of parsing natural language to SQL query. X-SQL proposes to enhance the structural schema representation with the contextual output from BERT-style pre-training model, and together with type information to learn a new schema representation for down-stream tasks. We evaluated X-SQL on the WikiSQL dataset and show its new state-of-the-art performance.","source":"http:\/\/arxiv.org\/pdf\/1908.08113","authors":["Pengcheng He","Yi Mao","Kaushik Chakrabarti","Weizhu Chen"],"categories":["cs.CL"],"published":"20190821","updated":"20190821","primary_category":"cs.CL"}
{"arxiv_id":"2303.11156","title":"Can AI-Generated Text be Reliably Detected?","abstract":"In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as particular writing styles, clever prompt design, or text paraphrasing. We also extend the impossibility result to include the case where pseudorandom number generators are used for AI-text generation instead of true randomness. We show that the same result holds with a negligible correction term for all polynomial-time computable detectors. Finally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks where adversarial humans can infer hidden LLM text signatures and add them to human-generated text to be detected as text generated by the LLMs, potentially causing reputational damage to their developers. We believe these results can open an honest conversation in the community regarding the ethical and reliable use of AI-generated text.","source":"http:\/\/arxiv.org\/pdf\/2303.11156","authors":["Vinu Sankar Sadasivan","Aounon Kumar","Sriram Balasubramanian","Wenxiao Wang","Soheil Feizi"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20230317","updated":"20230628","primary_category":"cs.CL"}
{"arxiv_id":"2102.12594","title":"Directional Bias Amplification","abstract":"Mitigating bias in machine learning systems requires refining our understanding of bias propagation pathways: from societal structures to large-scale data to trained models to impact on society. In this work, we focus on one aspect of the problem, namely bias amplification: the tendency of models to amplify the biases present in the data they are trained on. A metric for measuring bias amplification was introduced in the seminal work by Zhao et al. (2017); however, as we demonstrate, this metric suffers from a number of shortcomings including conflating different types of bias amplification and failing to account for varying base rates of protected attributes. We introduce and analyze a new, decoupled metric for measuring bias amplification, $\\text{BiasAmp}_{\\rightarrow}$ (Directional Bias Amplification). We thoroughly analyze and discuss both the technical assumptions and normative implications of this metric. We provide suggestions about its measurement by cautioning against predicting sensitive attributes, encouraging the use of confidence intervals due to fluctuations in the fairness of models across runs, and discussing the limitations of what this metric captures. Throughout this paper, we work to provide an interrogative look at the technical measurement of bias amplification, guided by our normative ideas of what we want it to encompass. Code is located at https:\/\/github.com\/princetonvisualai\/directional-bias-amp","source":"http:\/\/arxiv.org\/pdf\/2102.12594","authors":["Angelina Wang","Olga Russakovsky"],"categories":["cs.LG","cs.AI"],"published":"20210224","updated":"20210607","primary_category":"cs.LG"}
{"arxiv_id":"2211.08264","title":"QAmeleon: Multilingual QA with Only 5 Examples","abstract":"The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled examples in low resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show that few-shot prompt tuning for data synthesis scales across languages and is a viable alternative to large-scale annotation.","source":"http:\/\/arxiv.org\/pdf\/2211.08264","authors":["Priyanka Agrawal","Chris Alberti","Fantine Huot","Joshua Maynez","Ji Ma","Sebastian Ruder","Kuzman Ganchev","Dipanjan Das","Mirella Lapata"],"categories":["cs.CL"],"published":"20221115","updated":"20230807","primary_category":"cs.CL"}
{"arxiv_id":"1908.06083","title":"Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack","abstract":"The detection of offensive language in the context of a dialogue has become an increasingly important application of natural language processing. The detection of trolls in public forums (Gal\\'an-Garc\\'ia et al., 2016), and the deployment of chatbots in the public domain (Wolf et al., 2017) are two examples that show the necessity of guarding against adversarially offensive behavior on the part of humans. In this work, we develop a training scheme for a model to become robust to such human attacks by an iterative build it, break it, fix it strategy with humans and models in the loop. In detailed experiments we show this approach is considerably more robust than previous systems. Further, we show that offensive language used within a conversation critically depends on the dialogue context, and cannot be viewed as a single sentence offensive detection task as in most previous work. Our newly collected tasks and methods will be made open source and publicly available.","source":"http:\/\/arxiv.org\/pdf\/1908.06083","authors":["Emily Dinan","Samuel Humeau","Bharath Chintagunta","Jason Weston"],"categories":["cs.CL"],"published":"20190817","updated":"20190817","primary_category":"cs.CL"}
{"arxiv_id":"2211.09085","title":"Galactica: A Large Language Model for Science","abstract":"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.","source":"http:\/\/arxiv.org\/pdf\/2211.09085","authors":["Ross Taylor","Marcin Kardas","Guillem Cucurull","Thomas Scialom","Anthony Hartshorn","Elvis Saravia","Andrew Poulton","Viktor Kerkez","Robert Stojnic"],"categories":["cs.CL","stat.ML"],"published":"20221116","updated":"20221116","primary_category":"cs.CL"}
{"arxiv_id":"2210.13236","title":"Universal and Independent: Multilingual Probing Framework for Exhaustive Model Interpretation and Evaluation","abstract":"Linguistic analysis of language models is one of the ways to explain and describe their reasoning, weaknesses, and limitations. In the probing part of the model interpretability research, studies concern individual languages as well as individual linguistic structures. The question arises: are the detected regularities linguistically coherent, or on the contrary, do they dissonate at the typological scale? Moreover, the majority of studies address the inherent set of languages and linguistic structures, leaving the actual typological diversity knowledge out of scope. In this paper, we present and apply the GUI-assisted framework allowing us to easily probe a massive number of languages for all the morphosyntactic features present in the Universal Dependencies data. We show that reflecting the anglo-centric trend in NLP over the past years, most of the regularities revealed in the mBERT model are typical for the western-European languages. Our framework can be integrated with the existing probing toolboxes, model cards, and leaderboards, allowing practitioners to use and share their standard probing methods to interpret multilingual models. Thus we propose a toolkit to systematize the multilingual flaws in multilingual models, providing a reproducible experimental setup for 104 languages and 80 morphosyntactic features. https:\/\/github.com\/AIRI-Institute\/Probing_framework","source":"http:\/\/arxiv.org\/pdf\/2210.13236","authors":["Oleg Serikov","Vitaly Protasov","Ekaterina Voloshina","Viktoria Knyazkova","Tatiana Shavrina"],"categories":["cs.CL","cs.AI","68-04, 68-06, 68T50","G.3; I.2.7"],"published":"20221024","updated":"20221024","primary_category":"cs.CL"}
{"arxiv_id":"2211.08411","title":"Large Language Models Struggle to Learn Long-Tail Knowledge","abstract":"The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.","source":"http:\/\/arxiv.org\/pdf\/2211.08411","authors":["Nikhil Kandpal","Haikang Deng","Adam Roberts","Eric Wallace","Colin Raffel"],"categories":["cs.CL","cs.LG"],"published":"20221115","updated":"20230727","primary_category":"cs.CL"}
{"arxiv_id":"1910.10486","title":"Does Gender Matter? Towards Fairness in Dialogue Systems","abstract":"Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as \"gorillas\". As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.","source":"http:\/\/arxiv.org\/pdf\/1910.10486","authors":["Haochen Liu","Jamell Dacon","Wenqi Fan","Hui Liu","Zitao Liu","Jiliang Tang"],"categories":["cs.CL","cs.AI"],"published":"20191016","updated":"20201031","primary_category":"cs.CL"}
{"arxiv_id":"1705.07485","title":"Shake-Shake regularization","abstract":"The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of applications. Code is available at https:\/\/github.com\/xgastaldi\/shake-shake","source":"http:\/\/arxiv.org\/pdf\/1705.07485","authors":["Xavier Gastaldi"],"categories":["cs.LG","cs.CV"],"published":"20170521","updated":"20170523","primary_category":"cs.LG"}
{"arxiv_id":"2005.12246","title":"Demoting Racial Bias in Hate Speech Detection","abstract":"In current hate speech datasets, there exists a high correlation between annotators' perceptions of toxicity and signals of African American English (AAE). This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive\/offensive\/hate speech with a high false positive rate by current hate speech classifiers. In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts. Experimental results on a hate speech dataset and an AAE dataset suggest that our method is able to substantially reduce the false positive rate for AAE text while only minimally affecting the performance of hate speech classification.","source":"http:\/\/arxiv.org\/pdf\/2005.12246","authors":["Mengzhou Xia","Anjalie Field","Yulia Tsvetkov"],"categories":["cs.CL"],"published":"20200525","updated":"20200525","primary_category":"cs.CL"}
{"arxiv_id":"2301.07597","title":"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection","abstract":"The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https:\/\/github.com\/Hello-SimpleAI\/chatgpt-comparison-detection.","source":"http:\/\/arxiv.org\/pdf\/2301.07597","authors":["Biyang Guo","Xin Zhang","Ziyuan Wang","Minqi Jiang","Jinran Nie","Yuxuan Ding","Jianwei Yue","Yupeng Wu"],"categories":["cs.CL"],"published":"20230118","updated":"20230118","primary_category":"cs.CL"}
{"arxiv_id":"1904.09482","title":"Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding","abstract":"This paper explores the use of knowledge distillation to improve a Multi-Task Deep Neural Network (MT-DNN) (Liu et al., 2019) for learning text representations across multiple natural language understanding tasks. Although ensemble learning can improve model performance, serving an ensemble of large DNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge distillation method (Hinton et al., 2015) in the multi-task learning setting. For each task, we train an ensemble of different MT-DNNs (teacher) that outperforms any single model, and then train a single MT-DNN (student) via multi-task learning to \\emph{distill} knowledge from these ensemble teachers. We show that the distilled MT-DNN significantly outperforms the original MT-DNN on 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\\% (1.5\\% absolute improvement\\footnote{ Based on the GLUE leaderboard at https:\/\/gluebenchmark.com\/leaderboard as of April 1, 2019.}). The code and pre-trained models will be made publicly available at https:\/\/github.com\/namisan\/mt-dnn.","source":"http:\/\/arxiv.org\/pdf\/1904.09482","authors":["Xiaodong Liu","Pengcheng He","Weizhu Chen","Jianfeng Gao"],"categories":["cs.CL"],"published":"20190420","updated":"20190420","primary_category":"cs.CL"}
{"arxiv_id":"2202.08904","title":"SGPT: GPT Sentence Embeddings for Semantic Search","abstract":"Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https:\/\/github.com\/Muennighoff\/sgpt.","source":"http:\/\/arxiv.org\/pdf\/2202.08904","authors":["Niklas Muennighoff"],"categories":["cs.CL","cs.AI","cs.IR"],"published":"20220217","updated":"20220805","primary_category":"cs.CL"}
{"arxiv_id":"2211.04325","title":"Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning","abstract":"We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.","source":"http:\/\/arxiv.org\/pdf\/2211.04325","authors":["Pablo Villalobos","Jaime Sevilla","Lennart Heim","Tamay Besiroglu","Marius Hobbhahn","Anson Ho"],"categories":["cs.LG","cs.AI","cs.CL","cs.CV","cs.CY"],"published":"20221026","updated":"20221026","primary_category":"cs.LG"}
{"arxiv_id":"1904.08783","title":"Evaluating the Underlying Gender Bias in Contextualized Word Embeddings","abstract":"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","source":"http:\/\/arxiv.org\/pdf\/1904.08783","authors":["Christine Basta","Marta R. Costa-juss\u00e0","Noe Casas"],"categories":["cs.CL","cs.LG"],"published":"20190418","updated":"20190418","primary_category":"cs.CL"}
{"arxiv_id":"1911.03914","title":"Zero-Shot Fine-Grained Style Transfer: Leveraging Distributed Continuous Style Representations to Transfer To Unseen Styles","abstract":"Text style transfer is usually performed using attributes that can take a handful of discrete values (e.g., positive to negative reviews). In this work, we introduce an architecture that can leverage pre-trained consistent continuous distributed style representations and use them to transfer to an attribute unseen during training, without requiring any re-tuning of the style transfer model. We demonstrate the method by training an architecture to transfer text conveying one sentiment to another sentiment, using a fine-grained set of over 20 sentiment labels rather than the binary positive\/negative often used in style transfer. Our experiments show that this model can then rewrite text to match a target sentiment that was unseen during training.","source":"http:\/\/arxiv.org\/pdf\/1911.03914","authors":["Eric Michael Smith","Diana Gonzalez-Rico","Emily Dinan","Y-Lan Boureau"],"categories":["cs.CL"],"published":"20191110","updated":"20191110","primary_category":"cs.CL"}
{"arxiv_id":"1905.09165","title":"A framework for the extraction of Deep Neural Networks by leveraging public data","abstract":"Machine learning models trained on confidential datasets are increasingly being deployed for profit. Machine Learning as a Service (MLaaS) has made such models easily accessible to end-users. Prior work has developed model extraction attacks, in which an adversary extracts an approximation of MLaaS models by making black-box queries to it. However, none of these works is able to satisfy all the three essential criteria for practical model extraction: (1) the ability to work on deep learning models, (2) the non-requirement of domain knowledge and (3) the ability to work with a limited query budget. We design a model extraction framework that makes use of active learning and large public datasets to satisfy them. We demonstrate that it is possible to use this framework to steal deep classifiers trained on a variety of datasets from image and text domains. By querying a model via black-box access for its top prediction, our framework improves performance on an average over a uniform noise baseline by 4.70x for image tasks and 2.11x for text tasks respectively, while using only 30% (30,000 samples) of the public dataset at its disposal.","source":"http:\/\/arxiv.org\/pdf\/1905.09165","authors":["Soham Pal","Yash Gupta","Aditya Shukla","Aditya Kanade","Shirish Shevade","Vinod Ganapathy"],"categories":["cs.LG","cs.AI","cs.CR","stat.ML"],"published":"20190522","updated":"20190522","primary_category":"cs.LG"}
{"arxiv_id":"1910.12840","title":"Evaluating the Factual Consistency of Abstractive Text Summarization","abstract":"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.","source":"http:\/\/arxiv.org\/pdf\/1910.12840","authors":["Wojciech Kry\u015bci\u0144ski","Bryan McCann","Caiming Xiong","Richard Socher"],"categories":["cs.CL"],"published":"20191028","updated":"20191028","primary_category":"cs.CL"}
{"arxiv_id":"1607.01759","title":"Bag of Tricks for Efficient Text Classification","abstract":"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.","source":"http:\/\/arxiv.org\/pdf\/1607.01759","authors":["Armand Joulin","Edouard Grave","Piotr Bojanowski","Tomas Mikolov"],"categories":["cs.CL"],"published":"20160706","updated":"20160809","primary_category":"cs.CL"}
{"arxiv_id":"2005.05921","title":"Intersectional Bias in Hate Speech and Abusive Language Datasets","abstract":"Algorithms are widely applied to detect hate speech and abusive language in social media. We investigated whether the human-annotated data used to train these algorithms are biased. We utilized a publicly available annotated Twitter dataset (Founta et al. 2018) and classified the racial, gender, and party identification dimensions of 99,996 tweets. The results showed that African American tweets were up to 3.7 times more likely to be labeled as abusive, and African American male tweets were up to 77% more likely to be labeled as hateful compared to the others. These patterns were statistically significant and robust even when party identification was added as a control variable. This study provides the first systematic evidence on intersectional bias in datasets of hate speech and abusive language.","source":"http:\/\/arxiv.org\/pdf\/2005.05921","authors":["Jae Yeon Kim","Carlos Ortiz","Sarah Nam","Sarah Santiago","Vivek Datta"],"categories":["cs.CL","cs.SI"],"published":"20200512","updated":"20200528","primary_category":"cs.CL"}
{"arxiv_id":"2304.09542","title":"Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent","abstract":"Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com\/sunnweiwei\/RankGPT","source":"http:\/\/arxiv.org\/pdf\/2304.09542","authors":["Weiwei Sun","Lingyong Yan","Xinyu Ma","Pengjie Ren","Dawei Yin","Zhaochun Ren"],"categories":["cs.CL","cs.IR"],"published":"20230419","updated":"20230419","primary_category":"cs.CL"}
{"arxiv_id":"2205.01825","title":"AmbiPun: Generating Humorous Puns with Ambiguous Context","abstract":"In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52\\% of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.","source":"http:\/\/arxiv.org\/pdf\/2205.01825","authors":["Anirudh Mittal","Yufei Tian","Nanyun Peng"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20220504","updated":"20220504","primary_category":"cs.CL"}
{"arxiv_id":"2103.12028","title":"Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets","abstract":"With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard\/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.","source":"http:\/\/arxiv.org\/pdf\/2103.12028","authors":["Julia Kreutzer","Isaac Caswell","Lisa Wang","Ahsan Wahab","Daan van Esch","Nasanbayar Ulzii-Orshikh","Allahsera Tapo","Nishant Subramani","Artem Sokolov","Claytone Sikasote","Monang Setyawan","Supheakmungkol Sarin","Sokhar Samb","Beno\u00eet Sagot","Clara Rivera","Annette Rios","Isabel Papadimitriou","Salomey Osei","Pedro Ortiz Suarez","Iroro Orife","Kelechi Ogueji","Andre Niyongabo Rubungo","Toan Q. Nguyen","Mathias M\u00fcller","Andr\u00e9 M\u00fcller","Shamsuddeen Hassan Muhammad","Nanda Muhammad","Ayanda Mnyakeni","Jamshidbek Mirzakhalov","Tapiwanashe Matangira","Colin Leong","Nze Lawson","Sneha Kudugunta","Yacine Jernite","Mathias Jenny","Orhan Firat","Bonaventure F. P. Dossou","Sakhile Dlamini","Nisansa de Silva","Sakine \u00c7abuk Ball\u0131","Stella Biderman","Alessia Battisti","Ahmed Baruwa","Ankur Bapna","Pallavi Baljekar","Israel Abebe Azime","Ayodele Awokoya","Duygu Ataman","Orevaoghene Ahia","Oghenefego Ahia","Sweta Agrawal","Mofetoluwa Adeyemi"],"categories":["cs.CL","cs.AI"],"published":"20210322","updated":"20220221","primary_category":"cs.CL"}
{"arxiv_id":"2207.05608","title":"Inner Monologue: Embodied Reasoning through Planning with Language Models","abstract":"Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.","source":"http:\/\/arxiv.org\/pdf\/2207.05608","authors":["Wenlong Huang","Fei Xia","Ted Xiao","Harris Chan","Jacky Liang","Pete Florence","Andy Zeng","Jonathan Tompson","Igor Mordatch","Yevgen Chebotar","Pierre Sermanet","Noah Brown","Tomas Jackson","Linda Luu","Sergey Levine","Karol Hausman","Brian Ichter"],"categories":["cs.RO","cs.AI","cs.CL","cs.CV","cs.LG"],"published":"20220712","updated":"20220712","primary_category":"cs.RO"}
{"arxiv_id":"2301.00303","title":"Rethinking with Retrieval: Faithful Large Language Model Inference","abstract":"Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.","source":"http:\/\/arxiv.org\/pdf\/2301.00303","authors":["Hangfeng He","Hongming Zhang","Dan Roth"],"categories":["cs.CL","cs.AI"],"published":"20221231","updated":"20221231","primary_category":"cs.CL"}
{"arxiv_id":"2004.05150","title":"Longformer: The Long-Document Transformer","abstract":"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.","source":"http:\/\/arxiv.org\/pdf\/2004.05150","authors":["Iz Beltagy","Matthew E. Peters","Arman Cohan"],"categories":["cs.CL"],"published":"20200410","updated":"20201202","primary_category":"cs.CL"}
{"arxiv_id":"2206.07635","title":"AI Ethics Issues in Real World: Evidence from AI Incident Database","abstract":"With the powerful performance of Artificial Intelligence (AI) also comes prevalent ethical issues. Though governments and corporations have curated multiple AI ethics guidelines to curb unethical behavior of AI, the effect has been limited, probably due to the vagueness of the guidelines. In this paper, we take a closer look at how AI ethics issues take place in real world, in order to have a more in-depth and nuanced understanding of different ethical issues as well as their social impact. With a content analysis of AI Incident Database, which is an effort to prevent repeated real world AI failures by cataloging incidents, we identified 13 application areas which often see unethical use of AI, with intelligent service robots, language\/vision models and autonomous driving taking the lead. Ethical issues appear in 8 different forms, from inappropriate use and racial discrimination, to physical safety and unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI practitioners with a practical guideline when trying to deploy AI applications ethically.","source":"http:\/\/arxiv.org\/pdf\/2206.07635","authors":["Mengyi Wei","Zhixuan Zhou"],"categories":["cs.AI","cs.CY"],"published":"20220615","updated":"20220818","primary_category":"cs.AI"}
{"arxiv_id":"1905.13319","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms","abstract":"We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https:\/\/math-qa.github.io\/math-QA\/","source":"http:\/\/arxiv.org\/pdf\/1905.13319","authors":["Aida Amini","Saadia Gabriel","Peter Lin","Rik Koncel-Kedziorski","Yejin Choi","Hannaneh Hajishirzi"],"categories":["cs.CL"],"published":"20190530","updated":"20190530","primary_category":"cs.CL"}
{"arxiv_id":"2302.07867","title":"Learning Performance-Improving Code Edits","abstract":"The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.","source":"http:\/\/arxiv.org\/pdf\/2302.07867","authors":["Aman Madaan","Alexander Shypula","Uri Alon","Milad Hashemi","Parthasarathy Ranganathan","Yiming Yang","Graham Neubig","Amir Yazdanbakhsh"],"categories":["cs.SE","cs.AI","cs.LG","cs.PF"],"published":"20230215","updated":"20230221","primary_category":"cs.SE"}
{"arxiv_id":"1804.06559","title":"SFace: An Efficient Network for Face Detection in Large Scale Variations","abstract":"Face detection serves as a fundamental research topic for many applications like face recognition. Impressive progress has been made especially with the recent development of convolutional neural networks. However, the issue of large scale variations, which widely exists in high resolution images\/videos, has not been well addressed in the literature. In this paper, we present a novel algorithm called SFace, which efficiently integrates the anchor-based method and anchor-free method to address the scale issues. A new dataset called 4K-Face is also introduced to evaluate the performance of face detection with extreme large scale variations. The SFace architecture shows promising results on the new 4K-Face benchmarks. In addition, our method can run at 50 frames per second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset, which outperforms the state-of-art algorithms by almost one order of magnitude in speed while achieves comparative performance.","source":"http:\/\/arxiv.org\/pdf\/1804.06559","authors":["Jianfeng Wang","Ye Yuan","Boxun Li","Gang Yu","Sun Jian"],"categories":["cs.CV"],"published":"20180418","updated":"20180423","primary_category":"cs.CV"}
{"arxiv_id":"2201.08239","title":"LaMDA: Language Models for Dialog Applications","abstract":"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.","source":"http:\/\/arxiv.org\/pdf\/2201.08239","authors":["Romal Thoppilan","Daniel De Freitas","Jamie Hall","Noam Shazeer","Apoorv Kulshreshtha","Heng-Tze Cheng","Alicia Jin","Taylor Bos","Leslie Baker","Yu Du","YaGuang Li","Hongrae Lee","Huaixiu Steven Zheng","Amin Ghafouri","Marcelo Menegali","Yanping Huang","Maxim Krikun","Dmitry Lepikhin","James Qin","Dehao Chen","Yuanzhong Xu","Zhifeng Chen","Adam Roberts","Maarten Bosma","Vincent Zhao","Yanqi Zhou","Chung-Ching Chang","Igor Krivokon","Will Rusch","Marc Pickett","Pranesh Srinivasan","Laichee Man","Kathleen Meier-Hellstern","Meredith Ringel Morris","Tulsee Doshi","Renelito Delos Santos","Toju Duke","Johnny Soraker","Ben Zevenbergen","Vinodkumar Prabhakaran","Mark Diaz","Ben Hutchinson","Kristen Olson","Alejandra Molina","Erin Hoffman-John","Josh Lee","Lora Aroyo","Ravi Rajakumar","Alena Butryna","Matthew Lamm","Viktoriya Kuzmina","Joe Fenton","Aaron Cohen","Rachel Bernstein","Ray Kurzweil","Blaise Aguera-Arcas","Claire Cui","Marian Croak","Ed Chi","Quoc Le"],"categories":["cs.CL","cs.AI"],"published":"20220120","updated":"20220210","primary_category":"cs.CL"}
{"arxiv_id":"2210.02969","title":"Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners","abstract":"Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as Flipped, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on average by 8.4% and 9.7% points, respectively. Flipped gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% average F1 score. This indicates that the strong task generalization of Flipped comes from improved generalization to novel labels. We release our code at https:\/\/github.com\/seonghyeonye\/Flipped-Learning.","source":"http:\/\/arxiv.org\/pdf\/2210.02969","authors":["Seonghyeon Ye","Doyoung Kim","Joel Jang","Joongbo Shin","Minjoon Seo"],"categories":["cs.CL"],"published":"20221006","updated":"20230606","primary_category":"cs.CL"}
{"arxiv_id":"2301.13196","title":"Looped Transformers as Programmable Computers","abstract":"We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read\/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.","source":"http:\/\/arxiv.org\/pdf\/2301.13196","authors":["Angeliki Giannou","Shashank Rajput","Jy-yong Sohn","Kangwook Lee","Jason D. Lee","Dimitris Papailiopoulos"],"categories":["cs.LG","cs.AI"],"published":"20230130","updated":"20230130","primary_category":"cs.LG"}
{"arxiv_id":"2006.03654","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","abstract":"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).","source":"http:\/\/arxiv.org\/pdf\/2006.03654","authors":["Pengcheng He","Xiaodong Liu","Jianfeng Gao","Weizhu Chen"],"categories":["cs.CL","cs.LG","cs.CL, cs.GL","I.2; I.7"],"published":"20200605","updated":"20211006","primary_category":"cs.CL"}
{"arxiv_id":"2212.06817","title":"RT-1: Robotics Transformer for Real-World Control at Scale","abstract":"By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io","source":"http:\/\/arxiv.org\/pdf\/2212.06817","authors":["Anthony Brohan","Noah Brown","Justice Carbajal","Yevgen Chebotar","Joseph Dabis","Chelsea Finn","Keerthana Gopalakrishnan","Karol Hausman","Alex Herzog","Jasmine Hsu","Julian Ibarz","Brian Ichter","Alex Irpan","Tomas Jackson","Sally Jesmonth","Nikhil J Joshi","Ryan Julian","Dmitry Kalashnikov","Yuheng Kuang","Isabel Leal","Kuang-Huei Lee","Sergey Levine","Yao Lu","Utsav Malla","Deeksha Manjunath","Igor Mordatch","Ofir Nachum","Carolina Parada","Jodilyn Peralta","Emily Perez","Karl Pertsch","Jornell Quiambao","Kanishka Rao","Michael Ryoo","Grecia Salazar","Pannag Sanketi","Kevin Sayed","Jaspiar Singh","Sumedh Sontakke","Austin Stone","Clayton Tan","Huong Tran","Vincent Vanhoucke","Steve Vega","Quan Vuong","Fei Xia","Ted Xiao","Peng Xu","Sichun Xu","Tianhe Yu","Brianna Zitkovich"],"categories":["cs.RO","cs.AI","cs.CL","cs.CV","cs.LG"],"published":"20221213","updated":"20230811","primary_category":"cs.RO"}
{"arxiv_id":"2002.06305","title":"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping","abstract":"Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.","source":"http:\/\/arxiv.org\/pdf\/2002.06305","authors":["Jesse Dodge","Gabriel Ilharco","Roy Schwartz","Ali Farhadi","Hannaneh Hajishirzi","Noah Smith"],"categories":["cs.CL","cs.LG"],"published":"20200215","updated":"20200215","primary_category":"cs.CL"}
{"arxiv_id":"1806.09055","title":"DARTS: Differentiable Architecture Search","abstract":"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.","source":"http:\/\/arxiv.org\/pdf\/1806.09055","authors":["Hanxiao Liu","Karen Simonyan","Yiming Yang"],"categories":["cs.LG","cs.CL","cs.CV","stat.ML"],"published":"20180624","updated":"20190423","primary_category":"cs.LG"}
{"arxiv_id":"2211.01786","title":"Crosslingual Generalization through Multitask Finetuning","abstract":"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https:\/\/github.com\/bigscience-workshop\/xmtf.","source":"http:\/\/arxiv.org\/pdf\/2211.01786","authors":["Niklas Muennighoff","Thomas Wang","Lintang Sutawika","Adam Roberts","Stella Biderman","Teven Le Scao","M Saiful Bari","Sheng Shen","Zheng-Xin Yong","Hailey Schoelkopf","Xiangru Tang","Dragomir Radev","Alham Fikri Aji","Khalid Almubarak","Samuel Albanie","Zaid Alyafeai","Albert Webson","Edward Raff","Colin Raffel"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20221103","updated":"20230529","primary_category":"cs.CL"}
{"arxiv_id":"1506.01186","title":"Cyclical Learning Rates for Training Neural Networks","abstract":"It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate \"reasonable bounds\" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.","source":"http:\/\/arxiv.org\/pdf\/1506.01186","authors":["Leslie N. Smith"],"categories":["cs.CV","cs.LG","cs.NE"],"published":"20150603","updated":"20170404","primary_category":"cs.CV"}
{"arxiv_id":"2202.12040","title":"Self-Training: A Survey","abstract":"Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this paper, we present self-training methods for binary and multi-class classification; as well as their variants and two related approaches, namely consistency-based approaches and transductive learning. We examine the impact of significant self-training features on various methods, using different general and image classification benchmarks, and we discuss our ideas for future research in self-training. To the best of our knowledge, this is the first thorough and complete survey on this subject.","source":"http:\/\/arxiv.org\/pdf\/2202.12040","authors":["Massih-Reza Amini","Vasilii Feofanov","Loic Pauletto","Lies Hadjadj","Emilie Devijver","Yury Maximov"],"categories":["cs.LG"],"published":"20220224","updated":"20230918","primary_category":"cs.LG"}
{"arxiv_id":"2104.09864","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding","abstract":"Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https:\/\/huggingface.co\/docs\/transformers\/model_doc\/roformer}.","source":"http:\/\/arxiv.org\/pdf\/2104.09864","authors":["Jianlin Su","Yu Lu","Shengfeng Pan","Ahmed Murtadha","Bo Wen","Yunfeng Liu"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20210420","updated":"20220809","primary_category":"cs.CL"}
{"arxiv_id":"2302.13971","title":"LLaMA: Open and Efficient Foundation Language Models","abstract":"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.","source":"http:\/\/arxiv.org\/pdf\/2302.13971","authors":["Hugo Touvron","Thibaut Lavril","Gautier Izacard","Xavier Martinet","Marie-Anne Lachaux","Timoth\u00e9e Lacroix","Baptiste Rozi\u00e8re","Naman Goyal","Eric Hambro","Faisal Azhar","Aurelien Rodriguez","Armand Joulin","Edouard Grave","Guillaume Lample"],"categories":["cs.CL"],"published":"20230227","updated":"20230227","primary_category":"cs.CL"}
{"arxiv_id":"1903.07785","title":"Cloze-driven Pretraining of Self-attention Networks","abstract":"We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with the concurrently introduced BERT model. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.","source":"http:\/\/arxiv.org\/pdf\/1903.07785","authors":["Alexei Baevski","Sergey Edunov","Yinhan Liu","Luke Zettlemoyer","Michael Auli"],"categories":["cs.CL"],"published":"20190319","updated":"20190319","primary_category":"cs.CL"}
{"arxiv_id":"1609.06038","title":"Enhanced LSTM for Natural Language Inference","abstract":"Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.","source":"http:\/\/arxiv.org\/pdf\/1609.06038","authors":["Qian Chen","Xiaodan Zhu","Zhenhua Ling","Si Wei","Hui Jiang","Diana Inkpen"],"categories":["cs.CL"],"published":"20160920","updated":"20170426","primary_category":"cs.CL"}
{"arxiv_id":"2302.07736","title":"Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech","abstract":"Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.","source":"http:\/\/arxiv.org\/pdf\/2302.07736","authors":["Fan Huang","Haewoon Kwak","Jisun An"],"categories":["cs.CL","cs.HC"],"published":"20230211","updated":"20230315","primary_category":"cs.CL"}
{"arxiv_id":"1912.12394","title":"All-in-One Image-Grounded Conversational Agents","abstract":"As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore. In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective. We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks. We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks. Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.","source":"http:\/\/arxiv.org\/pdf\/1912.12394","authors":["Da Ju","Kurt Shuster","Y-Lan Boureau","Jason Weston"],"categories":["cs.CL","cs.CV","cs.LG"],"published":"20191228","updated":"20200115","primary_category":"cs.CL"}
{"arxiv_id":"2108.10934","title":"Mitigating Statistical Bias within Differentially Private Synthetic Data","abstract":"Increasing interest in privacy-preserving machine learning has led to new and evolved approaches for generating private synthetic data from undisclosed real data. However, mechanisms of privacy preservation can significantly reduce the utility of synthetic data, which in turn impacts downstream tasks such as learning predictive models or inference. We propose several re-weighting strategies using privatised likelihood ratios that not only mitigate statistical bias of downstream estimators but also have general applicability to differentially private generative models. Through large-scale empirical evaluation, we show that private importance weighting provides simple and effective privacy-compliant augmentation for general applications of synthetic data.","source":"http:\/\/arxiv.org\/pdf\/2108.10934","authors":["Sahra Ghalebikesabi","Harrison Wilde","Jack Jewson","Arnaud Doucet","Sebastian Vollmer","Chris Holmes"],"categories":["stat.ML","cs.CR","cs.LG"],"published":"20210824","updated":"20220519","primary_category":"cs.CR"}
{"arxiv_id":"2106.09667","title":"Poisoning and Backdooring Contrastive Learning","abstract":"Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.","source":"http:\/\/arxiv.org\/pdf\/2106.09667","authors":["Nicholas Carlini","Andreas Terzis"],"categories":["cs.LG"],"published":"20210617","updated":"20220328","primary_category":"cs.LG"}
{"arxiv_id":"2010.04186","title":"Machine Learning for Gas and Oil Exploration","abstract":"Drilling boreholes for gas and oil extraction is an expensive process and profitability strongly depends on characteristics of the subsurface. As profitability is a key success factor, companies in the industry utilise well logs to explore the subsurface beforehand. These well logs contain various characteristics of the rock around the borehole, which allow petrophysicists to determine the expected amount of contained hydrocarbon. However, these logs are often incomplete and, as a consequence, the subsequent analyses cannot exploit the full potential of the well logs. In this paper we demonstrate that Machine Learning can be applied to \\emph{fill in the gaps} and estimate missing values. We investigate how the amount of training data influences the accuracy of prediction and how to best design regression models (Gradient Boosting and neural network) to obtain optimal results. We then explore the models' predictions both quantitatively, tracking the prediction error, and qualitatively, capturing the evolution of the measured and predicted values for a given property with depth. Combining the findings has enabled us to develop a predictive model that completes the well logs, increasing their quality and potential commercial value.","source":"http:\/\/arxiv.org\/pdf\/2010.04186","authors":["Vito Alexander Nordloh","Anna Roub\u00edckov\u00e1","Nick Brown"],"categories":["cs.LG","physics.geo-ph"],"published":"20201004","updated":"20201004","primary_category":"cs.LG"}
{"arxiv_id":"2104.10350","title":"Carbon Emissions and Large Neural Network Training","abstract":"The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume <1\/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.","source":"http:\/\/arxiv.org\/pdf\/2104.10350","authors":["David Patterson","Joseph Gonzalez","Quoc Le","Chen Liang","Lluis-Miquel Munguia","Daniel Rothchild","David So","Maud Texier","Jeff Dean"],"categories":["cs.LG","cs.CY"],"published":"20210421","updated":"20210423","primary_category":"cs.LG"}
{"arxiv_id":"2210.03070","title":"Toxicity in Multilingual Machine Translation at Scale","abstract":"Machine Translation systems can produce different types of errors, some of which are characterized as critical or catastrophic due to the specific negative impact that they can have on users. In this paper we focus on one type of critical error: added toxicity. We evaluate and analyze added toxicity when translating a large evaluation dataset (HOLISTICBIAS, over 472k sentences, covering 13 demographic axes) from English into 164 languages. An automatic toxicity evaluation shows that added toxicity across languages varies from 0% to 5%. The output languages with the most added toxicity tend to be low-resource ones, and the demographic axes with the most added toxicity include sexual orientation, gender and sex, and ability. We also perform human evaluation on a subset of 8 translation directions, confirming the prevalence of true added toxicity. We use a measurement of the amount of source contribution to the translation, where a low source contribution implies hallucination, to interpret what causes toxicity. Making use of the input attributions allows us to explain toxicity, because the source contributions significantly correlate with toxicity for 84% of languages studied. Given our findings, our recommendations to reduce added toxicity are to curate training data to avoid mistranslations, mitigate hallucination and check unstable translations.","source":"http:\/\/arxiv.org\/pdf\/2210.03070","authors":["Marta R. Costa-juss\u00e0","Eric Smith","Christophe Ropers","Daniel Licht","Jean Maillard","Javier Ferrando","Carlos Escolano"],"categories":["cs.CL","I.2.7"],"published":"20221006","updated":"20230405","primary_category":"cs.CL"}
{"arxiv_id":"2206.05229","title":"Measuring the Carbon Intensity of AI in Cloud Instances","abstract":"By providing unprecedented access to computational resources, cloud computing has enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, precluding development of actionable tactics. Cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions. In this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models for natural language processing and computer vision, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also show that the time of day has notable impact on operational software carbon intensity. Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.","source":"http:\/\/arxiv.org\/pdf\/2206.05229","authors":["Jesse Dodge","Taylor Prewitt","Remi Tachet Des Combes","Erika Odmark","Roy Schwartz","Emma Strubell","Alexandra Sasha Luccioni","Noah A. Smith","Nicole DeCario","Will Buchanan"],"categories":["cs.LG"],"published":"20220610","updated":"20220610","primary_category":"cs.LG"}
{"arxiv_id":"2104.08142","title":"Supervising Model Attention with Human Explanations for Robust Natural Language Inference","abstract":"Natural Language Inference (NLI) models are known to learn from biases and artefacts within their training data, impacting how well they generalise to other unseen datasets. Existing de-biasing approaches focus on preventing the models from learning these biases, which can result in restrictive models and lower performance. We instead investigate teaching the model how a human would approach the NLI task, in order to learn features that will generalise better to previously unseen examples. Using natural language explanations, we supervise the model's attention weights to encourage more attention to be paid to the words present in the explanations, significantly improving model performance. Our experiments show that the in-distribution improvements of this method are also accompanied by out-of-distribution improvements, with the supervised models learning from features that generalise better to other NLI datasets. Analysis of the model indicates that human explanations encourage increased attention on the important words, with more attention paid to words in the premise and less attention paid to punctuation and stop-words.","source":"http:\/\/arxiv.org\/pdf\/2104.08142","authors":["Joe Stacey","Yonatan Belinkov","Marek Rei"],"categories":["cs.CL","cs.LG"],"published":"20210416","updated":"20220501","primary_category":"cs.CL"}
{"arxiv_id":"2301.09211","title":"An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models","abstract":"Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from massive human-written data which contains latent societal biases and toxic contents. In this paper, we leverage the primary task of PTLMs, i.e., language modeling, and propose a new metric to quantify manifested implicit representational harms in PTLMs towards 13 marginalized demographics. Using this metric, we conducted an empirical analysis of 24 widely used PTLMs. Our analysis provides insights into the correlation between the proposed metric in this work and other related metrics for representational harm. We observe that our metric correlates with most of the gender-specific metrics in the literature. Through extensive experiments, we explore the connections between PTLMs architectures and representational harms across two dimensions: depth and width of the networks. We found that prioritizing depth over width, mitigates representational harms in some PTLMs. Our code and data can be found at https:\/\/github.com\/microsoft\/SafeNLP.","source":"http:\/\/arxiv.org\/pdf\/2301.09211","authors":["Saghar Hosseini","Hamid Palangi","Ahmed Hassan Awadallah"],"categories":["cs.CL","cs.AI","I.2.7"],"published":"20230122","updated":"20230122","primary_category":"cs.CL"}
{"arxiv_id":"2109.01247","title":"Do Prompt-Based Models Really Understand the Meaning of their Prompts?","abstract":"Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively \"good\" prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2022). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.","source":"http:\/\/arxiv.org\/pdf\/2109.01247","authors":["Albert Webson","Ellie Pavlick"],"categories":["cs.CL"],"published":"20210902","updated":"20220421","primary_category":"cs.CL"}
{"arxiv_id":"1812.04754","title":"Gradient Descent Happens in a Tiny Subspace","abstract":"We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.","source":"http:\/\/arxiv.org\/pdf\/1812.04754","authors":["Guy Gur-Ari","Daniel A. Roberts","Ethan Dyer"],"categories":["cs.LG","cs.AI","stat.ML"],"published":"20181212","updated":"20181212","primary_category":"cs.LG"}
{"arxiv_id":"1706.05125","title":"Deal or No Deal? End-to-End Learning for Negotiation Dialogues","abstract":"Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https:\/\/github.com\/facebookresearch\/end-to-end-negotiator).","source":"http:\/\/arxiv.org\/pdf\/1706.05125","authors":["Mike Lewis","Denis Yarats","Yann N. Dauphin","Devi Parikh","Dhruv Batra"],"categories":["cs.AI","cs.CL"],"published":"20170616","updated":"20170616","primary_category":"cs.AI"}
{"arxiv_id":"2305.09800","title":"Mirages: On Anthropomorphism in Dialogue Systems","abstract":"Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.","source":"http:\/\/arxiv.org\/pdf\/2305.09800","authors":["Gavin Abercrombie","Amanda Cercas Curry","Tanvi Dinkar","Zeerak Talat"],"categories":["cs.CL"],"published":"20230516","updated":"20230516","primary_category":"cs.CL"}
{"arxiv_id":"2210.11399","title":"Transcending Scaling Laws with 0.1% Extra Compute","abstract":"Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving $\\sim$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.","source":"http:\/\/arxiv.org\/pdf\/2210.11399","authors":["Yi Tay","Jason Wei","Hyung Won Chung","Vinh Q. Tran","David R. So","Siamak Shakeri","Xavier Garcia","Huaixiu Steven Zheng","Jinfeng Rao","Aakanksha Chowdhery","Denny Zhou","Donald Metzler","Slav Petrov","Neil Houlsby","Quoc V. Le","Mostafa Dehghani"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20221020","updated":"20221116","primary_category":"cs.CL"}
{"arxiv_id":"2205.12604","title":"Leveraging QA Datasets to Improve Generative Data Augmentation","abstract":"The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLMs' ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.","source":"http:\/\/arxiv.org\/pdf\/2205.12604","authors":["Dheeraj Mekala","Tu Vu","Timo Schick","Jingbo Shang"],"categories":["cs.CL"],"published":"20220525","updated":"20221025","primary_category":"cs.CL"}
{"arxiv_id":"1902.03545","title":"Task2Vec: Task Embedding for Meta-Learning","abstract":"We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a \"probe network\" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.","source":"http:\/\/arxiv.org\/pdf\/1902.03545","authors":["Alessandro Achille","Michael Lam","Rahul Tewari","Avinash Ravichandran","Subhransu Maji","Charless Fowlkes","Stefano Soatto","Pietro Perona"],"categories":["cs.LG","cs.AI","stat.ML"],"published":"20190210","updated":"20190210","primary_category":"cs.LG"}
{"arxiv_id":"2111.07997","title":"Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection","abstract":"The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.","source":"http:\/\/arxiv.org\/pdf\/2111.07997","authors":["Maarten Sap","Swabha Swayamdipta","Laura Vianna","Xuhui Zhou","Yejin Choi","Noah A. Smith"],"categories":["cs.CL","cs.HC"],"published":"20211115","updated":"20220509","primary_category":"cs.CL"}
{"arxiv_id":"1809.10610","title":"Counterfactual Fairness in Text Classification through Robustness","abstract":"In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that \"Some people are gay\" is toxic while \"Some people are straight\" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.","source":"http:\/\/arxiv.org\/pdf\/1809.10610","authors":["Sahaj Garg","Vincent Perot","Nicole Limtiaco","Ankur Taly","Ed H. Chi","Alex Beutel"],"categories":["cs.LG","stat.ML"],"published":"20180927","updated":"20190213","primary_category":"cs.LG"}
{"arxiv_id":"1707.00061","title":"Racial Disparity in Natural Language Processing: A Case Study of Social Media African-American English","abstract":"We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.","source":"http:\/\/arxiv.org\/pdf\/1707.00061","authors":["Su Lin Blodgett","Brendan O'Connor"],"categories":["cs.CY","cs.CL"],"published":"20170630","updated":"20170630","primary_category":"cs.CY"}
{"arxiv_id":"1909.04387","title":"A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents","abstract":"How should conversational agents respond to verbal abuse through the user? To answer this question, we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. Our results show that some strategies, such as \"polite refusal\" score highly across the board, while for other strategies demographic factors, such as age, as well as the severity of the preceding abuse influence the user's perception of which response is appropriate. In addition, we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness.","source":"http:\/\/arxiv.org\/pdf\/1909.04387","authors":["Amanda Cercas Curry","Verena Rieser"],"categories":["cs.HC","cs.CL"],"published":"20190910","updated":"20190910","primary_category":"cs.HC"}
{"arxiv_id":"2208.11663","title":"PEER: A Collaborative Language Model","abstract":"Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.","source":"http:\/\/arxiv.org\/pdf\/2208.11663","authors":["Timo Schick","Jane Dwivedi-Yu","Zhengbao Jiang","Fabio Petroni","Patrick Lewis","Gautier Izacard","Qingfei You","Christoforos Nalmpantis","Edouard Grave","Sebastian Riedel"],"categories":["cs.CL"],"published":"20220824","updated":"20220824","primary_category":"cs.CL"}
{"arxiv_id":"2205.14135","title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness","abstract":"Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads\/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).","source":"http:\/\/arxiv.org\/pdf\/2205.14135","authors":["Tri Dao","Daniel Y. Fu","Stefano Ermon","Atri Rudra","Christopher R\u00e9"],"categories":["cs.LG"],"published":"20220527","updated":"20220623","primary_category":"cs.LG"}
{"arxiv_id":"2207.00560","title":"Is neural language acquisition similar to natural? A chronological probing study","abstract":"The probing methodology allows one to obtain a partial representation of linguistic phenomena stored in the inner layers of the neural network, using external classifiers and statistical analysis. Pre-trained transformer-based language models are widely used both for natural language understanding (NLU) and natural language generation (NLG) tasks making them most commonly used for downstream applications. However, little analysis was carried out, whether the models were pre-trained enough or contained knowledge correlated with linguistic theory. We are presenting the chronological probing study of transformer English models such as MultiBERT and T5. We sequentially compare the information about the language learned by the models in the process of training on corpora. The results show that 1) linguistic information is acquired in the early stages of training 2) both language models demonstrate capabilities to capture various features from various levels of language, including morphology, syntax, and even discourse, while they also can inconsistently fail on tasks that are perceived as easy. We also introduce the open-source framework for chronological probing research, compatible with other transformer-based models. https:\/\/github.com\/EkaterinaVoloshina\/chronological_probing","source":"http:\/\/arxiv.org\/pdf\/2207.00560","authors":["Ekaterina Voloshina","Oleg Serikov","Tatiana Shavrina"],"categories":["cs.CL"],"published":"20220701","updated":"20220701","primary_category":"cs.CL"}
{"arxiv_id":"2305.09941","title":"\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation","abstract":"Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.","source":"http:\/\/arxiv.org\/pdf\/2305.09941","authors":["Anaelia Ovalle","Palash Goyal","Jwala Dhamala","Zachary Jaggers","Kai-Wei Chang","Aram Galstyan","Richard Zemel","Rahul Gupta"],"categories":["cs.CL","cs.AI","cs.CY","cs.LG","I.2; I.7; K.4"],"published":"20230517","updated":"20230601","primary_category":"cs.CL"}
{"arxiv_id":"2209.07753","title":"Code as Policies: Language Model Programs for Embodied Control","abstract":"Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\"faster\") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https:\/\/code-as-policies.github.io","source":"http:\/\/arxiv.org\/pdf\/2209.07753","authors":["Jacky Liang","Wenlong Huang","Fei Xia","Peng Xu","Karol Hausman","Brian Ichter","Pete Florence","Andy Zeng"],"categories":["cs.RO"],"published":"20220916","updated":"20230525","primary_category":"cs.RO"}
{"arxiv_id":"2006.07235","title":"SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020)","abstract":"We present the results and main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The task involves three subtasks corresponding to the hierarchical taxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The task featured five languages: English, Arabic, Danish, Greek, and Turkish for Subtask A. In addition, English also featured Subtasks B and C. OffensEval 2020 was one of the most popular tasks at SemEval-2020 attracting a large number of participants across all subtasks and also across all languages. A total of 528 teams signed up to participate in the task, 145 teams submitted systems during the evaluation period, and 70 submitted system description papers.","source":"http:\/\/arxiv.org\/pdf\/2006.07235","authors":["Marcos Zampieri","Preslav Nakov","Sara Rosenthal","Pepa Atanasova","Georgi Karadzhov","Hamdy Mubarak","Leon Derczynski","Zeses Pitenis","\u00c7a\u011fr\u0131 \u00c7\u00f6ltekin"],"categories":["cs.CL","68T50, 68T07","I.2.7"],"published":"20200612","updated":"20200930","primary_category":"cs.CL"}
{"arxiv_id":"2207.04672","title":"No Language Left Behind: Scaling Human-Centered Machine Translation","abstract":"Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https:\/\/github.com\/facebookresearch\/fairseq\/tree\/nllb.","source":"http:\/\/arxiv.org\/pdf\/2207.04672","authors":["NLLB Team","Marta R. Costa-juss\u00e0","James Cross","Onur \u00c7elebi","Maha Elbayad","Kenneth Heafield","Kevin Heffernan","Elahe Kalbassi","Janice Lam","Daniel Licht","Jean Maillard","Anna Sun","Skyler Wang","Guillaume Wenzek","Al Youngblood","Bapi Akula","Loic Barrault","Gabriel Mejia Gonzalez","Prangthip Hansanti","John Hoffman","Semarley Jarrett","Kaushik Ram Sadagopan","Dirk Rowe","Shannon Spruit","Chau Tran","Pierre Andrews","Necip Fazil Ayan","Shruti Bhosale","Sergey Edunov","Angela Fan","Cynthia Gao","Vedanuj Goswami","Francisco Guzm\u00e1n","Philipp Koehn","Alexandre Mourachko","Christophe Ropers","Safiyyah Saleem","Holger Schwenk","Jeff Wang"],"categories":["cs.CL","cs.AI","68T50","I.2.7"],"published":"20220711","updated":"20220825","primary_category":"cs.CL"}
{"arxiv_id":"2110.11822","title":"Unraveling the Hidden Environmental Impacts of AI Solutions for Environment","abstract":"In the past ten years, artificial intelligence has encountered such dramatic progress that it is now seen as a tool of choice to solve environmental issues and in the first place greenhouse gas emissions (GHG). At the same time the deep learning community began to realize that training models with more and more parameters requires a lot of energy and as a consequence GHG emissions. To our knowledge, questioning the complete net environmental impacts of AI solutions for the environment (AI for Green), and not only GHG, has never been addressed directly. In this article, we propose to study the possible negative impacts of AI for Green. First, we review the different types of AI impacts, then we present the different methodologies used to assess those impacts, and show how to apply life cycle assessment to AI services. Finally, we discuss how to assess the environmental usefulness of a general AI service, and point out the limitations of existing work in AI for Green.","source":"http:\/\/arxiv.org\/pdf\/2110.11822","authors":["Anne-Laure Ligozat","Julien Lef\u00e8vre","Aur\u00e9lie Bugeau","Jacques Combaz"],"categories":["cs.AI","cs.CY"],"published":"20211022","updated":"20220421","primary_category":"cs.AI"}
{"arxiv_id":"2205.12374","title":"Learning to Model Editing Processes","abstract":"Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits.","source":"http:\/\/arxiv.org\/pdf\/2205.12374","authors":["Machel Reid","Graham Neubig"],"categories":["cs.CL","cs.LG"],"published":"20220524","updated":"20220524","primary_category":"cs.CL"}
{"arxiv_id":"1811.00937","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge","abstract":"When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.","source":"http:\/\/arxiv.org\/pdf\/1811.00937","authors":["Alon Talmor","Jonathan Herzig","Nicholas Lourie","Jonathan Berant"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20181102","updated":"20190315","primary_category":"cs.CL"}
{"arxiv_id":"1902.01007","title":"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference","abstract":"A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area","source":"http:\/\/arxiv.org\/pdf\/1902.01007","authors":["R. Thomas McCoy","Ellie Pavlick","Tal Linzen"],"categories":["cs.CL"],"published":"20190204","updated":"20190624","primary_category":"cs.CL"}
{"arxiv_id":"2105.13231","title":"AndroidEnv: A Reinforcement Learning Platform for Android","abstract":"We introduce AndroidEnv, an open-source platform for Reinforcement Learning (RL) research built on top of the Android ecosystem. AndroidEnv allows RL agents to interact with a wide variety of apps and services commonly used by humans through a universal touchscreen interface. Since agents train on a realistic simulation of an Android device, they have the potential to be deployed on real devices. In this report, we give an overview of the environment, highlighting the significant features it provides for research, and we present an empirical evaluation of some popular reinforcement learning agents on a set of tasks built on this platform.","source":"http:\/\/arxiv.org\/pdf\/2105.13231","authors":["Daniel Toyama","Philippe Hamel","Anita Gergely","Gheorghe Comanici","Amelia Glaese","Zafarali Ahmed","Tyler Jackson","Shibl Mourad","Doina Precup"],"categories":["cs.LG","cs.AI"],"published":"20210527","updated":"20210527","primary_category":"cs.LG"}
{"arxiv_id":"1609.03193","title":"Wav2Letter: an End-to-End ConvNet-based Speech Recognition System","abstract":"This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.","source":"http:\/\/arxiv.org\/pdf\/1609.03193","authors":["Ronan Collobert","Christian Puhrsch","Gabriel Synnaeve"],"categories":["cs.LG","cs.AI","cs.CL","I.2.6; I.2.7"],"published":"20160911","updated":"20160913","primary_category":"cs.LG"}
{"arxiv_id":"2106.11410","title":"A Survey of Race, Racism, and Anti-Racism in NLP","abstract":"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.","source":"http:\/\/arxiv.org\/pdf\/2106.11410","authors":["Anjalie Field","Su Lin Blodgett","Zeerak Waseem","Yulia Tsvetkov"],"categories":["cs.CL"],"published":"20210621","updated":"20210715","primary_category":"cs.CL"}
{"arxiv_id":"1608.03983","title":"SGDR: Stochastic Gradient Descent with Warm Restarts","abstract":"Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https:\/\/github.com\/loshchil\/SGDR","source":"http:\/\/arxiv.org\/pdf\/1608.03983","authors":["Ilya Loshchilov","Frank Hutter"],"categories":["cs.LG","cs.NE","math.OC"],"published":"20160813","updated":"20170503","primary_category":"cs.LG"}
{"arxiv_id":"2203.05115","title":"Internet-augmented language models through few-shot prompting for open-domain question answering","abstract":"In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.","source":"http:\/\/arxiv.org\/pdf\/2203.05115","authors":["Angeliki Lazaridou","Elena Gribovskaya","Wojciech Stokowiec","Nikolai Grigorev"],"categories":["cs.CL","cs.LG"],"published":"20220310","updated":"20220523","primary_category":"cs.CL"}
{"arxiv_id":"2004.09095","title":"The State and Fate of Linguistic Diversity and Inclusion in the NLP World","abstract":"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \"language agnostic\" status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.","source":"http:\/\/arxiv.org\/pdf\/2004.09095","authors":["Pratik Joshi","Sebastin Santy","Amar Budhiraja","Kalika Bali","Monojit Choudhury"],"categories":["cs.CL"],"published":"20200420","updated":"20210127","primary_category":"cs.CL"}
{"arxiv_id":"2302.04761","title":"Toolformer: Language Models Can Teach Themselves to Use Tools","abstract":"Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.","source":"http:\/\/arxiv.org\/pdf\/2302.04761","authors":["Timo Schick","Jane Dwivedi-Yu","Roberto Dess\u00ec","Roberta Raileanu","Maria Lomeli","Luke Zettlemoyer","Nicola Cancedda","Thomas Scialom"],"categories":["cs.CL"],"published":"20230209","updated":"20230209","primary_category":"cs.CL"}
{"arxiv_id":"1902.09666","title":"Predicting the Type and Target of Offensive Posts in Social Media","abstract":"As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.","source":"http:\/\/arxiv.org\/pdf\/1902.09666","authors":["Marcos Zampieri","Shervin Malmasi","Preslav Nakov","Sara Rosenthal","Noura Farra","Ritesh Kumar"],"categories":["cs.CL"],"published":"20190225","updated":"20190416","primary_category":"cs.CL"}
{"arxiv_id":"2210.07382","title":"Behavior Cloned Transformers are Neurosymbolic Reasoners","abstract":"In this work, we explore techniques for augmenting interactive agents with information from symbolic modules, much like humans use tools like calculators and GPS systems to assist with arithmetic and navigation. We test our agent's abilities in text games -- challenging benchmarks for evaluating the multi-step reasoning abilities of game agents in grounded, language-based environments. Our experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games. This action injection technique is easily extended to new agents, environments, and symbolic modules.","source":"http:\/\/arxiv.org\/pdf\/2210.07382","authors":["Ruoyao Wang","Peter Jansen","Marc-Alexandre C\u00f4t\u00e9","Prithviraj Ammanabrolu"],"categories":["cs.CL","cs.AI"],"published":"20221013","updated":"20230211","primary_category":"cs.CL"}
{"arxiv_id":"2211.00295","title":"CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation","abstract":"The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that can process negation effectively, we present CONDAQA, the first English reading comprehension dataset which requires reasoning about the implications of negated statements in paragraphs. We collect paragraphs with diverse negation cues, then have crowdworkers ask questions about the implications of the negated statement in the passage. We also have workers make three kinds of edits to the passage -- paraphrasing the negated statement, changing the scope of the negation, and reversing the negation -- resulting in clusters of question-answer pairs that are difficult for models to answer with spurious shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique negation cues and is challenging for current state-of-the-art models. The best performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our consistency metric, well below human performance which is 81%. We release our dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to facilitate the development of future NLP methods that work on negated language.","source":"http:\/\/arxiv.org\/pdf\/2211.00295","authors":["Abhilasha Ravichander","Matt Gardner","Ana Marasovi\u0107"],"categories":["cs.CL","cs.AI"],"published":"20221101","updated":"20221101","primary_category":"cs.CL"}
{"arxiv_id":"2305.02301","title":"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes","abstract":"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled\/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset. We release the code at: https:\/\/github.com\/google-research\/distilling-step-by-step .","source":"http:\/\/arxiv.org\/pdf\/2305.02301","authors":["Cheng-Yu Hsieh","Chun-Liang Li","Chih-Kuan Yeh","Hootan Nakhost","Yasuhisa Fujii","Alexander Ratner","Ranjay Krishna","Chen-Yu Lee","Tomas Pfister"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20230503","updated":"20230705","primary_category":"cs.CL"}
{"arxiv_id":"1904.01201","title":"Habitat: A Platform for Embodied AI Research","abstract":"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.","source":"http:\/\/arxiv.org\/pdf\/1904.01201","authors":["Manolis Savva","Abhishek Kadian","Oleksandr Maksymets","Yili Zhao","Erik Wijmans","Bhavana Jain","Julian Straub","Jia Liu","Vladlen Koltun","Jitendra Malik","Devi Parikh","Dhruv Batra"],"categories":["cs.CV","cs.AI","cs.CL","cs.LG","cs.RO"],"published":"20190402","updated":"20191125","primary_category":"cs.CV"}
{"arxiv_id":"2209.07858","title":"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned","abstract":"We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.","source":"http:\/\/arxiv.org\/pdf\/2209.07858","authors":["Deep Ganguli","Liane Lovitt","Jackson Kernion","Amanda Askell","Yuntao Bai","Saurav Kadavath","Ben Mann","Ethan Perez","Nicholas Schiefer","Kamal Ndousse","Andy Jones","Sam Bowman","Anna Chen","Tom Conerly","Nova DasSarma","Dawn Drain","Nelson Elhage","Sheer El-Showk","Stanislav Fort","Zac Hatfield-Dodds","Tom Henighan","Danny Hernandez","Tristan Hume","Josh Jacobson","Scott Johnston","Shauna Kravec","Catherine Olsson","Sam Ringer","Eli Tran-Johnson","Dario Amodei","Tom Brown","Nicholas Joseph","Sam McCandlish","Chris Olah","Jared Kaplan","Jack Clark"],"categories":["cs.CL","cs.AI","cs.CY"],"published":"20220823","updated":"20221122","primary_category":"cs.CL"}
{"arxiv_id":"2109.07958","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods","abstract":"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo\/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","source":"http:\/\/arxiv.org\/pdf\/2109.07958","authors":["Stephanie Lin","Jacob Hilton","Owain Evans"],"categories":["cs.CL","cs.AI","cs.CY","cs.LG"],"published":"20210908","updated":"20220508","primary_category":"cs.CL"}
{"arxiv_id":"2301.10226","title":"A Watermark for Large Language Models","abstract":"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.","source":"http:\/\/arxiv.org\/pdf\/2301.10226","authors":["John Kirchenbauer","Jonas Geiping","Yuxin Wen","Jonathan Katz","Ian Miers","Tom Goldstein"],"categories":["cs.LG","cs.CL","cs.CR"],"published":"20230124","updated":"20230606","primary_category":"cs.LG"}
{"arxiv_id":"2001.09977","title":"Towards a Human-like Open-Domain Chatbot","abstract":"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.","source":"http:\/\/arxiv.org\/pdf\/2001.09977","authors":["Daniel Adiwardana","Minh-Thang Luong","David R. So","Jamie Hall","Noah Fiedel","Romal Thoppilan","Zi Yang","Apoorv Kulshreshtha","Gaurav Nemade","Yifeng Lu","Quoc V. Le"],"categories":["cs.CL","cs.LG","cs.NE","stat.ML"],"published":"20200127","updated":"20200227","primary_category":"cs.CL"}
{"arxiv_id":"2212.00193","title":"Distilling Reasoning Capabilities into Smaller Language Models","abstract":"Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https:\/\/github.com\/kumar-shridhar\/Distiiling-LM","source":"http:\/\/arxiv.org\/pdf\/2212.00193","authors":["Kumar Shridhar","Alessandro Stolfo","Mrinmaya Sachan"],"categories":["cs.LG","cs.CL"],"published":"20221201","updated":"20230518","primary_category":"cs.LG"}
{"arxiv_id":"2112.10684","title":"Efficient Large Scale Language Modeling with Mixtures of Experts","abstract":"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.","source":"http:\/\/arxiv.org\/pdf\/2112.10684","authors":["Mikel Artetxe","Shruti Bhosale","Naman Goyal","Todor Mihaylov","Myle Ott","Sam Shleifer","Xi Victoria Lin","Jingfei Du","Srinivasan Iyer","Ramakanth Pasunuru","Giri Anantharaman","Xian Li","Shuohui Chen","Halil Akin","Mandeep Baines","Louis Martin","Xing Zhou","Punit Singh Koura","Brian O'Horo","Jeff Wang","Luke Zettlemoyer","Mona Diab","Zornitsa Kozareva","Ves Stoyanov"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20211220","updated":"20221026","primary_category":"cs.CL"}
{"arxiv_id":"2109.01652","title":"Finetuned Language Models Are Zero-Shot Learners","abstract":"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.","source":"http:\/\/arxiv.org\/pdf\/2109.01652","authors":["Jason Wei","Maarten Bosma","Vincent Y. Zhao","Kelvin Guu","Adams Wei Yu","Brian Lester","Nan Du","Andrew M. Dai","Quoc V. Le"],"categories":["cs.CL"],"published":"20210903","updated":"20220208","primary_category":"cs.CL"}
{"arxiv_id":"2204.14146","title":"Training Language Models with Language Feedback","abstract":"Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization ability.","source":"http:\/\/arxiv.org\/pdf\/2204.14146","authors":["J\u00e9r\u00e9my Scheurer","Jon Ander Campos","Jun Shern Chan","Angelica Chen","Kyunghyun Cho","Ethan Perez"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20220429","updated":"20221117","primary_category":"cs.CL"}
{"arxiv_id":"2207.09983","title":"Diffsound: Discrete Diffusion Model for Text-to-sound Generation","abstract":"Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 \\textit{v.s} 2.786, and the generation speed is five times faster than the AR decoder.","source":"http:\/\/arxiv.org\/pdf\/2207.09983","authors":["Dongchao Yang","Jianwei Yu","Helin Wang","Wen Wang","Chao Weng","Yuexian Zou","Dong Yu"],"categories":["cs.SD","cs.AI","eess.AS"],"published":"20220720","updated":"20230428","primary_category":"cs.SD"}
{"arxiv_id":"2212.08286","title":"ALERT: Adapting Language Models to Reasoning Tasks","abstract":"Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.","source":"http:\/\/arxiv.org\/pdf\/2212.08286","authors":["Ping Yu","Tianlu Wang","Olga Golovneva","Badr AlKhamissi","Siddharth Verma","Zhijing Jin","Gargi Ghosh","Mona Diab","Asli Celikyilmaz"],"categories":["cs.CL"],"published":"20221216","updated":"20230707","primary_category":"cs.CL"}
{"arxiv_id":"1909.03087","title":"ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons","abstract":"While dialogue remains an important end-goal of natural language research, the difficulty of evaluation is an oft-quoted reason why it remains troublesome to make real progress towards its solution. Evaluation difficulties are actually two-fold: not only do automatic metrics not correlate well with human judgments, but also human judgments themselves are in fact difficult to measure. The two most used human judgment tests, single-turn pairwise evaluation and multi-turn Likert scores, both have serious flaws as we discuss in this work. We instead provide a novel procedure involving comparing two full dialogues, where a human judge is asked to pay attention to only one speaker within each, and make a pairwise judgment. The questions themselves are optimized to maximize the robustness of judgments across different annotators, resulting in better tests. We also show how these tests work in self-play model chat setups, resulting in faster, cheaper tests. We hope these tests become the de facto standard, and will release open-source code to that end.","source":"http:\/\/arxiv.org\/pdf\/1909.03087","authors":["Margaret Li","Jason Weston","Stephen Roller"],"categories":["cs.CL"],"published":"20190906","updated":"20190906","primary_category":"cs.CL"}
{"arxiv_id":"1602.02410","title":"Exploring the Limits of Language Modeling","abstract":"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.","source":"http:\/\/arxiv.org\/pdf\/1602.02410","authors":["Rafal Jozefowicz","Oriol Vinyals","Mike Schuster","Noam Shazeer","Yonghui Wu"],"categories":["cs.CL"],"published":"20160207","updated":"20160211","primary_category":"cs.CL"}
{"arxiv_id":"1906.02243","title":"Energy and Policy Considerations for Deep Learning in NLP","abstract":"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.","source":"http:\/\/arxiv.org\/pdf\/1906.02243","authors":["Emma Strubell","Ananya Ganesh","Andrew McCallum"],"categories":["cs.CL"],"published":"20190605","updated":"20190605","primary_category":"cs.CL"}
{"arxiv_id":"2209.14375","title":"Improving alignment of dialogue agents via targeted human judgements","abstract":"We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.","source":"http:\/\/arxiv.org\/pdf\/2209.14375","authors":["Amelia Glaese","Nat McAleese","Maja Tr\u0119bacz","John Aslanides","Vlad Firoiu","Timo Ewalds","Maribeth Rauh","Laura Weidinger","Martin Chadwick","Phoebe Thacker","Lucy Campbell-Gillingham","Jonathan Uesato","Po-Sen Huang","Ramona Comanescu","Fan Yang","Abigail See","Sumanth Dathathri","Rory Greig","Charlie Chen","Doug Fritz","Jaume Sanchez Elias","Richard Green","So\u0148a Mokr\u00e1","Nicholas Fernando","Boxi Wu","Rachel Foley","Susannah Young","Iason Gabriel","William Isaac","John Mellor","Demis Hassabis","Koray Kavukcuoglu","Lisa Anne Hendricks","Geoffrey Irving"],"categories":["cs.LG","cs.CL"],"published":"20220928","updated":"20220928","primary_category":"cs.LG"}
{"arxiv_id":"2204.07580","title":"mGPT: Few-Shot Learners Go Multilingual","abstract":"Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model to choose the most optimal multilingual tokenization strategy. We measure the model perplexity in all covered languages and evaluate it on the wide spectre of multilingual tasks, including classification, generative, sequence labeling and knowledge probing. The models were evaluated with the zero-shot and few-shot methods. Furthermore, we compared the classification tasks with the state-of-the-art multilingual model XGLM. source code and the mGPT XL model are publicly released.","source":"http:\/\/arxiv.org\/pdf\/2204.07580","authors":["Oleh Shliazhko","Alena Fenogenova","Maria Tikhonova","Vladislav Mikhailov","Anastasia Kozlova","Tatiana Shavrina"],"categories":["cs.CL","cs.AI","68-06, 68-04, 68T50, 68T01","I.2; I.2.7"],"published":"20220415","updated":"20220415","primary_category":"cs.CL"}
{"arxiv_id":"2303.12712","title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4","abstract":"Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.","source":"http:\/\/arxiv.org\/pdf\/2303.12712","authors":["S\u00e9bastien Bubeck","Varun Chandrasekaran","Ronen Eldan","Johannes Gehrke","Eric Horvitz","Ece Kamar","Peter Lee","Yin Tat Lee","Yuanzhi Li","Scott Lundberg","Harsha Nori","Hamid Palangi","Marco Tulio Ribeiro","Yi Zhang"],"categories":["cs.CL","cs.AI"],"published":"20230322","updated":"20230413","primary_category":"cs.CL"}
{"arxiv_id":"2008.12348","title":"Neural Generation Meets Real People: Towards Emotionally Engaging Mixed-Initiative Conversations","abstract":"We present Chirpy Cardinal, an open-domain dialogue agent, as a research platform for the 2019 Alexa Prize competition. Building an open-domain socialbot that talks to real people is challenging - such a system must meet multiple user expectations such as broad world knowledge, conversational style, and emotional connection. Our socialbot engages users on their terms - prioritizing their interests, feelings and autonomy. As a result, our socialbot provides a responsive, personalized user experience, capable of talking knowledgeably about a wide variety of topics, as well as chatting empathetically about ordinary life. Neural generation plays a key role in achieving these goals, providing the backbone for our conversational and emotional tone. At the end of the competition, Chirpy Cardinal progressed to the finals with an average rating of 3.6\/5.0, a median conversation duration of 2 minutes 16 seconds, and a 90th percentile duration of over 12 minutes.","source":"http:\/\/arxiv.org\/pdf\/2008.12348","authors":["Ashwin Paranjape","Abigail See","Kathleen Kenealy","Haojun Li","Amelia Hardy","Peng Qi","Kaushik Ram Sadagopan","Nguyet Minh Phu","Dilara Soylu","Christopher D. Manning"],"categories":["cs.CL","cs.AI"],"published":"20200827","updated":"20200905","primary_category":"cs.CL"}
{"arxiv_id":"1906.07337","title":"Measuring Bias in Contextualized Word Representations","abstract":"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1)~propose a template-based method to quantify bias in BERT; (2)~show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3)~conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.","source":"http:\/\/arxiv.org\/pdf\/1906.07337","authors":["Keita Kurita","Nidhi Vyas","Ayush Pareek","Alan W Black","Yulia Tsvetkov"],"categories":["cs.CL"],"published":"20190618","updated":"20190618","primary_category":"cs.CL"}
{"arxiv_id":"2006.06217","title":"SECure: A Social and Environmental Certificate for AI Systems","abstract":"In a world increasingly dominated by AI applications, an understudied aspect is the carbon and social footprint of these power-hungry algorithms that require copious computation and a trove of data for training and prediction. While profitable in the short-term, these practices are unsustainable and socially extractive from both a data-use and energy-use perspective. This work proposes an ESG-inspired framework combining socio-technical measures to build eco-socially responsible AI systems. The framework has four pillars: compute-efficient machine learning, federated learning, data sovereignty, and a LEEDesque certificate. Compute-efficient machine learning is the use of compressed network architectures that show marginal decreases in accuracy. Federated learning augments the first pillar's impact through the use of techniques that distribute computational loads across idle capacity on devices. This is paired with the third pillar of data sovereignty to ensure the privacy of user data via techniques like use-based privacy and differential privacy. The final pillar ties all these factors together and certifies products and services in a standardized manner on their environmental and social impacts, allowing consumers to align their purchase with their values.","source":"http:\/\/arxiv.org\/pdf\/2006.06217","authors":["Abhishek Gupta","Camylle Lanteigne","Sara Kingsley"],"categories":["cs.CY","cs.AI","cs.LG","econ.GN","q-fin.EC"],"published":"20200611","updated":"20200719","primary_category":"cs.CY"}
{"arxiv_id":"2110.07574","title":"Can Machines Learn Morality? The Delphi Experiment","abstract":"As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it. To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., \"helping a friend\" is generally good, while \"helping a friend spread fake news\" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense. Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.","source":"http:\/\/arxiv.org\/pdf\/2110.07574","authors":["Liwei Jiang","Jena D. Hwang","Chandra Bhagavatula","Ronan Le Bras","Jenny Liang","Jesse Dodge","Keisuke Sakaguchi","Maxwell Forbes","Jon Borchardt","Saadia Gabriel","Yulia Tsvetkov","Oren Etzioni","Maarten Sap","Regina Rini","Yejin Choi"],"categories":["cs.CL"],"published":"20211014","updated":"20220712","primary_category":"cs.CL"}
{"arxiv_id":"2007.07399","title":"Bringing the People Back In: Contesting Benchmark Machine Learning Datasets","abstract":"In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to \"bring the people back in\" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.","source":"http:\/\/arxiv.org\/pdf\/2007.07399","authors":["Emily Denton","Alex Hanna","Razvan Amironesei","Andrew Smart","Hilary Nicole","Morgan Klaus Scheuerman"],"categories":["cs.CY"],"published":"20200714","updated":"20200714","primary_category":"cs.CY"}
{"arxiv_id":"1704.04683","title":"RACE: Large-scale ReAding Comprehension Dataset From Examinations","abstract":"We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http:\/\/www.cs.cmu.edu\/~glai1\/data\/race\/ and the code is available at https:\/\/github.com\/qizhex\/RACE_AR_baselines.","source":"http:\/\/arxiv.org\/pdf\/1704.04683","authors":["Guokun Lai","Qizhe Xie","Hanxiao Liu","Yiming Yang","Eduard Hovy"],"categories":["cs.CL","cs.AI","cs.LG"],"published":"20170415","updated":"20171205","primary_category":"cs.CL"}
{"arxiv_id":"1911.03891","title":"Social Bias Frames: Reasoning about Social and Power Implications of Language","abstract":"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that \"we shouldn't lower our standards to hire more women,\" most listeners will infer the implicature intended by the speaker -- that \"women (candidates) are less qualified.\" Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text. We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.","source":"http:\/\/arxiv.org\/pdf\/1911.03891","authors":["Maarten Sap","Saadia Gabriel","Lianhui Qin","Dan Jurafsky","Noah A. Smith","Yejin Choi"],"categories":["cs.CL"],"published":"20191110","updated":"20200423","primary_category":"cs.CL"}
{"arxiv_id":"1904.10509","title":"Generating Long Sequences with Sparse Transformers","abstract":"Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.","source":"http:\/\/arxiv.org\/pdf\/1904.10509","authors":["Rewon Child","Scott Gray","Alec Radford","Ilya Sutskever"],"categories":["cs.LG","stat.ML"],"published":"20190423","updated":"20190423","primary_category":"cs.LG"}
{"arxiv_id":"2111.02080","title":"An Explanation of In-context Learning as Implicit Bayesian Inference","abstract":"Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.","source":"http:\/\/arxiv.org\/pdf\/2111.02080","authors":["Sang Michael Xie","Aditi Raghunathan","Percy Liang","Tengyu Ma"],"categories":["cs.CL","cs.LG"],"published":"20211103","updated":"20220721","primary_category":"cs.CL"}
{"arxiv_id":"1912.01412","title":"Deep Learning for Symbolic Mathematics","abstract":"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.","source":"http:\/\/arxiv.org\/pdf\/1912.01412","authors":["Guillaume Lample","Fran\u00e7ois Charton"],"categories":["cs.SC","cs.LG"],"published":"20191202","updated":"20191202","primary_category":"cs.SC"}
{"arxiv_id":"2010.03058","title":"Characterising Bias in Compressed Models","abstract":"The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. These techniques achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides disproportionately high errors on a small subset of examples; we call this subset Compression Identified Exemplars (CIE). We further establish that for CIE examples, compression amplifies existing algorithmic bias. Pruning disproportionately impacts performance on underrepresented features, which often coincides with considerations of fairness. Given that CIE is a relatively small subset but a great contributor of error in the model, we propose its use as a human-in-the-loop auditing tool to surface a tractable subset of the dataset for further inspection or annotation by a domain expert. We provide qualitative and quantitative support that CIE surfaces the most challenging examples in the data distribution for human-in-the-loop auditing.","source":"http:\/\/arxiv.org\/pdf\/2010.03058","authors":["Sara Hooker","Nyalleng Moorosi","Gregory Clark","Samy Bengio","Emily Denton"],"categories":["cs.LG","cs.AI"],"published":"20201006","updated":"20201218","primary_category":"cs.LG"}
{"arxiv_id":"1705.03551","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension","abstract":"We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http:\/\/nlp.cs.washington.edu\/triviaqa\/","source":"http:\/\/arxiv.org\/pdf\/1705.03551","authors":["Mandar Joshi","Eunsol Choi","Daniel S. Weld","Luke Zettlemoyer"],"categories":["cs.CL"],"published":"20170509","updated":"20170513","primary_category":"cs.CL"}
{"arxiv_id":"2210.02406","title":"Decomposed Prompting: A Modular Approach for Solving Complex Tasks","abstract":"Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https:\/\/github.com\/allenai\/DecomP.","source":"http:\/\/arxiv.org\/pdf\/2210.02406","authors":["Tushar Khot","Harsh Trivedi","Matthew Finlayson","Yao Fu","Kyle Richardson","Peter Clark","Ashish Sabharwal"],"categories":["cs.CL"],"published":"20221005","updated":"20230411","primary_category":"cs.CL"}
{"arxiv_id":"2211.02001","title":"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model","abstract":"Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of~\\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.","source":"http:\/\/arxiv.org\/pdf\/2211.02001","authors":["Alexandra Sasha Luccioni","Sylvain Viguier","Anne-Laure Ligozat"],"categories":["cs.LG"],"published":"20221103","updated":"20221103","primary_category":"cs.LG"}
{"arxiv_id":"2104.08728","title":"Revealing Persona Biases in Dialogue Systems","abstract":"Dialogue systems in the form of chatbots and personal assistants are being increasingly integrated into people's lives. Modern dialogue systems may consider adopting anthropomorphic personas, mimicking societal demographic groups to appear more approachable and trustworthy to users. However, the adoption of a persona can result in the adoption of biases. In this paper, we present the first large-scale study on persona biases in dialogue systems and conduct analyses on personas of different social classes, sexual orientations, races, and genders. We define persona biases as harmful differences in responses (e.g., varying levels of offensiveness, agreement with harmful statements) generated from adopting different demographic personas. Furthermore, we introduce an open-source framework, UnitPersonaBias, to explore and aggregate persona biases in dialogue systems. By analyzing the Blender and DialoGPT dialogue systems, we observe that adopting personas can actually decrease harmful responses, compared to not using any personas. Additionally, we find that persona choices can affect the degree of harms in generated responses and thus should be systematically evaluated before deployment. We also analyze how personas can result in different amounts of harm towards specific demographics.","source":"http:\/\/arxiv.org\/pdf\/2104.08728","authors":["Emily Sheng","Josh Arnold","Zhou Yu","Kai-Wei Chang","Nanyun Peng"],"categories":["cs.CL"],"published":"20210418","updated":"20211215","primary_category":"cs.CL"}
{"arxiv_id":"2305.03047","title":"Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision","abstract":"Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.","source":"http:\/\/arxiv.org\/pdf\/2305.03047","authors":["Zhiqing Sun","Yikang Shen","Qinhong Zhou","Hongxin Zhang","Zhenfang Chen","David Cox","Yiming Yang","Chuang Gan"],"categories":["cs.LG","cs.AI","cs.CL","cs.CY"],"published":"20230504","updated":"20230504","primary_category":"cs.LG"}
{"arxiv_id":"2210.01241","title":"Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization","abstract":"We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.","source":"http:\/\/arxiv.org\/pdf\/2210.01241","authors":["Rajkumar Ramamurthy","Prithviraj Ammanabrolu","Kiant\u00e9 Brantley","Jack Hessel","Rafet Sifa","Christian Bauckhage","Hannaneh Hajishirzi","Yejin Choi"],"categories":["cs.CL","cs.LG"],"published":"20221003","updated":"20230301","primary_category":"cs.CL"}
{"arxiv_id":"2204.06745","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model","abstract":"We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \\model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https:\/\/github.com\/EleutherAI\/gpt-neox.","source":"http:\/\/arxiv.org\/pdf\/2204.06745","authors":["Sid Black","Stella Biderman","Eric Hallahan","Quentin Anthony","Leo Gao","Laurence Golding","Horace He","Connor Leahy","Kyle McDonell","Jason Phang","Michael Pieler","USVSN Sai Prashanth","Shivanshu Purohit","Laria Reynolds","Jonathan Tow","Ben Wang","Samuel Weinbach"],"categories":["cs.CL"],"published":"20220414","updated":"20220414","primary_category":"cs.CL"}
{"arxiv_id":"1707.06347","title":"Proximal Policy Optimization Algorithms","abstract":"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.","source":"http:\/\/arxiv.org\/pdf\/1707.06347","authors":["John Schulman","Filip Wolski","Prafulla Dhariwal","Alec Radford","Oleg Klimov"],"categories":["cs.LG"],"published":"20170720","updated":"20170828","primary_category":"cs.LG"}
{"arxiv_id":"2307.09288","title":"Llama 2: Open Foundation and Fine-Tuned Chat Models","abstract":"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","source":"http:\/\/arxiv.org\/pdf\/2307.09288","authors":["Hugo Touvron","Louis Martin","Kevin Stone","Peter Albert","Amjad Almahairi","Yasmine Babaei","Nikolay Bashlykov","Soumya Batra","Prajjwal Bhargava","Shruti Bhosale","Dan Bikel","Lukas Blecher","Cristian Canton Ferrer","Moya Chen","Guillem Cucurull","David Esiobu","Jude Fernandes","Jeremy Fu","Wenyin Fu","Brian Fuller","Cynthia Gao","Vedanuj Goswami","Naman Goyal","Anthony Hartshorn","Saghar Hosseini","Rui Hou","Hakan Inan","Marcin Kardas","Viktor Kerkez","Madian Khabsa","Isabel Kloumann","Artem Korenev","Punit Singh Koura","Marie-Anne Lachaux","Thibaut Lavril","Jenya Lee","Diana Liskovich","Yinghai Lu","Yuning Mao","Xavier Martinet","Todor Mihaylov","Pushkar Mishra","Igor Molybog","Yixin Nie","Andrew Poulton","Jeremy Reizenstein","Rashi Rungta","Kalyan Saladi","Alan Schelten","Ruan Silva","Eric Michael Smith","Ranjan Subramanian","Xiaoqing Ellen Tan","Binh Tang","Ross Taylor","Adina Williams","Jian Xiang Kuan","Puxin Xu","Zheng Yan","Iliyan Zarov","Yuchen Zhang","Angela Fan","Melanie Kambadur","Sharan Narang","Aurelien Rodriguez","Robert Stojnic","Sergey Edunov","Thomas Scialom"],"categories":["cs.CL","cs.AI"],"published":"20230718","updated":"20230719","primary_category":"cs.CL"}
{"arxiv_id":"2211.05100","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","abstract":"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.","source":"http:\/\/arxiv.org\/pdf\/2211.05100","authors":["BigScience Workshop",":","Teven Le Scao","Angela Fan","Christopher Akiki","Ellie Pavlick","Suzana Ili\u0107","Daniel Hesslow","Roman Castagn\u00e9","Alexandra Sasha Luccioni","Fran\u00e7ois Yvon","Matthias Gall\u00e9","Jonathan Tow","Alexander M. Rush","Stella Biderman","Albert Webson","Pawan Sasanka Ammanamanchi","Thomas Wang","Beno\u00eet Sagot","Niklas Muennighoff","Albert Villanova del Moral","Olatunji Ruwase","Rachel Bawden","Stas Bekman","Angelina McMillan-Major","Iz Beltagy","Huu Nguyen","Lucile Saulnier","Samson Tan","Pedro Ortiz Suarez","Victor Sanh","Hugo Lauren\u00e7on","Yacine Jernite","Julien Launay","Margaret Mitchell","Colin Raffel","Aaron Gokaslan","Adi Simhi","Aitor Soroa","Alham Fikri Aji","Amit Alfassy","Anna Rogers","Ariel Kreisberg Nitzav","Canwen Xu","Chenghao Mou","Chris Emezue","Christopher Klamm","Colin Leong","Daniel van Strien","David Ifeoluwa Adelani","Dragomir Radev","Eduardo Gonz\u00e1lez Ponferrada","Efrat Levkovizh","Ethan Kim","Eyal Bar Natan","Francesco De Toni","G\u00e9rard Dupont","Germ\u00e1n Kruszewski","Giada Pistilli","Hady Elsahar","Hamza Benyamina","Hieu Tran","Ian Yu","Idris Abdulmumin","Isaac Johnson","Itziar Gonzalez-Dios","Javier de la Rosa","Jenny Chim","Jesse Dodge","Jian Zhu","Jonathan Chang","J\u00f6rg Frohberg","Joseph Tobing","Joydeep Bhattacharjee","Khalid Almubarak","Kimbo Chen","Kyle Lo","Leandro Von Werra","Leon Weber","Long Phan","Loubna Ben allal","Ludovic Tanguy","Manan Dey","Manuel Romero Mu\u00f1oz","Maraim Masoud","Mar\u00eda Grandury","Mario \u0160a\u0161ko","Max Huang","Maximin Coavoux","Mayank Singh","Mike Tian-Jian Jiang","Minh Chien Vu","Mohammad A. Jauhar","Mustafa Ghaleb","Nishant Subramani","Nora Kassner","Nurulaqilla Khamis","Olivier Nguyen","Omar Espejel","Ona de Gibert","Paulo Villegas","Peter Henderson","Pierre Colombo","Priscilla Amuok","Quentin Lhoest","Rheza Harliman","Rishi Bommasani","Roberto Luis L\u00f3pez","Rui Ribeiro","Salomey Osei","Sampo Pyysalo","Sebastian Nagel","Shamik Bose","Shamsuddeen Hassan Muhammad","Shanya Sharma","Shayne Longpre","Somaieh Nikpoor","Stanislav Silberberg","Suhas Pai","Sydney Zink","Tiago Timponi Torrent","Timo Schick","Tristan Thrush","Valentin Danchev","Vassilina Nikoulina","Veronika Laippala","Violette Lepercq","Vrinda Prabhu","Zaid Alyafeai","Zeerak Talat","Arun Raja","Benjamin Heinzerling","Chenglei Si","Davut Emre Ta\u015far","Elizabeth Salesky","Sabrina J. Mielke","Wilson Y. Lee","Abheesht Sharma","Andrea Santilli","Antoine Chaffin","Arnaud Stiegler","Debajyoti Datta","Eliza Szczechla","Gunjan Chhablani","Han Wang","Harshit Pandey","Hendrik Strobelt","Jason Alan Fries","Jos Rozen","Leo Gao","Lintang Sutawika","M Saiful Bari","Maged S. Al-shaibani","Matteo Manica","Nihal Nayak","Ryan Teehan","Samuel Albanie","Sheng Shen","Srulik Ben-David","Stephen H. Bach","Taewoon Kim","Tali Bers","Thibault Fevry","Trishala Neeraj","Urmish Thakker","Vikas Raunak","Xiangru Tang","Zheng-Xin Yong","Zhiqing Sun","Shaked Brody","Yallow Uri","Hadar Tojarieh","Adam Roberts","Hyung Won Chung","Jaesung Tae","Jason Phang","Ofir Press","Conglong Li","Deepak Narayanan","Hatim Bourfoune","Jared Casper","Jeff Rasley","Max Ryabinin","Mayank Mishra","Minjia Zhang","Mohammad Shoeybi","Myriam Peyrounette","Nicolas Patry","Nouamane Tazi","Omar Sanseviero","Patrick von Platen","Pierre Cornette","Pierre Fran\u00e7ois Lavall\u00e9e","R\u00e9mi Lacroix","Samyam Rajbhandari","Sanchit Gandhi","Shaden Smith","St\u00e9phane Requena","Suraj Patil","Tim Dettmers","Ahmed Baruwa","Amanpreet Singh","Anastasia Cheveleva","Anne-Laure Ligozat","Arjun Subramonian","Aur\u00e9lie N\u00e9v\u00e9ol","Charles Lovering","Dan Garrette","Deepak Tunuguntla","Ehud Reiter","Ekaterina Taktasheva","Ekaterina Voloshina","Eli Bogdanov","Genta Indra Winata","Hailey Schoelkopf","Jan-Christoph Kalo","Jekaterina Novikova","Jessica Zosa Forde","Jordan Clive","Jungo Kasai","Ken Kawamura","Liam Hazan","Marine Carpuat","Miruna Clinciu","Najoung Kim","Newton Cheng","Oleg Serikov","Omer Antverg","Oskar van der Wal","Rui Zhang","Ruochen Zhang","Sebastian Gehrmann","Shachar Mirkin","Shani Pais","Tatiana Shavrina","Thomas Scialom","Tian Yun","Tomasz Limisiewicz","Verena Rieser","Vitaly Protasov","Vladislav Mikhailov","Yada Pruksachatkun","Yonatan Belinkov","Zachary Bamberger","Zden\u011bk Kasner","Alice Rueda","Amanda Pestana","Amir Feizpour","Ammar Khan","Amy Faranak","Ana Santos","Anthony Hevia","Antigona Unldreaj","Arash Aghagol","Arezoo Abdollahi","Aycha Tammour","Azadeh HajiHosseini","Bahareh Behroozi","Benjamin Ajibade","Bharat Saxena","Carlos Mu\u00f1oz Ferrandis","Daniel McDuff","Danish Contractor","David Lansky","Davis David","Douwe Kiela","Duong A. Nguyen","Edward Tan","Emi Baylor","Ezinwanne Ozoani","Fatima Mirza","Frankline Ononiwu","Habib Rezanejad","Hessie Jones","Indrani Bhattacharya","Irene Solaiman","Irina Sedenko","Isar Nejadgholi","Jesse Passmore","Josh Seltzer","Julio Bonis Sanz","Livia Dutra","Mairon Samagaio","Maraim Elbadri","Margot Mieskes","Marissa Gerchick","Martha Akinlolu","Michael McKenna","Mike Qiu","Muhammed Ghauri","Mykola Burynok","Nafis Abrar","Nazneen Rajani","Nour Elkott","Nour Fahmy","Olanrewaju Samuel","Ran An","Rasmus Kromann","Ryan Hao","Samira Alizadeh","Sarmad Shubber","Silas Wang","Sourav Roy","Sylvain Viguier","Thanh Le","Tobi Oyebade","Trieu Le","Yoyo Yang","Zach Nguyen","Abhinav Ramesh Kashyap","Alfredo Palasciano","Alison Callahan","Anima Shukla","Antonio Miranda-Escalada","Ayush Singh","Benjamin Beilharz","Bo Wang","Caio Brito","Chenxi Zhou","Chirag Jain","Chuxin Xu","Cl\u00e9mentine Fourrier","Daniel Le\u00f3n Peri\u00f1\u00e1n","Daniel Molano","Dian Yu","Enrique Manjavacas","Fabio Barth","Florian Fuhrimann","Gabriel Altay","Giyaseddin Bayrak","Gully Burns","Helena U. Vrabec","Imane Bello","Ishani Dash","Jihyun Kang","John Giorgi","Jonas Golde","Jose David Posada","Karthik Rangasai Sivaraman","Lokesh Bulchandani","Lu Liu","Luisa Shinzato","Madeleine Hahn de Bykhovetz","Maiko Takeuchi","Marc P\u00e0mies","Maria A Castillo","Marianna Nezhurina","Mario S\u00e4nger","Matthias Samwald","Michael Cullan","Michael Weinberg","Michiel De Wolf","Mina Mihaljcic","Minna Liu","Moritz Freidank","Myungsun Kang","Natasha Seelam","Nathan Dahlberg","Nicholas Michio Broad","Nikolaus Muellner","Pascale Fung","Patrick Haller","Ramya Chandrasekhar","Renata Eisenberg","Robert Martin","Rodrigo Canalli","Rosaline Su","Ruisi Su","Samuel Cahyawijaya","Samuele Garda","Shlok S Deshmukh","Shubhanshu Mishra","Sid Kiblawi","Simon Ott","Sinee Sang-aroonsiri","Srishti Kumar","Stefan Schweter","Sushil Bharati","Tanmay Laud","Th\u00e9o Gigant","Tomoya Kainuma","Wojciech Kusa","Yanis Labrak","Yash Shailesh Bajaj","Yash Venkatraman","Yifan Xu","Yingxin Xu","Yu Xu","Zhe Tan","Zhongli Xie","Zifan Ye","Mathilde Bras","Younes Belkada","Thomas Wolf"],"categories":["cs.CL"],"published":"20221109","updated":"20230627","primary_category":"cs.CL"}
{"arxiv_id":"2009.10031","title":"Training Production Language Models without Memorizing User Data","abstract":"This paper presents the first consumer-scale next-word prediction (NWP) model trained with Federated Learning (FL) while leveraging the Differentially Private Federated Averaging (DP-FedAvg) technique. There has been prior work on building practical FL infrastructure, including work demonstrating the feasibility of training language models on mobile devices using such infrastructure. It has also been shown (in simulations on a public corpus) that it is possible to train NWP models with user-level differential privacy using the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models with DP-FedAvg in a real-world production environment on a heterogeneous fleet of mobile phones requires addressing numerous challenges. For instance, the coordinating central server has to keep track of the devices available at the start of each round and sample devices uniformly at random from them, while ensuring \\emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL work of which we are aware, for the first time we demonstrate the deployment of a differentially private mechanism for the training of a production neural network in FL, as well as the instrumentation of the production training infrastructure to perform an end-to-end empirical measurement of unintended memorization.","source":"http:\/\/arxiv.org\/pdf\/2009.10031","authors":["Swaroop Ramaswamy","Om Thakkar","Rajiv Mathews","Galen Andrew","H. Brendan McMahan","Fran\u00e7oise Beaufays"],"categories":["cs.LG","cs.CR","stat.ML"],"published":"20200921","updated":"20200921","primary_category":"cs.LG"}
