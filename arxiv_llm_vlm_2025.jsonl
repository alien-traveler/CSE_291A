{"id": "2510.18876v1", "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for   Multimodal LLMs", "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.", "authors": ["Haochen Wang", "Yuhao Wang", "Tao Zhang", "Yikang Zhou", "Yanwei Li", "Jiacong Wang", "Ye Tian", "Jiahao Meng", "Zilong Huang", "Guangcan Mai", "Anran Wang", "Yunhai Tong", "Zhuochen Wang", "Xiangtai Li", "Zhaoxiang Zhang"], "published": "2025-10-21T17:59:59Z", "updated": "2025-10-21T17:59:59Z", "link_pdf": "http://arxiv.org/pdf/2510.18876v1", "link_page": "http://arxiv.org/abs/2510.18876v1"}
{"id": "2510.18873v1", "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence", "summary": "Reasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly 1,000 dynamic videos and over 1,700 manually annotated questions covering nine decoupled motion patterns of observers and objects. Spatially and temporally symmetric designs reduce biases and enable systematic evaluation of models' reasoning about self-motion and object motion. Our evaluation of 14 VLMs and expert models reveals key limitations: models often conflate observer and object motion, exhibit semantic biases, and fail to accurately infer relative relationships in dynamic scenarios. Our DSI-Bench provides valuable findings and insights about the future development of general and expertise models with dynamic spatial intelligence.", "authors": ["Ziang Zhang", "Zehan Wang", "Guanghao Zhang", "Weilong Dai", "Yan Xia", "Ziang Yan", "Minjie Hong", "Zhou Zhao"], "published": "2025-10-21T17:59:36Z", "updated": "2025-10-21T17:59:36Z", "link_pdf": "http://arxiv.org/pdf/2510.18873v1", "link_page": "http://arxiv.org/abs/2510.18873v1"}
{"id": "2510.18871v1", "title": "How Do LLMs Use Their Depth?", "summary": "Growing evidence suggests that large language models do not use their depth uniformly, yet we still lack a fine-grained understanding of their layer-wise prediction dynamics. In this paper, we trace the intermediate representations of several open-weight models during inference and reveal a structured and nuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework that explains how LLMs internally structure their computations to make predictions. We first show that the top-ranked predictions in early LLM layers are composed primarily of high-frequency tokens, which act as statistical guesses proposed by the model early on due to the lack of appropriate contextual information. As contextual information develops deeper into the model, these initial guesses get refined into contextually appropriate tokens. Even high-frequency token predictions from early layers get refined >70% of the time, indicating that correct token prediction is not \"one-and-done\". We then go beyond frequency-based prediction to examine the dynamic usage of layer depth across three case studies. (i) Part-of-speech analysis shows that function words are, on average, the earliest to be predicted correctly. (ii) Fact recall task analysis shows that, in a multi-token answer, the first token requires more computational depth than the rest. (iii) Multiple-choice task analysis shows that the model identifies the format of the response within the first half of the layers, but finalizes its response only toward the end. Together, our results provide a detailed view of depth usage in LLMs, shedding light on the layer-by-layer computations that underlie successful predictions and providing insights for future works to improve computational efficiency in transformer-based models.", "authors": ["Akshat Gupta", "Jay Yeung", "Gopala Anumanchipalli", "Anna Ivanova"], "published": "2025-10-21T17:59:05Z", "updated": "2025-10-21T17:59:05Z", "link_pdf": "http://arxiv.org/pdf/2510.18871v1", "link_page": "http://arxiv.org/abs/2510.18871v1"}
{"id": "2510.18866v1", "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation", "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.", "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"], "published": "2025-10-21T17:58:17Z", "updated": "2025-10-21T17:58:17Z", "link_pdf": "http://arxiv.org/pdf/2510.18866v1", "link_page": "http://arxiv.org/abs/2510.18866v1"}
{"id": "2510.18849v1", "title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit   Reinforcement Learning", "summary": "Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.", "authors": ["Chenghao Zhu", "Meiling Tao", "Tiannan Wang", "Dongyi Ding", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "published": "2025-10-21T17:40:03Z", "updated": "2025-10-21T17:40:03Z", "link_pdf": "http://arxiv.org/pdf/2510.18849v1", "link_page": "http://arxiv.org/abs/2510.18849v1"}
{"id": "2510.18840v1", "title": "See the Text: From Tokenization to Visual Reading", "summary": "People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.", "authors": ["Ling Xing", "Alex Jinpeng Wang", "Rui Yan", "Hongyu Qu", "Zechao Li", "Jinhui Tang"], "published": "2025-10-21T17:34:48Z", "updated": "2025-10-21T17:34:48Z", "link_pdf": "http://arxiv.org/pdf/2510.18840v1", "link_page": "http://arxiv.org/abs/2510.18840v1"}
{"id": "2510.18837v1", "title": "FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning", "summary": "Federated learning (FL) enables multiple clients to collaboratively train machine learning models without exposing local data, balancing performance and privacy. However, domain shift and label heterogeneity across clients often hinder the generalization of the aggregated global model. Recently, large-scale vision-language models like CLIP have shown strong zero-shot classification capabilities, raising the question of how to effectively fine-tune CLIP across domains in a federated setting. In this work, we propose an adaptive federated prompt tuning framework, FedDEAP, to enhance CLIP's generalization in multi-domain scenarios. Our method includes the following three key components: (1) To mitigate the loss of domain-specific information caused by label-supervised tuning, we disentangle semantic and domain-specific features in images by using semantic and domain transformation networks with unbiased mappings; (2) To preserve domain-specific knowledge during global prompt aggregation, we introduce a dual-prompt design with a global semantic prompt and a local domain prompt to balance shared and personalized information; (3) To maximize the inclusion of semantic and domain information from images in the generated text features, we align textual and visual representations under the two learned transformations to preserve semantic and domain consistency. Theoretical analysis and extensive experiments on four datasets demonstrate the effectiveness of our method in enhancing the generalization of CLIP for federated image recognition across multiple domains.", "authors": ["Yubin Zheng", "Pak-Hei Yeung", "Jing Xia", "Tianjie Ju", "Peng Tang", "Weidong Qiu", "Jagath C. Rajapakse"], "published": "2025-10-21T17:32:44Z", "updated": "2025-10-21T17:32:44Z", "link_pdf": "http://arxiv.org/pdf/2510.18837v1", "link_page": "http://arxiv.org/abs/2510.18837v1"}
{"id": "2510.18830v1", "title": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long   Context Training", "summary": "The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at https://github.com/microsoft/MInference/tree/main/MTraining.", "authors": ["Wenxuan Li", "Chengruidong Zhang", "Huiqiang Jiang", "Yucheng Li", "Yuqing Yang", "Lili Qiu"], "published": "2025-10-21T17:25:32Z", "updated": "2025-10-21T17:25:32Z", "link_pdf": "http://arxiv.org/pdf/2510.18830v1", "link_page": "http://arxiv.org/abs/2510.18830v1"}
{"id": "2510.18817v1", "title": "Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for   Industrial Asset Health Monitoring", "summary": "Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: https://github.com/IBM/FailureSensorIQ.", "authors": ["Shuxin Lin", "Dhaval Patel", "Christodoulos Constantinides"], "published": "2025-10-21T17:18:24Z", "updated": "2025-10-21T17:18:24Z", "link_pdf": "http://arxiv.org/pdf/2510.18817v1", "link_page": "http://arxiv.org/abs/2510.18817v1"}
{"id": "2510.18779v1", "title": "KAT-Coder Technical Report", "summary": "Recent advances in large language models (LLMs) have enabled progress in agentic coding, where models autonomously reason, plan, and act within interactive software development workflows. However, bridging the gap between static text-based training and dynamic real-world agentic execution remains a core challenge. In this technical report, we present KAT-Coder, a large-scale agentic code model trained through a multi-stage curriculum encompassing Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning (RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances reasoning, planning, and reflection capabilities through a corpus of real software engineering data and synthetic agentic interactions. The SFT stage constructs a million-sample dataset balancing twenty programming languages, ten development contexts, and ten task archetypes. The RFT stage introduces a novel multi-ground-truth reward formulation for stable and sample-efficient policy optimization. Finally, the Reinforcement-to-Deployment phase adapts the model to production-grade IDE environments using Error-Masked SFT and Tree-Structured Trajectory Training. In summary, these stages enable KAT-Coder to achieve robust tool-use reliability, instruction alignment, and long-context reasoning, forming a deployable foundation for real-world intelligent coding agents. Our KAT series 32B model, KAT-Dev, has been open-sourced on https://huggingface.co/Kwaipilot/KAT-Dev.", "authors": ["Zizheng Zhan", "Ken Deng", "Xiaojiang Zhang", "Jinghui Wang", "Huaixi Tang", "Zhiyi Lai", "Haoyang Huang", "Wen Xiang", "Kun Wu", "Wenhao Zhuang", "Minglei Zhang", "Shaojie Wang", "Shangpeng Yan", "Kepeng Lei", "Zongxian Feng", "Huiming Wang", "Zheng Lin", "Mengtong Li", "Mengfei Xie", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Weihao Li", "Wenqiang Zhu", "Jiarong Zhang", "Jingxuan Xu", "Songwei Yu", "Yifan Yao", "Xinping Lei", "Han Li", "Junqi Xiong", "Zuchen Gao", "Dailin Li", "Haimo Li", "Jiaheng Liu", "Yuqun Zhang", "Junyi Peng", "Haotian Zhang", "Bin Chen"], "published": "2025-10-21T16:27:47Z", "updated": "2025-10-21T16:27:47Z", "link_pdf": "http://arxiv.org/pdf/2510.18779v1", "link_page": "http://arxiv.org/abs/2510.18779v1"}
{"id": "2510.18751v1", "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and   Segmentation", "summary": "Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.", "authors": ["Patterson Hsieh", "Jerry Yeh", "Mao-Chi He", "Wen-Han Hsieh", "Elvis Hsieh"], "published": "2025-10-21T15:59:00Z", "updated": "2025-10-21T15:59:00Z", "link_pdf": "http://arxiv.org/pdf/2510.18751v1", "link_page": "http://arxiv.org/abs/2510.18751v1"}
{"id": "2510.18731v1", "title": "Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate   Lost-in-Conversation", "summary": "Large Language Models demonstrate strong capabilities in single-turn instruction following but suffer from Lost-in-Conversation (LiC), a degradation in performance as information is revealed progressively in multi-turn settings. Motivated by the current progress on Reinforcement Learning with Verifiable Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not only to generate correct answers, but also to judge the solvability of questions in the multi-turn conversation setting. Our approach employs a competence-gated curriculum that incrementally increases dialogue difficulty (in terms of instruction shards), stabilizing training while promoting reliability. Using multi-turn, on-policy rollouts and a mixed-reward system, RLAAR teaches models to balance problem-solving with informed abstention, reducing premature answering behaviors that cause LiC. Evaluated on LiC benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to 75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together, these results provide a practical recipe for building multi-turn reliable and trustworthy LLMs.", "authors": ["Ming Li"], "published": "2025-10-21T15:32:26Z", "updated": "2025-10-21T15:32:26Z", "link_pdf": "http://arxiv.org/pdf/2510.18731v1", "link_page": "http://arxiv.org/abs/2510.18731v1"}
{"id": "2510.18728v1", "title": "HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large   Language Models", "summary": "Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a hierarchical semantic network; a feedback-driven Simulator for iterative query refinement; and a Network Traverser for real-time adaptive attack execution. HarmNet systematically explores and refines the adversarial space to uncover stealthy, high-success attack paths. Experiments across closed-source and open-source LLMs show that HarmNet outperforms state-of-the-art methods, achieving higher attack success rates. For example, on Mistral-7B, HarmNet achieves a 99.4% attack success rate, 13.9% higher than the best baseline. Index terms: jailbreak attacks; large language models; adversarial framework; query refinement.", "authors": ["Sidhant Narula", "Javad Rafiei Asl", "Mohammad Ghasemigol", "Eduardo Blanco", "Daniel Takabi"], "published": "2025-10-21T15:28:20Z", "updated": "2025-10-21T15:28:20Z", "link_pdf": "http://arxiv.org/pdf/2510.18728v1", "link_page": "http://arxiv.org/abs/2510.18728v1"}
{"id": "2510.18726v1", "title": "IF-VidCap: Can Video Caption Models Follow Instructions?", "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.", "authors": ["Shihao Li", "Yuanxing Zhang", "Jiangtao Wu", "Zhide Lei", "Yiwen He", "Runzhe Wen", "Chenxi Liao", "Chengkang Jiang", "An Ping", "Shuo Gao", "Suhan Wang", "Zhaozhou Bian", "Zijun Zhou", "Jingyi Xie", "Jiayi Zhou", "Jing Wang", "Yifan Yao", "Weihao Xie", "Yingshui Tan", "Yanghai Wang", "Qianqian Xie", "Zhaoxiang Zhang", "Jiaheng Liu"], "published": "2025-10-21T15:25:08Z", "updated": "2025-10-21T15:25:08Z", "link_pdf": "http://arxiv.org/pdf/2510.18726v1", "link_page": "http://arxiv.org/abs/2510.18726v1"}
{"id": "2510.18725v1", "title": "SemiAdapt and SemiLoRA: Efficient Domain Adaptation for   Transformer-based Low-Resource Language Translation with a Case Study on   Irish", "summary": "Fine-tuning is widely used to tailor large language models for specific tasks such as neural machine translation (NMT). However, leveraging transfer learning is computationally expensive when fine-tuning large multilingual models with billions of parameters, thus creating a barrier to entry for researchers working on low-resource domains such as Irish translation. Parameter-efficient fine-tuning (PEFT) bridges this gap by training on a fraction of the original model parameters, with the Low-Rank Adaptation (LoRA) approach introducing small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as semi-supervised inference-efficient approaches that strengthen domain adaptation and lead to improved overall performance in NMT. We demonstrate that SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA can propel PEFT methods to match or even outperform full-model fine-tuning. We further evaluate domain-by-dataset fine-tuning and demonstrate that our embedding-based inference methods perform especially well on larger and noisier corpora. All Irish translation models developed in this work are released as open resources. These methods aim to make high-quality domain adaptation and fine-tuning more accessible to researchers working with low-resource languages.", "authors": ["Josh McGiff", "Nikola S. Nikolov"], "published": "2025-10-21T15:24:15Z", "updated": "2025-10-21T15:24:15Z", "link_pdf": "http://arxiv.org/pdf/2510.18725v1", "link_page": "http://arxiv.org/abs/2510.18725v1"}
{"id": "2510.18713v1", "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons:   Benefits of Multiple Options", "summary": "We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\\tilde{\\mathcal{O}}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\\Omega \\left( \\frac{d}{K \\sqrt{T}} \\right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.", "authors": ["Joongkyu Lee", "Seouh-won Yi", "Min-hwan Oh"], "published": "2025-10-21T15:11:01Z", "updated": "2025-10-21T15:11:01Z", "link_pdf": "http://arxiv.org/pdf/2510.18713v1", "link_page": "http://arxiv.org/abs/2510.18713v1"}
{"id": "2510.18703v1", "title": "Exploring a Unified Vision-Centric Contrastive Alternatives on   Multi-Modal Web Documents", "summary": "Contrastive vision-language models such as CLIP have demonstrated strong performance across a wide range of multimodal tasks by learning from aligned image-text pairs. However, their ability to handle complex, real-world web documents remains limited, particularly in scenarios where text and images are interleaved, loosely aligned, or embedded in visual form. To address these challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified framework that models text, images, and their combinations using a single vision transformer. VC2L operates entirely in pixel space by rendering all inputs, whether textual, visual, or combined, as images, thus eliminating the need for OCR, text tokenization, or modality fusion strategy. To capture complex cross-modal relationships in multimodal web documents, VC2L employs a snippet-level contrastive learning objective that aligns consecutive multimodal segments, leveraging the inherent coherence of documents without requiring explicitly paired image-text data. To assess the effectiveness of this approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR, designed to evaluate cross-modal retrieval, fine-grained sequential understanding, and generalization to unseen data, respectively. Empirical results show that VC2L achieves competitive or superior performance compared to CLIP-style models on both the proposed benchmarks and established datasets such as M-BEIR and MTEB. These findings underscore the potential of multimodal web data as a valuable training resource for contrastive learning and illustrate the scalability of a unified, vision-centric approach for multimodal representation learning. Code and models are available at: https://github.com/showlab/VC2L.", "authors": ["Yiqi Lin", "Alex Jinpeng Wang", "Linjie Li", "Zhengyuan Yang", "Mike Zheng Shou"], "published": "2025-10-21T14:59:29Z", "updated": "2025-10-21T14:59:29Z", "link_pdf": "http://arxiv.org/pdf/2510.18703v1", "link_page": "http://arxiv.org/abs/2510.18703v1"}
{"id": "2510.18701v1", "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image   Generation", "summary": "Recent progress in text-to-image (T2I) generation underscores the importance of reliable benchmarks in evaluating how accurately generated images reflect the semantics of their textual prompt. However, (1) existing benchmarks lack the diversity of prompt scenarios and multilingual support, both essential for real-world applicability; (2) they offer only coarse evaluations across primary dimensions, covering a narrow range of sub-dimensions, and fall short in fine-grained sub-dimension assessment. To address these limitations, we introduce UniGenBench++, a unified semantic assessment benchmark for T2I generation. Specifically, it comprises 600 prompts organized hierarchically to ensure both coverage and efficiency: (1) spans across diverse real-world scenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively probes T2I models' semantic consistency over 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. To rigorously assess model robustness to variations in language and prompt length, we provide both English and Chinese versions of each prompt in short and long forms. Leveraging the general world knowledge and fine-grained image understanding capabilities of a closed-source Multi-modal Large Language Model (MLLM), i.e., Gemini-2.5-Pro, an effective pipeline is developed for reliable benchmark construction and streamlined model assessment. Moreover, to further facilitate community use, we train a robust evaluation model that enables offline assessment of T2I model outputs. Through comprehensive benchmarking of both open- and closed-sourced T2I models, we systematically reveal their strengths and weaknesses across various aspects.", "authors": ["Yibin Wang", "Zhimin Li", "Yuhang Zang", "Jiazi Bu", "Yujie Zhou", "Yi Xin", "Junjun He", "Chunyu Wang", "Qinglin Lu", "Cheng Jin", "Jiaqi Wang"], "published": "2025-10-21T14:56:46Z", "updated": "2025-10-21T14:56:46Z", "link_pdf": "http://arxiv.org/pdf/2510.18701v1", "link_page": "http://arxiv.org/abs/2510.18701v1"}
{"id": "2510.18674v1", "title": "Exploring Membership Inference Vulnerabilities in Clinical Large   Language Models", "summary": "As large language models (LLMs) become progressively more embedded in clinical decision-support, documentation, and patient-information systems, ensuring their privacy and trustworthiness has emerged as an imperative challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic health record (EHR) data improves domain alignment but also raises the risk of exposing patient information through model behaviors. In this work-in-progress, we present an exploratory empirical study on membership inference vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if specific patient records were used during model training. Using a state-of-the-art clinical question-answering model, Llemr, we evaluate both canonical loss-based attacks and a domain-motivated paraphrasing-based perturbation strategy that more realistically reflects clinical adversarial conditions. Our preliminary findings reveal limited but measurable membership leakage, suggesting that current clinical LLMs provide partial resistance yet remain susceptible to subtle privacy risks that could undermine trust in clinical AI adoption. These results motivate continued development of context-aware, domain-specific privacy evaluations and defenses such as differential privacy fine-tuning and paraphrase-aware training, to strengthen the security and trustworthiness of healthcare AI systems.", "authors": ["Alexander Nemecek", "Zebin Yun", "Zahra Rahmani", "Yaniv Harel", "Vipin Chaudhary", "Mahmood Sharif", "Erman Ayday"], "published": "2025-10-21T14:27:48Z", "updated": "2025-10-21T14:27:48Z", "link_pdf": "http://arxiv.org/pdf/2510.18674v1", "link_page": "http://arxiv.org/abs/2510.18674v1"}
{"id": "2510.18672v1", "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study", "summary": "The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving.", "authors": ["Qi Li", "Junpan Wu", "Xiang Liu", "Yuxin Wang", "Zeyu Li", "Zhenheng Tang", "Yuhan Chen", "Shaohuai Shi", "Xiaowen Chu"], "published": "2025-10-21T14:25:51Z", "updated": "2025-10-21T14:25:51Z", "link_pdf": "http://arxiv.org/pdf/2510.18672v1", "link_page": "http://arxiv.org/abs/2510.18672v1"}
{"id": "2510.18632v1", "title": "Think with 3D: Geometric Imagination Grounded Spatial Reasoning from   Limited Views", "summary": "Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at https://github.com/zhangquanchen/3DThinker.", "authors": ["Zhangquan Chen", "Manyuan Zhang", "Xinlei Yu", "Xufang Luo", "Mingze Sun", "Zihao Pan", "Yan Feng", "Peng Pei", "Xunliang Cai", "Ruqi Huang"], "published": "2025-10-21T13:36:58Z", "updated": "2025-10-21T13:36:58Z", "link_pdf": "http://arxiv.org/pdf/2510.18632v1", "link_page": "http://arxiv.org/abs/2510.18632v1"}
{"id": "2510.18619v1", "title": "VAR: Visual Attention Reasoning via Structured Search and Backtracking", "summary": "Multimodal Large Language Models (MLLMs), despite their advances, are hindered by their high hallucination tendency and heavy reliance on brittle, linear reasoning processes, leading to failures in complex tasks. To address these limitations, we introduce Visual Attention Reasoning (VAR), a novel framework that recasts grounded reasoning as a structured search over a reasoning trajectory space. VAR decomposes the reasoning process into two key stages: traceable evidence grounding and search-based chain-of-thought (CoT) generation, which incorporates a backtracking mechanism for self-correction. The search is guided by a multi-faceted reward function with semantic and geometric self-verification components, which penalize outputs that are not faithfully grounded in the visual input. We provide a theoretical analysis for our search strategy, validating its capability to find the correct solution with high probability. Experimental results show that our 7B model, VAR-7B, sets a new state-of-the-art on a comprehensive suite of hallucination and safety benchmarks, significantly outperforming existing open-source models and demonstrating competitive performance against leading proprietary systems.", "authors": ["Wei Cai", "Jian Zhao", "Yuchen Yuan", "Tianle Zhang", "Ming Zhu", "Haichuan Tang", "Chi Zhang", "Xuelong Li"], "published": "2025-10-21T13:18:44Z", "updated": "2025-10-21T13:18:44Z", "link_pdf": "http://arxiv.org/pdf/2510.18619v1", "link_page": "http://arxiv.org/abs/2510.18619v1"}
{"id": "2510.18596v1", "title": "CUARewardBench: A Benchmark for Evaluating Reward Models on   Computer-using Agent", "summary": "Computer-using agents (CUAs) enable task completion through natural interaction with operating systems and software interfaces. While script-based verifiers are widely adopted for evaluation, they suffer from limited scalability and inability to provide step-wise assessment. Reward models offer promising alternatives, but their effectiveness on CUA evaluation remains largely underexplored. To address this gap, we present CUARewardBench, comprising four key contributions: (1) First-ever Comprehensive CUA Reward Benchmark: We introduce the first benchmark for evaluating both outcome reward models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic assessment across trajectory-level and step-level evaluation. (2) Diverse, Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10 software categories and 7 agent architectures with varying performance levels (25.9%-50.8% success rates). All trajectories are expertly annotated through carefully designed protocols, with rigorous quality control to ensure reliability and practical applicability. (3) Comprehensive Analysis and Insights: Through extensive experiments across 7 vision-language models and 3 prompt templates, we reveal critical limitations of current CUA RMs, including insufficient visual reasoning capabilities, knowledge deficiencies, and the superiority of general VLMs over specialized CUA models for reward evaluation. (4) Unanimous Prompt Ensemble (UPE): Based on the insights from our comprehensive analysis, we propose UPE, a novel ensemble method that significantly enhances reward model reliability through strict unanimous voting and strategic prompt-template configurations. UPE achieves 89.8% precision and 93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially outperforming single VLMs and traditional ensemble approaches.", "authors": ["Haojia Lin", "Xiaoyu Tan", "Yulei Qin", "Zihan Xu", "Yuchen Shi", "Zongyi Li", "Gang Li", "Shaofei Cai", "Siqi Cai", "Chaoyou Fu", "Ke Li", "Xing Sun"], "published": "2025-10-21T12:53:40Z", "updated": "2025-10-21T12:53:40Z", "link_pdf": "http://arxiv.org/pdf/2510.18596v1", "link_page": "http://arxiv.org/abs/2510.18596v1"}
{"id": "2510.18583v1", "title": "CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with   Trainable Text Encoder", "summary": "Multimodal dataset distillation aims to synthesize a small set of image-text pairs that enables efficient training of large-scale vision-language models. While dataset distillation has shown promise in unimodal tasks, extending it to multimodal contrastive learning presents key challenges: learning cross-modal alignment and managing the high computational cost of large encoders. Prior approaches address scalability by freezing the text encoder and update only the image encoder and text projection layer. However, we find this severely limits semantic alignment and becomes a bottleneck for performance scaling. We propose CovMatch, a scalable dataset distillation framework that aligns the cross-covariance of real and synthetic features while regularizing feature distributions within each modality. Unlike prior approaches, CovMatch enables joint optimization of both encoders, leading to stronger cross-modal alignment and improved performance. Evaluated on Flickr30K and COCO, CovMatch outperforms state-of-the-art multimodal distillation methods and achieves up to 6.8% absolute gains in retrieval accuracy using only 500 synthetic pairs.", "authors": ["Yongmin Lee", "Hye Won Chung"], "published": "2025-10-21T12:36:25Z", "updated": "2025-10-21T12:36:25Z", "link_pdf": "http://arxiv.org/pdf/2510.18583v1", "link_page": "http://arxiv.org/abs/2510.18583v1"}
{"id": "2510.18561v1", "title": "Large language models for folktale type automation based on motifs:   Cinderella case study", "summary": "Artificial intelligence approaches are being adapted to many research areas, including digital humanities. We built a methodology for large-scale analyses in folkloristics. Using machine learning and natural language processing, we automatically detected motifs in a large collection of Cinderella variants and analysed their similarities and differences with clustering and dimensionality reduction. The results show that large language models detect complex interactions in tales, enabling computational analysis of extensive text collections and facilitating cross-lingual comparisons.", "authors": ["Tjaša Arčon", "Marko Robnik-Šikonja", "Polona Tratnik"], "published": "2025-10-21T12:18:20Z", "updated": "2025-10-21T12:18:20Z", "link_pdf": "http://arxiv.org/pdf/2510.18561v1", "link_page": "http://arxiv.org/abs/2510.18561v1"}
{"id": "2510.18556v1", "title": "Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency", "summary": "Large language models offer transformative potential for healthcare, yet their responsible and equitable development depends critically on a deeper understanding of how training data characteristics influence model behavior, including the potential for bias. Current practices in dataset curation and bias assessment often lack the necessary transparency, creating an urgent need for comprehensive evaluation frameworks to foster trust and guide improvements. In this study, we present an in-depth analysis of potential downstream biases in clinical language models, with a focus on differential opioid prescription tendencies across diverse demographic groups, such as ethnicity, gender, and age. As part of this investigation, we introduce HC4: Healthcare Comprehensive Commons Corpus, a novel and extensively curated pretraining dataset exceeding 89 billion tokens. Our evaluation leverages both established general benchmarks and a novel, healthcare-specific methodology, offering crucial insights to support fairness and safety in clinical AI applications.", "authors": ["Svetlana Maslenkova", "Clement Christophe", "Marco AF Pimentel", "Tathagata Raha", "Muhammad Umar Salman", "Ahmed Al Mahrooqi", "Avani Gupta", "Shadab Khan", "Ronnie Rajan", "Praveenkumar Kanithi"], "published": "2025-10-21T12:08:39Z", "updated": "2025-10-21T12:08:39Z", "link_pdf": "http://arxiv.org/pdf/2510.18556v1", "link_page": "http://arxiv.org/abs/2510.18556v1"}
{"id": "2510.18546v1", "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation   Map Caching and Retrieval", "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code will be released soon.", "authors": ["Zebin Yang", "Sunjian Zheng", "Tong Xie", "Tianshi Xu", "Bo Yu", "Fan Wang", "Jie Tang", "Shaoshan Liu", "Meng Li"], "published": "2025-10-21T11:52:44Z", "updated": "2025-10-21T11:52:44Z", "link_pdf": "http://arxiv.org/pdf/2510.18546v1", "link_page": "http://arxiv.org/abs/2510.18546v1"}
{"id": "2510.18526v1", "title": "Counterfactual Reasoning for Steerable Pluralistic Value Alignment of   Large Language Models", "summary": "As large language models (LLMs) become increasingly integrated into applications serving users across diverse cultures, communities and demographics, it is critical to align LLMs with pluralistic human values beyond average principles (e.g., HHH). In psychological and social value theories such as Schwartz's Value Theory, pluralistic values are represented by multiple value dimensions paired with various priorities. However, existing methods encounter two challenges when aligning with such fine-grained value objectives: 1) they often treat multiple values as independent and equally important, ignoring their interdependence and relative priorities (value complexity); 2) they struggle to precisely control nuanced value priorities, especially those underrepresented ones (value steerability). To handle these challenges, we propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE alignment. It introduces a structural causal model (SCM) to feature complex interdependency and prioritization among features, as well as the causal relationship between high-level value dimensions and behaviors. Moreover, it applies counterfactual reasoning to generate outputs aligned with any desired value objectives. Benefitting from explicit causal modeling, COUPLE also provides better interpretability. We evaluate COUPLE on two datasets with different value systems and demonstrate that COUPLE advances other baselines across diverse types of value objectives.", "authors": ["Hanze Guo", "Jing Yao", "Xiao Zhou", "Xiaoyuan Yi", "Xing Xie"], "published": "2025-10-21T11:12:45Z", "updated": "2025-10-21T11:12:45Z", "link_pdf": "http://arxiv.org/pdf/2510.18526v1", "link_page": "http://arxiv.org/abs/2510.18526v1"}
{"id": "2510.18510v1", "title": "Identity-Aware Large Language Models require Cultural Reasoning", "summary": "Large language models have become the latest trend in natural language processing, heavily featuring in the digital tools we use every day. However, their replies often reflect a narrow cultural viewpoint that overlooks the diversity of global users. This missing capability could be referred to as cultural reasoning, which we define here as the capacity of a model to recognise culture-specific knowledge values and social norms, and to adjust its output so that it aligns with the expectations of individual users. Because culture shapes interpretation, emotional resonance, and acceptable behaviour, cultural reasoning is essential for identity-aware AI. When this capacity is limited or absent, models can sustain stereotypes, ignore minority perspectives, erode trust, and perpetuate hate. Recent empirical studies strongly suggest that current models default to Western norms when judging moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning on survey data only partly reduces this tendency. The present evaluation methods mainly report static accuracy scores and thus fail to capture adaptive reasoning in context. Although broader datasets can help, they cannot alone ensure genuine cultural competence. Therefore, we argue that cultural reasoning must be treated as a foundational capability alongside factual accuracy and linguistic coherence. By clarifying the concept and outlining initial directions for its assessment, a foundation is laid for future systems to be able to respond with greater sensitivity to the complex fabric of human culture.", "authors": ["Alistair Plum", "Anne-Marie Lutgen", "Christoph Purschke", "Achim Rettinger"], "published": "2025-10-21T10:50:51Z", "updated": "2025-10-21T10:50:51Z", "link_pdf": "http://arxiv.org/pdf/2510.18510v1", "link_page": "http://arxiv.org/abs/2510.18510v1"}
{"id": "2510.18502v1", "title": "Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented   Generation", "summary": "Vehicle make and model recognition (VMMR) is an important task in intelligent transportation systems, but existing approaches struggle to adapt to newly released models. Contrastive Language-Image Pretraining (CLIP) provides strong visual-text alignment, yet its fixed pretrained weights limit performance without costly image-specific finetuning. We propose a pipeline that integrates vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to support zero-shot recognition through text-based reasoning. A VLM converts vehicle images into descriptive attributes, which are compared against a database of textual features. Relevant entries are retrieved and combined with the description to form a prompt, and a language model (LM) infers the make and model. This design avoids large-scale retraining and enables rapid updates by adding textual descriptions of new vehicles. Experiments show that the proposed method improves recognition by nearly 20% over the CLIP baseline, demonstrating the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city applications.", "authors": ["Wei-Chia Chang", "Yan-Ann Chen"], "published": "2025-10-21T10:39:39Z", "updated": "2025-10-21T10:39:39Z", "link_pdf": "http://arxiv.org/pdf/2510.18502v1", "link_page": "http://arxiv.org/abs/2510.18502v1"}
{"id": "2510.18493v1", "title": "One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for   Customizable Privacy-Preserving Phone Scam Detection", "summary": "Phone scams remain a pervasive threat to both personal safety and financial security worldwide. Recent advances in large language models (LLMs) have demonstrated strong potential in detecting fraudulent behavior by analyzing transcribed phone conversations. However, these capabilities introduce notable privacy risks, as such conversations frequently contain sensitive personal information that may be exposed to third-party service providers during processing. In this work, we explore how to harness LLMs for phone scam detection while preserving user privacy. We propose MASK (Modular Adaptive Sanitization Kit), a trainable and extensible framework that enables dynamic privacy adjustment based on individual preferences. MASK provides a pluggable architecture that accommodates diverse sanitization methods - from traditional keyword-based techniques for high-privacy users to sophisticated neural approaches for those prioritizing accuracy. We also discuss potential modeling approaches and loss function designs for future development, enabling the creation of truly personalized, privacy-aware LLM-based detection systems that balance user trust and detection effectiveness, even beyond phone scam context.", "authors": ["Kangzhong Wang", "Zitong Shen", "Youqian Zhang", "Michael MK Cheung", "Xiapu Luo", "Grace Ngai", "Eugene Yujun Fu"], "published": "2025-10-21T10:30:36Z", "updated": "2025-10-21T10:30:36Z", "link_pdf": "http://dx.doi.org/10.1145/3746027.3758164", "link_page": "http://arxiv.org/abs/2510.18493v1"}
{"id": "2510.18483v1", "title": "StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal   Decision-Making and Information Seeking", "summary": "Human players do more than press buttons: they ground what they see on screen into precise keyboard-mouse actions and, when stuck, they seek information before trying again. We ask whether current vision-language models (VLMs) can do the same. Despite encouraging results under simplified control or tool scaffolds, human-like play in a real client - mapping raw screenshots to temporally coherent low-level actions while deciding when to ask for guidance - remains an open challenge. We introduce StarBench, a turn-based RPG benchmark derived from Honkai: Star Rail that targets these two human-like competencies: multimodal decision-making from pixels to actions and agentic information seeking. StarBench standardizes evaluation across eight combat tasks and two regimes with shared tasks and metrics: (i) direct control, where agents receive only screenshots and must emit low-level primitives (click and keypress) with no semantic hints; and (ii) tool-assisted control, where higher-level intents can be mapped to primitives by detectors and OCR outputs provide optional textualized observations to ease UI grounding. To mirror human practice, StarBench also includes an ask-or-act diagnostic that measures whether and when agents choose to request brief guidance before proceeding, and how that choice affects subsequent performance. We report reference baselines for contemporary VLMs and a human reference. Results expose sizable gaps in perception-to-control fidelity in the direct regime, while showing that judicious information seeking correlates with improved success, establishing StarBench as a reproducible yardstick for agentic information seeking and multimodal decision-making in real-client play.", "authors": ["Haoran Zhang", "Chenhao Zhu", "Sicong Guo", "Hanzhe Guo", "Haiming Li", "Donglin Yu"], "published": "2025-10-21T10:02:59Z", "updated": "2025-10-21T10:02:59Z", "link_pdf": "http://arxiv.org/pdf/2510.18483v1", "link_page": "http://arxiv.org/abs/2510.18483v1"}
{"id": "2510.18477v1", "title": "LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data   Sources", "summary": "Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.", "authors": ["Haichao Ji", "Zibo Wang", "Yifei Zhu", "Meng han", "Dan Wang", "Zhu Han"], "published": "2025-10-21T09:56:25Z", "updated": "2025-10-21T09:56:25Z", "link_pdf": "http://arxiv.org/pdf/2510.18477v1", "link_page": "http://arxiv.org/abs/2510.18477v1"}
{"id": "2510.18476v1", "title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents", "summary": "We present a probabilistic intent modeling framework for large language model (LLM) agents in multi-turn social dialogue. The framework maintains a belief distribution over a partner's latent intentions, initialized from contextual priors and dynamically updated through likelihood estimation after each utterance. The evolving distribution provides additional contextual grounding for the policy, enabling adaptive dialogue strategies under uncertainty. Preliminary experiments in the SOTOPIA environment show consistent improvements: the proposed framework increases the Overall score by 9.0% on SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and slightly surpasses an oracle agent that directly observes partner intentions. These early results suggest that probabilistic intent modeling can contribute to the development of socially intelligent LLM agents.", "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "published": "2025-10-21T09:54:44Z", "updated": "2025-10-21T09:54:44Z", "link_pdf": "http://arxiv.org/pdf/2510.18476v1", "link_page": "http://arxiv.org/abs/2510.18476v1"}
{"id": "2510.18475v1", "title": "DART: A Structured Dataset of Regulatory Drug Documents in Italian for   Clinical NLP", "summary": "The extraction of pharmacological knowledge from regulatory documents has become a key focus in biomedical natural language processing, with applications ranging from adverse event monitoring to AI-assisted clinical decision support. However, research in this field has predominantly relied on English-language corpora such as DrugBank, leaving a significant gap in resources tailored to other healthcare systems. To address this limitation, we introduce DART (Drug Annotation from Regulatory Texts), the first structured corpus of Italian Summaries of Product Characteristics derived from the official repository of the Italian Medicines Agency (AIFA). The dataset was built through a reproducible pipeline encompassing web-scale document retrieval, semantic segmentation of regulatory sections, and clinical summarization using a few-shot-tuned large language model with low-temperature decoding. DART provides structured information on key pharmacological domains such as indications, adverse drug reactions, and drug-drug interactions. To validate its utility, we implemented an LLM-based drug interaction checker that leverages the dataset to infer clinically meaningful interactions. Experimental results show that instruction-tuned LLMs can accurately infer potential interactions and their clinical implications when grounded in the structured textual fields of DART. We publicly release our code on GitHub: https://github.com/PRAISELab-PicusLab/DART.", "authors": ["Mariano Barone", "Antonio Laudante", "Giuseppe Riccio", "Antonio Romano", "Marco Postiglione", "Vincenzo Moscato"], "published": "2025-10-21T09:53:17Z", "updated": "2025-10-21T09:53:17Z", "link_pdf": "http://arxiv.org/pdf/2510.18475v1", "link_page": "http://arxiv.org/abs/2510.18475v1"}
{"id": "2510.18471v1", "title": "CodeRL+: Improving Code Generation via Reinforcement with Execution   Semantics Alignment", "summary": "While Large Language Models (LLMs) excel at code generation by learning from vast code corpora, a fundamental semantic gap remains between their training on textual patterns and the goal of functional correctness, which is governed by formal execution semantics. Reinforcement Learning with Verifiable Rewards (RLVR) approaches attempt to bridge this gap using outcome rewards from executing test cases. However, solely relying on binary pass/fail signals is inefficient for establishing a well-aligned connection between the textual representation of code and its execution semantics, especially for subtle logical errors within the code. In this paper, we propose CodeRL+, a novel approach that integrates execution semantics alignment into the RLVR training pipeline for code generation. CodeRL+ enables the model to infer variable-level execution trajectory, providing a direct learning signal of execution semantics. CodeRL+ can construct execution semantics alignment directly using existing on-policy rollouts and integrates seamlessly with various RL algorithms. Extensive experiments demonstrate that CodeRL+ outperforms post-training baselines (including RLVR and Distillation), achieving a 4.6% average relative improvement in pass@1. CodeRL+ generalizes effectively to other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning and test-output-generation benchmarks, respectively. CodeRL+ shows strong applicability across diverse RL algorithms and LLMs. Furthermore, probe analyses provide compelling evidence that CodeRL+ strengthens the alignment between code's textual representations and its underlying execution semantics.", "authors": ["Xue Jiang", "Yihong Dong", "Mengyang Liu", "Hongyi Deng", "Tian Wang", "Yongding Tao", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Fei Huang", "Yongbin Li", "Ge Li"], "published": "2025-10-21T09:48:06Z", "updated": "2025-10-21T09:48:06Z", "link_pdf": "http://arxiv.org/pdf/2510.18471v1", "link_page": "http://arxiv.org/abs/2510.18471v1"}
{"id": "2510.18470v1", "title": "CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning   Circuits in LLMs", "summary": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, but scaling their performance often relies on massive reasoning datasets that are computationally expensive to train on. Existing data selection methods aim to curate smaller, high-quality subsets but often rely on costly external models or opaque heuristics. In this work, we shift the focus from external heuristics to the model's internal mechanisms. We find that complex reasoning tasks consistently activate a sparse, specialized subset of attention heads, forming core reasoning circuits. Building on this insight, we propose CircuitSeer, a novel data selection method that quantifies the reasoning complexity of data by measuring its influence on these crucial circuits. Extensive experiments on 4 models and 9 datasets demonstrate CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of data selected by our method achieves a 1.4-point gain in average Pass@1 over training on the full dataset, highlighting its efficiency and effectiveness.", "authors": ["Shaobo Wang", "Yongliang Miao", "Yuancheng Liu", "and Qianli Ma", "Ning Liao", "Linfeng Zhang"], "published": "2025-10-21T09:47:00Z", "updated": "2025-10-21T09:47:00Z", "link_pdf": "http://arxiv.org/pdf/2510.18470v1", "link_page": "http://arxiv.org/abs/2510.18470v1"}
{"id": "2510.18468v1", "title": "IMB: An Italian Medical Benchmark for Question Answering", "summary": "Online medical forums have long served as vital platforms where patients seek professional healthcare advice, generating vast amounts of valuable knowledge. However, the informal nature and linguistic complexity of forum interactions pose significant challenges for automated question answering systems, especially when dealing with non-English languages. We present two comprehensive Italian medical benchmarks: \\textbf{IMB-QA}, containing 782,644 patient-doctor conversations from 77 medical categories, and \\textbf{IMB-MCQA}, comprising 25,862 multiple-choice questions from medical specialty examinations. We demonstrate how Large Language Models (LLMs) can be leveraged to improve the clarity and consistency of medical forum data while retaining their original meaning and conversational style, and compare a variety of LLM architectures on both open and multiple-choice question answering tasks. Our experiments with Retrieval Augmented Generation (RAG) and domain-specific fine-tuning reveal that specialized adaptation strategies can outperform larger, general-purpose models in medical question answering tasks. These findings suggest that effective medical AI systems may benefit more from domain expertise and efficient information retrieval than from increased model scale. We release both datasets and evaluation frameworks in our GitHub repository to support further research on multilingual medical question answering: https://github.com/PRAISELab-PicusLab/IMB.", "authors": ["Antonio Romano", "Giuseppe Riccio", "Mariano Barone", "Marco Postiglione", "Vincenzo Moscato"], "published": "2025-10-21T09:45:59Z", "updated": "2025-10-21T09:45:59Z", "link_pdf": "http://arxiv.org/pdf/2510.18468v1", "link_page": "http://arxiv.org/abs/2510.18468v1"}
{"id": "2510.18467v1", "title": "Simple and Efficient Heterogeneous Temporal Graph Neural Network", "summary": "Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the real world. Recently, to enhance representation learning on HTGs, numerous attention-based neural networks have been proposed. Despite these successes, existing methods rely on a decoupled temporal and spatial learning paradigm, which weakens interactions of spatio-temporal information and leads to a high model complexity. To bridge this gap, we propose a novel learning paradigm for HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network (SE-HTGNN). Specifically, we innovatively integrate temporal modeling into spatial learning via a novel dynamic attention mechanism, which retains attention information from historical graph snapshots to guide subsequent attention computation, thereby improving the overall discriminative representations learning of HTGs. Additionally, to comprehensively and adaptively understand HTGs, we leverage large language models to prompt SE-HTGNN, enabling the model to capture the implicit properties of node types as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up to 10x speed-up over the state-of-the-art and latest baseline while maintaining the best forecasting accuracy.", "authors": ["Yili Wang", "Tairan Huang", "Changlong He", "Qiutong Li", "Jianliang Gao"], "published": "2025-10-21T09:43:08Z", "updated": "2025-10-21T09:43:08Z", "link_pdf": "http://arxiv.org/pdf/2510.18467v1", "link_page": "http://arxiv.org/abs/2510.18467v1"}
{"id": "2510.18466v1", "title": "CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database   for Language Learning", "summary": "Although WordNet is a valuable resource owing to its structured semantic networks and extensive vocabulary, its fine-grained sense distinctions can be challenging for second-language learners. To address this, we developed a WordNet annotated with the Common European Framework of Reference for Languages (CEFR), integrating its semantic networks with language-proficiency levels. We automated this process using a large language model to measure the semantic similarity between sense definitions in WordNet and entries in the English Vocabulary Profile Online. To validate our method, we constructed a large-scale corpus containing both sense and CEFR-level information from our annotated WordNet and used it to develop contextual lexical classifiers. Our experiments demonstrate that models fine-tuned on our corpus perform comparably to those trained on gold-standard annotations. Furthermore, by combining our corpus with the gold-standard data, we developed a practical classifier that achieves a Macro-F1 score of 0.81, indicating the high accuracy of our annotations. Our annotated WordNet, corpus, and classifiers are publicly available to help bridge the gap between natural language processing and language education, thereby facilitating more effective and efficient language learning.", "authors": ["Masato Kikuchi", "Masatsugu Ono", "Toshioki Soga", "Tetsu Tanabe", "Tadachika Ozono"], "published": "2025-10-21T09:42:48Z", "updated": "2025-10-21T09:42:48Z", "link_pdf": "http://arxiv.org/pdf/2510.18466v1", "link_page": "http://arxiv.org/abs/2510.18466v1"}
{"id": "2510.18454v1", "title": "Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor   in Language Models", "summary": "Large language models are increasingly used for creative writing and engagement content, raising safety concerns about the outputs. Therefore, casting humor generation as a testbed, this work evaluates how funniness optimization in modern LLM pipelines couples with harmful content by jointly measuring humor, stereotypicality, and toxicity. This is further supplemented by analyzing incongruity signals through information-theoretic metrics. Across six models, we observe that harmful outputs receive higher humor scores which further increase under role-based prompting, indicating a bias amplification loop between generators and evaluators. Information-theoretic analyses show harmful cues widen predictive uncertainty and surprisingly, can even make harmful punchlines more expected for some models, suggesting structural embedding in learned humor distributions. External validation on an additional satire-generation task with human perceived funniness judgments shows that LLM satire increases stereotypicality and typically toxicity, including for closed models. Quantitatively, stereotypical/toxic jokes gain $10-21\\%$ in mean humor score, stereotypical jokes appear $11\\%$ to $28\\%$ more often among the jokes marked funny by LLM-based metric and up to $10\\%$ more often in generations perceived as funny by humans.", "authors": ["Atharvan Dogra", "Soumya Suvra Ghosal", "Ameet Deshpande", "Ashwin Kalyan", "Dinesh Manocha"], "published": "2025-10-21T09:28:09Z", "updated": "2025-10-21T09:28:09Z", "link_pdf": "http://arxiv.org/pdf/2510.18454v1", "link_page": "http://arxiv.org/abs/2510.18454v1"}
{"id": "2510.18442v1", "title": "PlanU: Large Language Model Decision Making through Planning under   Uncertainty", "summary": "Large Language Models (LLMs) are increasingly being explored across a range of decision-making tasks. However, LLMs sometimes struggle with decision-making tasks under uncertainty that are relatively easy for humans, such as planning actions in stochastic environments. The adoption of LLMs for decision-making is impeded by uncertainty challenges, such as LLM uncertainty and environmental uncertainty. LLM uncertainty arises from the stochastic sampling process inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM uncertainty through multiple reasoning chains or search trees. However, these approaches overlook environmental uncertainty, which leads to poor performance in environments with stochastic state transitions. Some recent LDM approaches deal with uncertainty by forecasting the probability of unknown variables. However, they are not designed for multi-step decision-making tasks that require interaction with the environment. To address uncertainty in LLM decision-making, we introduce PlanU, an LLM-based planning method that captures uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of each node in the MCTS as a quantile distribution, which uses a set of quantiles to represent the return distribution. To balance exploration and exploitation during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity (UCC) score which estimates the uncertainty of MCTS nodes. Through extensive experiments, we demonstrate the effectiveness of PlanU in LLM-based decision-making tasks under uncertainty.", "authors": ["Ziwei Deng", "Mian Deng", "Chenjing Liang", "Zeming Gao", "Chennan Ma", "Chenxing Lin", "Haipeng Zhang", "Songzhu Mei", "Cheng Wang", "Siqi Shen"], "published": "2025-10-21T09:17:50Z", "updated": "2025-10-21T09:17:50Z", "link_pdf": "http://arxiv.org/pdf/2510.18442v1", "link_page": "http://arxiv.org/abs/2510.18442v1"}
{"id": "2510.18439v1", "title": "Grounding or Guessing? Visual Signals for Detecting Hallucinations in   Sign Language Translation", "summary": "Hallucination, where models generate fluent text unsupported by visual evidence, remains a major flaw in vision-language models and is particularly critical in sign language translation (SLT). In SLT, meaning depends on precise grounding in video, and gloss-free models are especially vulnerable because they map continuous signer movements directly into natural language without intermediate gloss supervision that serves as alignment. We argue that hallucinations arise when models rely on language priors rather than visual input. To capture this, we propose a token-level reliability measure that quantifies how much the decoder uses visual information. Our method combines feature-based sensitivity, which measures internal changes when video is masked, with counterfactual signals, which capture probability differences between clean and altered video inputs. These signals are aggregated into a sentence-level reliability score, providing a compact and interpretable measure of visual grounding. We evaluate the proposed measure on two SLT benchmarks (PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our results show that reliability predicts hallucination rates, generalizes across datasets and architectures, and decreases under visual degradations. Beyond these quantitative trends, we also find that reliability distinguishes grounded tokens from guessed ones, allowing risk estimation without references; when combined with text-based signals (confidence, perplexity, or entropy), it further improves hallucination risk estimation. Qualitative analysis highlights why gloss-free models are more susceptible to hallucinations. Taken together, our findings establish reliability as a practical and reusable tool for diagnosing hallucinations in SLT, and lay the groundwork for more robust hallucination detection in multimodal generation.", "authors": ["Yasser Hamidullah", "Koel Dutta Chowdury", "Yusser Al-Ghussin", "Shakib Yazdani", "Cennet Oguz", "Josef van Genabith", "Cristina España-Bonet"], "published": "2025-10-21T09:13:46Z", "updated": "2025-10-21T09:13:46Z", "link_pdf": "http://arxiv.org/pdf/2510.18439v1", "link_page": "http://arxiv.org/abs/2510.18439v1"}
{"id": "2510.18433v1", "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for   Generative Model Personalization", "summary": "We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.", "authors": ["Yuanhe Guo", "Linxi Xie", "Zhuoran Chen", "Kangrui Yu", "Ryan Po", "Guandao Yang", "Gordon Wetztein", "Hongyi Wen"], "published": "2025-10-21T09:08:01Z", "updated": "2025-10-21T09:08:01Z", "link_pdf": "http://arxiv.org/pdf/2510.18433v1", "link_page": "http://arxiv.org/abs/2510.18433v1"}
{"id": "2510.18424v1", "title": "Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents", "summary": "Visual Language Models (VLMs) achieve promising results in medical reasoning but struggle with hallucinations, vague descriptions, inconsistent logic and poor localization. To address this, we propose a agent framework named Medical Visual Reasoning Agent (\\textbf{Med-VRAgent}). The approach is based on Visual Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By combining the Visual Guidance with tree search, Med-VRAgent improves the medical visual reasoning capabilities of VLMs. We use the trajectories collected by Med-VRAgent as feedback to further improve the performance by fine-tuning the VLMs with the proximal policy optimization (PPO) objective. Experiments on multiple medical VQA benchmarks demonstrate that our method outperforms existing approaches.", "authors": ["Guangfu Guo", "Xiaoqian Lu", "Yue Feng"], "published": "2025-10-21T08:56:23Z", "updated": "2025-10-21T08:56:23Z", "link_pdf": "http://arxiv.org/pdf/2510.18424v1", "link_page": "http://arxiv.org/abs/2510.18424v1"}
{"id": "2510.18413v1", "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference", "summary": "Large language models (LLMs) now support context windows of hundreds of thousands to millions of tokens, enabling applications such as long-document summarization, large-scale code synthesis, multi-document question answering and persistent multi-turn dialogue. However, such extended contexts exacerbate the quadratic cost of self-attention, leading to severe latency in autoregressive decoding. Existing sparse attention methods alleviate these costs but rely on heuristic patterns that struggle to recall critical key-value (KV) pairs for each query, resulting in accuracy degradation. We introduce Adamas, a lightweight yet highly accurate sparse attention mechanism designed for long-context inference. Adamas applies the Hadamard transform, bucketization and 2-bit compression to produce compact representations, and leverages Manhattan-distance estimation for efficient top-k selections. Experiments show that Adamas matches the accuracy of full attention with only a 64-token budget, achieves near-lossless performance at 128, and supports up to 8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences. Remarkably, Adamas attains comparable or even lower perplexity than full attention, underscoring its effectiveness in maintaining accuracy under aggressive sparsity.", "authors": ["Siyuan Yan", "Guo-Qing Jiang", "Yuchen Zhang", "Xiaoxing Ma", "Ran Zhu", "Chun Cao", "Jingwei Xu"], "published": "2025-10-21T08:44:47Z", "updated": "2025-10-21T08:44:47Z", "link_pdf": "http://arxiv.org/pdf/2510.18413v1", "link_page": "http://arxiv.org/abs/2510.18413v1"}
{"id": "2510.18383v1", "title": "MENTOR: A Reinforcement Learning Framework for Model Enhancement via   Teacher-Optimized Rewards in Small Models", "summary": "Distilling the tool-using capabilities of large language models (LLMs) into smaller, more efficient small language models (SLMs) is a key challenge for their practical application. The predominant approach, supervised fine-tuning (SFT), suffers from poor generalization as it trains models to imitate a static set of teacher trajectories rather than learn a robust methodology. While reinforcement learning (RL) offers an alternative, the standard RL using sparse rewards fails to effectively guide SLMs, causing them to struggle with inefficient exploration and adopt suboptimal strategies. To address these distinct challenges, we propose MENTOR, a framework that synergistically combines RL with teacher-guided distillation. Instead of simple imitation, MENTOR employs an RL-based process to learn a more generalizable policy through exploration. In addition, to solve the problem of reward sparsity, it uses a teacher's reference trajectory to construct a dense, composite teacher-guided reward that provides fine-grained guidance. Extensive experiments demonstrate that MENTOR significantly improves the cross-domain generalization and strategic competence of SLMs compared to both SFT and standard sparse-reward RL baselines.", "authors": ["ChangSu Choi", "Hoyun Song", "Dongyeon Kim", "WooHyeon Jung", "Minkyung Cho", "Sunjin Park", "NohHyeob Bae", "Seona Yu", "KyungTae Lim"], "published": "2025-10-21T08:03:14Z", "updated": "2025-10-21T08:03:14Z", "link_pdf": "http://arxiv.org/pdf/2510.18383v1", "link_page": "http://arxiv.org/abs/2510.18383v1"}
{"id": "2510.18368v1", "title": "KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning   LLMs", "summary": "We present $\\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for evaluating factuality in large language models (LLMs) with a focus on Korean cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade, consisting of 1,000 short, fact-seeking questions with unambiguous answers. We conduct a comprehensive evaluation across a diverse set of open-source LLMs of varying sizes that support Korean, and find that even the strongest model generates correct answer only 33.7% of the time, underscoring the challenging nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ substantially from those on the English SimpleQA, highlighting the unique value of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging reasoning capabilities in the factual QA task can both help models better elicit their latent knowledge and improve their ability to abstain when uncertain. KoSimpleQA can be found at https://anonymous.4open.science/r/KoSimpleQA-62EB.", "authors": ["Donghyeon Ko", "Yeguk Jin", "Kyubyung Chae", "Byungwook Lee", "Chansong Jo", "Sookyo In", "Jaehong Lee", "Taesup Kim", "Donghyun Kwak"], "published": "2025-10-21T07:37:51Z", "updated": "2025-10-21T07:37:51Z", "link_pdf": "http://arxiv.org/pdf/2510.18368v1", "link_page": "http://arxiv.org/abs/2510.18368v1"}
{"id": "2510.18355v1", "title": "KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory   Call Center for Bengali Farmers", "summary": "In Bangladesh, many farmers continue to face challenges in accessing timely, expert-level agricultural guidance. This paper presents KrishokBondhu, a voice-enabled, call-centre-integrated advisory platform built on a Retrieval-Augmented Generation (RAG) framework, designed specifically for Bengali-speaking farmers. The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation. KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.", "authors": ["Mohd Ruhul Ameen", "Akif Islam", "Farjana Aktar", "M. Saifuzzaman Rafat"], "published": "2025-10-21T07:24:55Z", "updated": "2025-10-21T07:24:55Z", "link_pdf": "http://arxiv.org/pdf/2510.18355v1", "link_page": "http://arxiv.org/abs/2510.18355v1"}
{"id": "2510.18344v1", "title": "Combining Distantly Supervised Models with In Context Learning for   Monolingual and Cross-Lingual Relation Extraction", "summary": "Distantly Supervised Relation Extraction (DSRE) remains a long-standing challenge in NLP, where models must learn from noisy bag-level annotations while making sentence-level predictions. While existing state-of-the-art (SoTA) DSRE models rely on task-specific training, their integration with in-context learning (ICL) using large language models (LLMs) remains underexplored. A key challenge is that the LLM may not learn relation semantics correctly, due to noisy annotation.   In response, we propose HYDRE -- HYbrid Distantly Supervised Relation Extraction framework. It first uses a trained DSRE model to identify the top-k candidate relations for a given test sentence, then uses a novel dynamic exemplar retrieval strategy that extracts reliable, sentence-level exemplars from training data, which are then provided in LLM prompt for outputting the final relation(s).   We further extend HYDRE to cross-lingual settings for RE in low-resource languages. Using available English DSRE training data, we evaluate all methods on English as well as a newly curated benchmark covering four diverse low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE achieves up to 20 F1 point gains in English and, on average, 17 F1 points on Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's efficacy compared to other prompting strategies.", "authors": ["Vipul Rathore", "Malik Hammad Faisal", "Parag Singla", "Mausam"], "published": "2025-10-21T06:55:19Z", "updated": "2025-10-21T06:55:19Z", "link_pdf": "http://arxiv.org/pdf/2510.18344v1", "link_page": "http://arxiv.org/abs/2510.18344v1"}
{"id": "2510.18340v1", "title": "Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs", "summary": "The classical policy gradient method is the theoretical and conceptual foundation of modern policy-based reinforcement learning (RL) algorithms. Most rigorous analyses of such methods, particularly those establishing convergence guarantees, assume a discount factor $\\gamma < 1$. In contrast, however, a recent line of work on policy-based RL for large language models uses the undiscounted total-reward setting with $\\gamma = 1$, rendering much of the existing theory inapplicable. In this paper, we provide analyses of the policy gradient method for undiscounted expected total-reward infinite-horizon MDPs based on two key insights: (i) the classification of the MDP states into recurrent and transient states is invariant over the set of policies that assign strictly positive probability to every action (as is typical in deep RL models employing a softmax output layer) and (ii) the classical state visitation measure (which may be ill-defined when $\\gamma = 1$) can be replaced with a new object that we call the transient visitation measure.", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "published": "2025-10-21T06:46:21Z", "updated": "2025-10-21T06:46:21Z", "link_pdf": "http://arxiv.org/pdf/2510.18340v1", "link_page": "http://arxiv.org/abs/2510.18340v1"}
{"id": "2510.18339v1", "title": "ECG-LLM -- training and evaluation of domain-specific large language   models for electrocardiography", "summary": "Domain-adapted open-weight large language models (LLMs) offer promising healthcare applications, from queryable knowledge bases to multimodal assistants, with the crucial advantage of local deployment for privacy preservation. However, optimal adaptation strategies, evaluation methodologies, and performance relative to general-purpose LLMs remain poorly characterized. We investigated these questions in electrocardiography, an important area of cardiovascular medicine, by finetuning open-weight models on domain-specific literature and implementing a multi-layered evaluation framework comparing finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7 as a representative general-purpose model. Finetuned Llama 3.1 70B achieved superior performance on multiple-choice evaluations and automatic text metrics, ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned models significantly outperformed their base counterparts across nearly all evaluation modes. Our findings reveal substantial performance heterogeneity across evaluation methodologies, underscoring assessment complexity. Nevertheless, domain-specific adaptation through finetuning and RAG achieves competitive performance with proprietary models, supporting the viability of privacy-preserving, locally deployable clinical solutions.", "authors": ["Lara Ahrens", "Wilhelm Haverkamp", "Nils Strodthoff"], "published": "2025-10-21T06:45:38Z", "updated": "2025-10-21T06:45:38Z", "link_pdf": "http://arxiv.org/pdf/2510.18339v1", "link_page": "http://arxiv.org/abs/2510.18339v1"}
{"id": "2510.18333v1", "title": "Position: LLM Watermarking Should Align Stakeholders' Incentives for   Practical Adoption", "summary": "Despite progress in watermarking algorithms for large language models (LLMs), real-world deployment remains limited. We argue that this gap stems from misaligned incentives among LLM providers, platforms, and end users, which manifest as four key barriers: competitive risk, detection-tool governance, robustness concerns and attribution issues. We revisit three classes of watermarking through this lens. \\emph{Model watermarking} naturally aligns with LLM provider interests, yet faces new challenges in open-source ecosystems. \\emph{LLM text watermarking} offers modest provider benefit when framed solely as an anti-misuse tool, but can gain traction in narrowly scoped settings such as dataset de-contamination or user-controlled provenance. \\emph{In-context watermarking} (ICW) is tailored for trusted parties, such as conference organizers or educators, who embed hidden watermarking instructions into documents. If a dishonest reviewer or student submits this text to an LLM, the output carries a detectable watermark indicating misuse. This setup aligns incentives: users experience no quality loss, trusted parties gain a detection tool, and LLM providers remain neutral by simply following watermark instructions. We advocate for a broader exploration of incentive-aligned methods, with ICW as an example, in domains where trusted parties need reliable tools to detect misuse. More broadly, we distill design principles for incentive-aligned, domain-specific watermarking and outline future research directions. Our position is that the practical adoption of LLM watermarking requires aligning stakeholder incentives in targeted application domains and fostering active community engagement.", "authors": ["Yepeng Liu", "Xuandong Zhao", "Dawn Song", "Gregory W. Wornell", "Yuheng Bu"], "published": "2025-10-21T06:34:51Z", "updated": "2025-10-21T06:34:51Z", "link_pdf": "http://arxiv.org/pdf/2510.18333v1", "link_page": "http://arxiv.org/abs/2510.18333v1"}
{"id": "2510.18321v1", "title": "Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive   Token Ensemble Decoding", "summary": "Large Vision-Language Models (LVLMs) have recently achieved impressive results in multimodal tasks such as image captioning and visual question answering. However, they remain prone to object hallucination -- generating descriptions of nonexistent or misidentified objects. Prior work has partially mitigated this via auxiliary training objectives or external modules, but challenges remain in terms of scalability, adaptability, and model independence. To address these limitations, we propose Adaptive Token Ensemble Decoding (ATED), a training-free, token-level ensemble framework that mitigates hallucination by aggregating predictions from multiple LVLMs during inference. ATED dynamically computes uncertainty-based weights for each model, reflecting their reliability at each decoding step. It also integrates diverse decoding paths to improve contextual grounding and semantic consistency. Experiments on standard hallucination detection benchmarks demonstrate that ATED significantly outperforms state-of-the-art methods, reducing hallucination without compromising fluency or relevance. Our findings highlight the benefits of adaptive ensembling and point to a promising direction for improving LVLM robustness in high-stakes applications. The code is available at https://github.com/jinlin2021/ATED.", "authors": ["Jinlin Li", "Yuran Wang", "Yifei Yuan", "Xiao Zhou", "Yingying Zhang", "Xixian Yong", "Yefeng Zheng", "Xian Wu"], "published": "2025-10-21T06:11:24Z", "updated": "2025-10-21T06:11:24Z", "link_pdf": "http://arxiv.org/pdf/2510.18321v1", "link_page": "http://arxiv.org/abs/2510.18321v1"}
{"id": "2510.18314v1", "title": "Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming", "summary": "As large language model (LLM) agents increasingly automate complex web tasks, they boost productivity while simultaneously introducing new security risks. However, relevant studies on web agent attacks remain limited. Existing red-teaming approaches mainly rely on manually crafted attack strategies or static models trained offline. Such methods fail to capture the underlying behavioral patterns of web agents, making it difficult to generalize across diverse environments. In web agent attacks, success requires the continuous discovery and evolution of attack strategies. To this end, we propose Genesis, a novel agentic framework composed of three modules: Attacker, Scorer, and Strategist. The Attacker generates adversarial injections by integrating the genetic algorithm with a hybrid strategy representation. The Scorer evaluates the target web agent's responses to provide feedback. The Strategist dynamically uncovers effective strategies from interaction logs and compiles them into a continuously growing strategy library, which is then re-deployed to enhance the Attacker's effectiveness. Extensive experiments across various web tasks show that our framework discovers novel strategies and consistently outperforms existing attack baselines.", "authors": ["Zheng Zhang", "Jiarui He", "Yuchen Cai", "Deheng Ye", "Peilin Zhao", "Ruili Feng", "Hao Wang"], "published": "2025-10-21T05:49:37Z", "updated": "2025-10-21T05:49:37Z", "link_pdf": "http://arxiv.org/pdf/2510.18314v1", "link_page": "http://arxiv.org/abs/2510.18314v1"}
{"id": "2510.18304v1", "title": "The Impact of Image Resolution on Biomedical Multimodal Large Language   Models", "summary": "Imaging technologies are fundamental to biomedical research and modern medicine, requiring analysis of high-resolution images across various modalities. While multimodal large language models (MLLMs) show promise for biomedical image analysis, most are designed for low-resolution images from general-purpose datasets, risking critical information loss. We investigate how image resolution affects MLLM performance in biomedical applications and demonstrate that: (1) native-resolution training and inference significantly improve performance across multiple tasks, (2) misalignment between training and inference resolutions severely degrades performance, and (3) mixed-resolution training effectively mitigates misalignment and balances computational constraints with performance requirements. Based on these findings, we recommend prioritizing native-resolution inference and mixed-resolution datasets to optimize biomedical MLLMs for transformative impact in scientific research and clinical applications.", "authors": ["Liangyu Chen", "James Burgess", "Jeffrey J Nirschl", "Orr Zohar", "Serena Yeung-Levy"], "published": "2025-10-21T05:19:43Z", "updated": "2025-10-21T05:19:43Z", "link_pdf": "http://arxiv.org/pdf/2510.18304v1", "link_page": "http://arxiv.org/abs/2510.18304v1"}
{"id": "2510.18303v1", "title": "Proactive Reasoning-with-Retrieval Framework for Medical Multimodal   Large Language Models", "summary": "Incentivizing the reasoning ability of Multimodal Large Language Models (MLLMs) is essential for medical applications to transparently analyze medical scans and provide reliable diagnosis. However, existing medical MLLMs rely solely on internal knowledge during reasoning, leading to hallucinated reasoning and factual inaccuracies when encountering cases beyond their training scope. Although recent Agentic Retrieval-Augmented Generation (RAG) methods elicit the medical model's proactive retrieval ability during reasoning, they are confined to unimodal LLMs, neglecting the crucial visual information during reasoning and retrieval. Consequently, we propose the first Multimodal Medical Reasoning-with-Retrieval framework, Med-RwR, which actively retrieves external knowledge by querying observed symptoms or domain-specific medical concepts during reasoning. Specifically, we design a two-stage reinforcement learning strategy with tailored rewards that stimulate the model to leverage both visual diagnostic findings and textual clinical information for effective retrieval. Building on this foundation, we further propose a Confidence-Driven Image Re-retrieval (CDIR) method for test-time scaling when low prediction confidence is detected. Evaluation on various public medical benchmarks demonstrates Med-RwR's significant improvements over baseline models, proving the effectiveness of enhancing reasoning capabilities with external knowledge integration. Furthermore, Med-RwR demonstrates remarkable generalizability to unfamiliar domains, evidenced by 8.8% performance gain on our proposed EchoCardiography Benchmark (ECBench), despite the scarcity of echocardiography data in the training corpus. Our data, model, and codes will be made publicly available at https://github.com/xmed-lab/Med-RwR.", "authors": ["Lehan Wang", "Yi Qin", "Honglong Yang", "Xiaomeng Li"], "published": "2025-10-21T05:18:18Z", "updated": "2025-10-21T05:18:18Z", "link_pdf": "http://arxiv.org/pdf/2510.18303v1", "link_page": "http://arxiv.org/abs/2510.18303v1"}
{"id": "2510.18297v1", "title": "From Retrieval to Generation: Unifying External and Parametric Knowledge   for Medical Question Answering", "summary": "Medical question answering (QA) requires extensive access to domain-specific knowledge. A promising direction is to enhance large language models (LLMs) with external knowledge retrieved from medical corpora or parametric knowledge stored in model parameters. Existing approaches typically fall into two categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning on externally retrieved evidence, and Generation-Augmented Generation (GAG), which depends solely on the models internal knowledge to generate contextual documents. However, RAG often suffers from noisy or incomplete retrieval, while GAG is vulnerable to hallucinated or inaccurate information due to unconstrained generation. Both issues can mislead reasoning and undermine answer reliability. To address these challenges, we propose MedRGAG, a unified retrieval-generation augmented framework that seamlessly integrates external and parametric knowledge for medical QA. MedRGAG comprises two key modules: Knowledge-Guided Context Completion (KGCC), which directs the generator to produce background documents that complement the missing knowledge revealed by retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively selects an optimal combination of retrieved and generated documents to form concise yet comprehensive evidence for answer generation. Extensive experiments on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5% improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the effectiveness of unifying retrieval and generation for knowledge-intensive reasoning. Our code and data are publicly available at https://anonymous.4open.science/r/MedRGAG", "authors": ["Lei Li", "Xiao Zhou", "Yingying Zhang", "Xian Wu"], "published": "2025-10-21T04:58:29Z", "updated": "2025-10-21T04:58:29Z", "link_pdf": "http://arxiv.org/pdf/2510.18297v1", "link_page": "http://arxiv.org/abs/2510.18297v1"}
{"id": "2510.18288v1", "title": "BrailleLLM: Braille Instruction Tuning with Large Language Models for   Braille Domain Tasks", "summary": "Braille plays a vital role in education and information accessibility for visually impaired individuals. However, Braille information processing faces challenges such as data scarcity and ambiguities in mixed-text contexts. We construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with mathematical formulas to support diverse Braille domain research, and propose a syntax tree-based augmentation method tailored for Braille data. To address the underperformance of traditional fine-tuning methods in Braille-related tasks, we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the learning difficulty of Braille contextual features. BrailleLLM employs BKFT via instruction tuning to achieve unified Braille translation, formula-to-Braille conversion, and mixed-text translation. Experiments demonstrate that BKFT achieves significant performance improvements over conventional fine-tuning in Braille translation scenarios. Our open-sourced datasets and methodologies establish a foundation for low-resource multilingual Braille research.", "authors": ["Tianyuan Huang", "Zepeng Zhu", "Hangdi Xing", "Zirui Shao", "Zhi Yu", "Chaoxiong Yang", "Jiaxian He", "Xiaozhong Liu", "Jiajun Bu"], "published": "2025-10-21T04:33:05Z", "updated": "2025-10-21T04:33:05Z", "link_pdf": "http://arxiv.org/pdf/2510.18288v1", "link_page": "http://arxiv.org/abs/2510.18288v1"}
{"id": "2510.18279v1", "title": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text   Inputs in Multimodal LLMs", "summary": "Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.", "authors": ["Yanhong Li", "Zixuan Lan", "Jiawei Zhou"], "published": "2025-10-21T04:07:20Z", "updated": "2025-10-21T04:07:20Z", "link_pdf": "http://arxiv.org/pdf/2510.18279v1", "link_page": "http://arxiv.org/abs/2510.18279v1"}
{"id": "2510.18269v1", "title": "StreamingTOM: Streaming Token Compression for Efficient Video   Understanding", "summary": "Unlike offline processing, streaming video vision-language models face two fundamental constraints: causality and accumulation. Causality prevents access to future frames that offline methods exploit, while accumulation causes tokens to grow unbounded, creating efficiency bottlenecks. However, existing approaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill unchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage framework that addresses both pre-LLM and post-LLM bottlenecks with predictable latency. Causal Temporal Reduction imposes a fixed per-frame budget and selects tokens based on adjacent-frame changes and token saliency, drastically reducing per-frame prefill cost by processing only a compact subset of visual tokens per frame instead of all visual tokens. Online Quantized Memory stores tokens in 4-bit format, retrieves relevant groups on demand, and dequantizes them, keeping the active kv-cache bounded regardless of stream length. Experiments demonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$ lower peak memory and $2\\times$ faster TTFT compared to prior SOTA. StreamingTOM maintains state-of-the-art accuracy among training-free methods with an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS. These results highlight the practical benefits of our two-stage approach for efficient streaming video understanding with bounded growth.", "authors": ["Xueyi Chen", "Keda Tao", "Kele Shao", "Huan Wang"], "published": "2025-10-21T03:39:41Z", "updated": "2025-10-21T03:39:41Z", "link_pdf": "http://arxiv.org/pdf/2510.18269v1", "link_page": "http://arxiv.org/abs/2510.18269v1"}
{"id": "2510.18262v1", "title": "UWBench: A Comprehensive Vision-Language Benchmark for Underwater   Understanding", "summary": "Large vision-language models (VLMs) have achieved remarkable success in natural scene understanding, yet their application to underwater environments remains largely unexplored. Underwater imagery presents unique challenges including severe light attenuation, color distortion, and suspended particle scattering, while requiring specialized knowledge of marine ecosystems and organism taxonomy. To bridge this gap, we introduce UWBench, a comprehensive benchmark specifically designed for underwater vision-language understanding. UWBench comprises 15,003 high-resolution underwater images captured across diverse aquatic environments, encompassing oceans, coral reefs, and deep-sea habitats. Each image is enriched with human-verified annotations including 15,281 object referring expressions that precisely describe marine organisms and underwater structures, and 124,983 question-answer pairs covering diverse reasoning capabilities from object recognition to ecological relationship understanding. The dataset captures rich variations in visibility, lighting conditions, and water turbidity, providing a realistic testbed for model evaluation. Based on UWBench, we establish three comprehensive benchmarks: detailed image captioning for generating ecologically informed scene descriptions, visual grounding for precise localization of marine organisms, and visual question answering for multimodal reasoning about underwater environments. Extensive experiments on state-of-the-art VLMs demonstrate that underwater understanding remains challenging, with substantial room for improvement. Our benchmark provides essential resources for advancing vision-language research in underwater contexts and supporting applications in marine science, ecological monitoring, and autonomous underwater exploration. Our code and benchmark will be available.", "authors": ["Da Zhang", "Chenggang Rong", "Bingyu Li", "Feiyu Wang", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "published": "2025-10-21T03:32:15Z", "updated": "2025-10-21T03:32:15Z", "link_pdf": "http://arxiv.org/pdf/2510.18262v1", "link_page": "http://arxiv.org/abs/2510.18262v1"}
{"id": "2510.18257v1", "title": "DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt   Optimization", "summary": "Prompt Optimization has emerged as a crucial approach due to its capabilities in steering Large Language Models to solve various tasks. However, current works mainly rely on the random rewriting ability of LLMs, and the optimization process generally focus on specific influencing factors, which makes it easy to fall into local optimum. Besides, the performance of the optimized prompt is often unstable, which limits its transferability in different tasks. To address the above challenges, we propose $\\textbf{DelvePO}$ ($\\textbf{D}$irection-Guid$\\textbf{e}$d Se$\\textbf{l}$f-E$\\textbf{v}$olving Framework for Fl$\\textbf{e}$xible $\\textbf{P}$rompt $\\textbf{O}$ptimization), a task-agnostic framework to optimize prompts in self-evolve manner. In our framework, we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks. On this basis, we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts. Extensive experiments conducted on different tasks covering various domains for both open- and closed-source LLMs, including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini. Experimental results show that DelvePO consistently outperforms previous SOTA methods under identical experimental settings, demonstrating its effectiveness and transferability across different tasks.", "authors": ["Tao Tao", "Guanghui Zhu", "Lang Guo", "Hongyi Chen", "Chunfeng Yuan", "Yihua Huang"], "published": "2025-10-21T03:28:53Z", "updated": "2025-10-21T03:28:53Z", "link_pdf": "http://arxiv.org/pdf/2510.18257v1", "link_page": "http://arxiv.org/abs/2510.18257v1"}
{"id": "2510.18254v1", "title": "Illusions of reflection: open-ended task reveals systematic failures in   Large Language Models' reflective reasoning", "summary": "Humans do not just find mistakes after the fact -- we often catch them mid-stream because 'reflection' is tied to the goal and its constraints. Today's large language models produce reasoning tokens and 'reflective' text, but is it functionally equivalent with human reflective reasoning? Prior work on closed-ended tasks -- with clear, external 'correctness' signals -- can make 'reflection' look effective while masking limits in self-correction. We therefore test eight frontier models on a simple, real-world task that is open-ended yet rule-constrained, with auditable success criteria: to produce valid scientific test items, then revise after considering their own critique. First-pass performance is poor (often zero valid items out of 4 required; mean $\\approx$ 1), and reflection yields only modest gains (also $\\approx$ 1). Crucially, the second attempt frequently repeats the same violation of constraint, indicating 'corrective gains' arise largely from chance production of a valid item rather than error detection and principled, constraint-sensitive repair. Performance before and after reflection deteriorates as open-endedness increases, and models marketed for 'reasoning' show no advantage. Our results suggest that current LLM 'reflection' lacks functional evidence of the active, goal-driven monitoring that helps humans respect constraints even on a first pass. Until such mechanisms are instantiated in the model itself, reliable performance requires external structure that enforces constraints.", "authors": ["Sion Weatherhead", "Flora Salim", "Aaron Belbasis"], "published": "2025-10-21T03:24:21Z", "updated": "2025-10-21T03:24:21Z", "link_pdf": "http://arxiv.org/pdf/2510.18254v1", "link_page": "http://arxiv.org/abs/2510.18254v1"}
{"id": "2510.18250v1", "title": "ssToken: Self-modulated and Semantic-aware Token Selection for LLM   Fine-tuning", "summary": "Data quality plays a critical role in enhancing supervised fine-tuning (SFT) for large language models (LLMs), and token-level data selection has emerged as a promising direction for its fine-grained nature. Despite their strong empirical performance, existing token-level selection methods share two key limitations: (1) requiring training or accessing an additional reference model, and (2) relying solely on loss information for token selection, which cannot well preserve semantically important tokens that are not favored by loss-based metrics. To address these challenges, we propose ssToken, a Self-modulated and Semantic-aware Token Selection approach. ssToken leverages readily accessible history models to compute the per-token loss difference with the current model, which serves as a self-modulated signal that enables the model to adaptively select tokens along its optimization trajectory, rather than relying on excess loss from an offline-trained reference model as in prior works. We further introduce a semantic-aware, attention-based token importance estimation metric, orthogonal to loss-based selection and providing complementary semantic information for more effective filtering. Extensive experiments across different model families and scales demonstrate that both self-modulated selection and semantic-aware selection alone outperform full-data fine-tuning, while their integration--ssToken--achieves synergistic gains and further surpasses prior token-level selection methods, delivering performance improvements while maintaining training efficiency.", "authors": ["Xiaohan Qin", "Xiaoxing Wang", "Ning Liao", "Cancheng Zhang", "Xiangdong Zhang", "Mingquan Feng", "Jingzhi Wang", "Junchi Yan"], "published": "2025-10-21T03:21:04Z", "updated": "2025-10-21T03:21:04Z", "link_pdf": "http://arxiv.org/pdf/2510.18250v1", "link_page": "http://arxiv.org/abs/2510.18250v1"}
{"id": "2510.18245v1", "title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs", "summary": "Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2.", "authors": ["Song Bian", "Tao Yu", "Shivaram Venkataraman", "Youngsuk Park"], "published": "2025-10-21T03:08:48Z", "updated": "2025-10-21T03:08:48Z", "link_pdf": "http://arxiv.org/pdf/2510.18245v1", "link_page": "http://arxiv.org/abs/2510.18245v1"}
{"id": "2510.18228v1", "title": "Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with   Projected Gradient-Aligned Perturbations", "summary": "Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization has emerged as a promising alternative to traditional gradient-based methods due to its reduced memory footprint requirement. However, existing ZO methods suffer from high variance in gradient estimation, leading to slow convergence and suboptimal performance on large-scale models. In this work, we propose P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with Projected Gradient-Aligned Perturbations. Specifically, we first estimate a low-dimensional gradient space and then align perturbations in projected gradients' direction within the space. This approach enables reduced the number of perturbed parameters and decreased variance, therefore accelerated convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP consistently surpasses the baselines, achieving up to 6% increase in accuracy on classification tasks and up to 12% higher accuracy on generation tasks, with up to about 81% less training iterations and 70% less GPU hours. These results demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM fine-tuning.", "authors": ["Zhendong Mi", "Qitao Tan", "Grace Li Zhang", "Zhaozhuo Xu", "Geng Yuan", "Shaoyi Huang"], "published": "2025-10-21T02:19:11Z", "updated": "2025-10-21T02:19:11Z", "link_pdf": "http://arxiv.org/pdf/2510.18228v1", "link_page": "http://arxiv.org/abs/2510.18228v1"}
{"id": "2510.18204v1", "title": "RESCUE: Retrieval Augmented Secure Code Generation", "summary": "Despite recent advances, Large Language Models (LLMs) still generate vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to enhance LLMs for secure code generation by incorporating external security knowledge. However, the conventional RAG design struggles with the noise of raw security-related documents, and existing retrieval methods overlook the significant security semantics implicitly embedded in task descriptions. To address these issues, we propose RESCUE, a new RAG framework for secure code generation with two key innovations. First, we propose a hybrid knowledge base construction method that combines LLM-assisted cluster-then-summarize distillation with program slicing, producing both high-level security guidelines and concise, security-focused code examples. Second, we design a hierarchical multi-faceted retrieval to traverse the constructed knowledge base from top to bottom and integrates multiple security-critical facts at each hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated RESCUE on four benchmarks and compared it with five state-of-the-art secure code generation methods on six LLMs. The results demonstrate that RESCUE improves the SecurePass@1 metric by an average of 4.8 points, establishing a new state-of-the-art performance for security. Furthermore, we performed in-depth analysis and ablation studies to rigorously validate the effectiveness of individual components in RESCUE.", "authors": ["Jiahao Shi", "Tianyi Zhang"], "published": "2025-10-21T01:13:03Z", "updated": "2025-10-21T01:13:03Z", "link_pdf": "http://arxiv.org/pdf/2510.18204v1", "link_page": "http://arxiv.org/abs/2510.18204v1"}
{"id": "2510.18196v1", "title": "Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge", "summary": "Large Language Models (LLMs) are commonly used as evaluators in various applications, but the reliability of the outcomes remains a challenge. One such challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores from a specified range without any references. We first show that this challenge stems from LLM judge outputs being associated with score range bias, i.e., LLM judge outputs are highly sensitive to pre-defined score ranges, preventing the search for optimal score ranges. We also show that similar biases exist among models from the same family. We then mitigate this bias through contrastive decoding, achieving up to 11.3% relative improvement on average in Spearman correlation with human judgments across different score ranges.", "authors": ["Yoshinari Fujinuma"], "published": "2025-10-21T00:47:11Z", "updated": "2025-10-21T00:47:11Z", "link_pdf": "http://arxiv.org/pdf/2510.18196v1", "link_page": "http://arxiv.org/abs/2510.18196v1"}
{"id": "2510.18188v1", "title": "RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and   Multi-Target Segmentation in Radiology", "summary": "Most current medical vision language models struggle to jointly generate diagnostic text and pixel-level segmentation masks in response to complex visual questions. This represents a major limitation towards clinical application, as assistive systems that fail to provide both modalities simultaneously offer limited value to medical practitioners. To alleviate this limitation, we first introduce RadDiagSeg-D, a dataset combining abnormality detection, diagnosis, and multi-target segmentation into a unified and hierarchical task. RadDiagSeg-D covers multiple imaging modalities and is precisely designed to support the development of models that produce descriptive text and corresponding segmentation masks in tandem. Subsequently, we leverage the dataset to propose a novel vision-language model, RadDiagSeg-M, capable of joint abnormality detection, diagnosis, and flexible segmentation. RadDiagSeg-M provides highly informative and clinically useful outputs, effectively addressing the need to enrich contextual information for assistive diagnosis. Finally, we benchmark RadDiagSeg-M and showcase its strong performance across all components involved in the task of multi-target text-and-mask generation, establishing a robust and competitive baseline.", "authors": ["Chengrun Li", "Corentin Royer", "Haozhe Luo", "Bastian Wittmann", "Xia Li", "Ibrahim Hamamci", "Sezgin Er", "Anjany Sekuboyina", "Bjoern Menze"], "published": "2025-10-21T00:28:13Z", "updated": "2025-10-21T00:28:13Z", "link_pdf": "http://arxiv.org/pdf/2510.18188v1", "link_page": "http://arxiv.org/abs/2510.18188v1"}
{"id": "2510.18184v1", "title": "ActivationReasoning: Logical Reasoning in Latent Activation Spaces", "summary": "Large language models (LLMs) excel at generating fluent text, but their internal reasoning remains opaque and difficult to control. Sparse autoencoders (SAEs) make hidden activations more interpretable by exposing latent features that often align with human concepts. Yet, these features are fragile and passive, offering no mechanism for systematic reasoning or model control. To address this, we introduce ActivationReasoning (AR), a framework that embeds explicit logical reasoning into the latent space of LLMs. It proceeds in three stages: (1) Finding latent representations, first latent concept representations are identified (e.g., via SAEs) and organized into a dictionary; (2) Activating propositions, at inference time AR detects activating concepts and maps them to logical propositions; and (3)Logical reasoning, applying logical rules over these propositions to infer higher-order structures, compose new concepts, and steer model behavior. We evaluate AR on multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept cues (Rail2Country), reasoning over natural and diverse language (ProverQA), and context-sensitive safety (BeaverTails). Across all tasks, AR scales robustly with reasoning complexity, generalizes to abstract and context-sensitive tasks, and transfers across model backbones. These results demonstrate that grounding logical structure in latent activations not only improves transparency but also enables structured reasoning, reliable control, and alignment with desired behaviors, providing a path toward more reliable and auditable AI.", "authors": ["Lukas Helff", "Ruben Härle", "Wolfgang Stammer", "Felix Friedrich", "Manuel Brack", "Antonia Wüst", "Hikaru Shindo", "Patrick Schramowski", "Kristian Kersting"], "published": "2025-10-21T00:21:04Z", "updated": "2025-10-21T00:21:04Z", "link_pdf": "http://arxiv.org/pdf/2510.18184v1", "link_page": "http://arxiv.org/abs/2510.18184v1"}
{"id": "2510.18176v1", "title": "Local Coherence or Global Validity? Investigating RLVR Traces in Math   Domains", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of Large Language Models (LLMs) has been shown to improve accuracy on reasoning tasks and continues to attract significant attention. Existing RLVR methods, however, typically treat all tokens uniformly without accounting for token-level advantages. These methods primarily evaluate performance based on final answer correctness or Pass@K accuracy, and yet make claims about RL post-training leading to improved reasoning traces. This motivates our investigation into the effect of RL post-training on intermediate tokens which are not directly incentivized. To study this, we design an experimental setup using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We introduce trace coherence, a First-Order Logic (FOL)-based measure to capture the consistency of reasoning steps by identifying errors in the traces. We distinguish between trace validity and trace coherence, noting that the former implies logical soundness while the latter measures local coherence via lack of errors. Our results show that RL post-training overall improves trace coherence with the most significant gains on problems where the base model fails but the RL model succeeds. Surprisingly, RL enhances local coherence without necessarily producing valid or correct solutions. This highlights a crucial distinction: improved local coherence in reasoning steps does not guarantee final answer correctness. We argue that claims of improved reasoning via RL must be examined with care, as these may be based on improved trace coherence, which may not translate into fully valid mathematical proofs.", "authors": ["Soumya Rani Samineni", "Durgesh Kalwar", "Vardaan Gangal", "Siddhant Bhambri", "Subbarao Kambhampati"], "published": "2025-10-20T23:58:31Z", "updated": "2025-10-20T23:58:31Z", "link_pdf": "http://arxiv.org/pdf/2510.18176v1", "link_page": "http://arxiv.org/abs/2510.18176v1"}
{"id": "2510.18173v1", "title": "CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing   Robustness in Large Language Models", "summary": "LLM Driven text-to-table (T2T) systems often rely on extensive prompt-engineering or iterative event extraction in code-parsable formats, which boosts scores but are computationally expensive and obscure how models actually reason over temporal evolving narratives to summarise key information. We present CMT-Bench, a diagnostic benchmark built from live cricket commentary that requires dynamic table generation across two evolving schemas under a dense, rule-governed policy. CMT-Bench is designed to probe robustness via three semantics-preserving dimensions: (i) extractive-cue ablation to separate extractive shortcuts from state tracking, (ii) temporal prefixing to test long-context stability, and (iii) entity-form perturbations (anonymization, outof-distribution substitutions, role-entangling paraphrases) to assess sensitivity to surface variation. Across diverse long-context stateof-the-art LLMs, we find large drops without extractive summaries, monotonic degradation with input length, and consistent accuracy drop under entity-form changes. Complementary distributional tests confirm significant shifts in numeric error patterns, indicating drift in reasoning rather than mere noise. Our results show that current LLMs are brittle in dynamic Textto-table generation, motivating robustness-first evaluation as a prerequisite for developing efficient and scalable approaches for this task.", "authors": ["Ritam Upadhyay", "Naman Ahuja", "Rishabh Baral", "Aparna Garimella", "Vivek Gupta"], "published": "2025-10-20T23:51:28Z", "updated": "2025-10-20T23:51:28Z", "link_pdf": "http://arxiv.org/pdf/2510.18173v1", "link_page": "http://arxiv.org/abs/2510.18173v1"}
{"id": "2510.18162v1", "title": "Automatic Prompt Generation via Adaptive Selection of Prompting   Techniques", "summary": "Prompt engineering is crucial for achieving reliable and effective outputs from large language models (LLMs), but its design requires specialized knowledge of prompting techniques and a deep understanding of target tasks. To address this challenge, we propose a novel method that adaptively selects task-appropriate prompting techniques based on users' abstract task descriptions and automatically generates high-quality prompts without relying on pre-existing templates or frameworks. The proposed method constructs a knowledge base that associates task clusters, characterized by semantic similarity across diverse tasks, with their corresponding prompting techniques. When users input task descriptions, the system assigns them to the most relevant task cluster and dynamically generates prompts by integrating techniques drawn from the knowledge base. An experimental evaluation of the proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates superior performance compared with standard prompts and existing automatic prompt-generation tools, as measured by both arithmetic and harmonic mean scores. This research establishes a foundation for streamlining and standardizing prompt creation, enabling non-experts to effectively leverage LLMs.", "authors": ["Yohei Ikenoue", "Hitomi Tashiro", "Shigeru Kuroyanagi"], "published": "2025-10-20T23:28:23Z", "updated": "2025-10-20T23:28:23Z", "link_pdf": "http://arxiv.org/pdf/2510.18162v1", "link_page": "http://arxiv.org/abs/2510.18162v1"}
{"id": "2510.18155v1", "title": "LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and   Consumer Behavior", "summary": "Simulating consumer decision-making is vital for designing and evaluating marketing strategies before costly real-world deployment. However, post-event analyses and rule-based agent-based models (ABMs) struggle to capture the complexity of human behavior and social interaction. We introduce an LLM-powered multi-agent simulation framework that models consumer decisions and social dynamics. Building on recent advances in large language model simulation in a sandbox environment, our framework enables generative agents to interact, express internal reasoning, form habits, and make purchasing decisions without predefined rules. In a price-discount marketing scenario, the system delivers actionable strategy-testing outcomes and reveals emergent social patterns beyond the reach of conventional methods. This approach offers marketers a scalable, low-risk tool for pre-implementation testing, reducing reliance on time-intensive post-event evaluations and lowering the risk of underperforming campaigns.", "authors": ["Man-Lin Chu", "Lucian Terhorst", "Kadin Reed", "Tom Ni", "Weiwei Chen", "Rongyu Lin"], "published": "2025-10-20T23:15:44Z", "updated": "2025-10-20T23:15:44Z", "link_pdf": "http://arxiv.org/pdf/2510.18155v1", "link_page": "http://arxiv.org/abs/2510.18155v1"}
{"id": "2510.18147v1", "title": "LLMs Encode How Difficult Problems Are", "summary": "Large language models exhibit a puzzling inconsistency: they solve complex problems yet frequently fail on seemingly simpler ones. We investigate whether LLMs internally encode problem difficulty in a way that aligns with human judgment, and whether this representation tracks generalization during reinforcement learning post-training. We train linear probes across layers and token positions on 60 models, evaluating on mathematical and coding subsets of Easy2HardBench. We find that human-labeled difficulty is strongly linearly decodable (AMC: $\\rho \\approx 0.88$) and exhibits clear model-size scaling, whereas LLM-derived difficulty is substantially weaker and scales poorly. Steering along the difficulty direction reveals that pushing models toward \"easier\" representations reduces hallucination and improves accuracy. During GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and positively correlates with test accuracy across training steps, while the LLM-difficulty probe degrades and negatively correlates with performance. These results suggest that human annotations provide a stable difficulty signal that RL amplifies, while automated difficulty estimates derived from model performance become misaligned precisely as models improve. We release probe code and evaluation scripts to facilitate replication.", "authors": ["William Lugoloobi", "Chris Russell"], "published": "2025-10-20T22:48:23Z", "updated": "2025-10-20T22:48:23Z", "link_pdf": "http://arxiv.org/pdf/2510.18147v1", "link_page": "http://arxiv.org/abs/2510.18147v1"}
{"id": "2510.18121v1", "title": "Efficient Long-context Language Model Training by Core Attention   Disaggregation", "summary": "We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocated with other layers; at long context lengths, its quadratic compute growth compared to the near-linear growth of other components causes load imbalance and stragglers across data and pipeline parallel groups. CAD is enabled by two observations. First, core attention is stateless: it has no trainable parameters and only minimal transient data, so balancing reduces to scheduling compute-bound tasks. Second, it is composable: modern attention kernels retain high efficiency when processing fused batches of token-level shards with arbitrary lengths. CAD partitions core attention into token-level tasks and dispatches them to dedicated attention servers, which dynamically rebatch tasks to equalize compute without sacrificing kernel efficiency. We implement CAD in a system called DistCA, which uses a ping-pong execution scheme to fully overlap communication with computation and in-place execution on attention servers to reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens, DistCA improves end-to-end training throughput by up to 1.35x, eliminates data and pipeline parallel stragglers, and achieves near-perfect compute and memory balance.", "authors": ["Yonghao Zhuang", "Junda Chen", "Bo Pang", "Yi Gu", "Yibo Zhu", "Yimin Jiang", "Ion Stoica", "Eric Xing", "Hao Zhang"], "published": "2025-10-20T21:40:51Z", "updated": "2025-10-20T21:40:51Z", "link_pdf": "http://arxiv.org/pdf/2510.18121v1", "link_page": "http://arxiv.org/abs/2510.18121v1"}
{"id": "2510.18117v1", "title": "Online In-Context Distillation for Low-Resource Vision Language Models", "summary": "As the field continues its push for ever more resources, this work turns the spotlight on a critical question: how can vision-language models (VLMs) be adapted to thrive in low-resource, budget-constrained settings? While large VLMs offer strong performance, they are impractical to deploy in such settings. Small VLMs, on the other hand, are efficient but typically require costly fine-tuning to close the performance gap with larger models in the deployment domain. Inspired by the in-context learning framework, we propose an online In-Context Distillation (ICD) method, in which a small VLM collaborates with a stronger teacher model at inference time, distilling its knowledge via sparse demonstrations to efficiently bridge the gap between them. Our method is built on an in-depth analysis that identifies the scale and the choice of models for which vision-language ICL is currently feasible, and demonstrates the advantage of ICL over fine-tuning under constrained compute budgets. We enhance our method with a novel cross-modal demonstration selection strategy, teacher test-time scaling to reduce noise, and student uncertainty conditioning to dynamically populate a demonstration pool and minimize teacher queries. Our ICD method significantly boosts the performance of small models (up to 33%) using scarce teacher annotations (as low as 4%), and competes with the teacher's zero-shot performance.", "authors": ["Zhiqi Kang", "Rahaf Aljundi", "Vaggelis Dorovatas", "Karteek Alahari"], "published": "2025-10-20T21:35:17Z", "updated": "2025-10-20T21:35:17Z", "link_pdf": "http://arxiv.org/pdf/2510.18117v1", "link_page": "http://arxiv.org/abs/2510.18117v1"}
{"id": "2510.18112v1", "title": "Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt   Engineering Experiment", "summary": "This paper explores the application of Large Language Models (LLMs) and reasoning to predict Dungeons & Dragons (DnD) player actions and format them as Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model, LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the importance of providing specific instructions to models, that even single sentence changes in prompts can greatly affect the output of models, and that instruct models are sufficient for this task compared to reasoning models.", "authors": ["Patricia Delafuente", "Arya Honraopatil", "Lara J. Martin"], "published": "2025-10-20T21:23:23Z", "updated": "2025-10-20T21:23:23Z", "link_pdf": "http://arxiv.org/pdf/2510.18112v1", "link_page": "http://arxiv.org/abs/2510.18112v1"}
{"id": "2510.18095v1", "title": "SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for   LLM-Driven Reasoning and Planning", "summary": "Large Language Models (LLMs) have redefined complex task automation with exceptional generalization capabilities. Despite these advancements, state-of-the-art methods rely on single-strategy prompting, missing the synergy of diverse reasoning approaches. No single strategy excels universally, highlighting the need for frameworks that fuse strategies to maximize performance and ensure robustness. We introduce the Select, Mix, and ReinvenT (SMaRT) framework, an innovative strategy fusion approach designed to overcome this constraint by creating balanced and efficient solutions through the seamless integration of diverse reasoning strategies. Unlike existing methods, which employ LLMs merely as evaluators, SMaRT uses them as intelligent integrators, unlocking the \"best of all worlds\" across tasks. Extensive empirical evaluations across benchmarks in reasoning, planning, and sequential decision-making highlight the robustness and adaptability of SMaRT. The framework consistently outperforms state-of-the-art baselines in solution quality, constraint adherence, and performance metrics. This work redefines LLM-driven decision-making by pioneering a new paradigm in cross-strategy calibration, unlocking superior outcomes for reasoning systems and advancing the boundaries of self-refining methodologies.", "authors": ["Nikhil Verma", "Manasa Bharadwaj", "Wonjun Jang", "Harmanpreet Singh", "Yixiao Wang", "Homa Fashandi", "Chul Lee"], "published": "2025-10-20T20:42:24Z", "updated": "2025-10-20T20:42:24Z", "link_pdf": "http://arxiv.org/pdf/2510.18095v1", "link_page": "http://arxiv.org/abs/2510.18095v1"}
{"id": "2510.18087v1", "title": "Planned Diffusion", "summary": "A central challenge in large language model inference is the trade-off between generation speed and output quality. Autoregressive models produce high-quality text but generate tokens sequentially. Diffusion models can generate tokens in parallel but often need many iterations to match the same quality. We propose planned diffusion, a hybrid method that combines the strengths of both paradigms. Planned diffusion works in two stages: first, the model creates a short autoregressive plan that breaks the output into smaller, independent spans. Second, the model generates these spans simultaneously using diffusion. This approach expands the speed-quality Pareto frontier and provides a practical path to faster, high-quality text generation. On AlpacaEval, a suite of 805 instruction-following prompts, planned diffusion achieves Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x speedup over autoregressive generation with only 0.87\\% to 5.4\\% drop in win rate, respectively. Our sensitivity analysis shows that the planning mechanism of planned diffusion is minimal and reliable, and simple runtime knobs exist to provide flexible control of the quality-latency trade-off.", "authors": ["Daniel Israel", "Tian Jin", "Ellie Cheng", "Guy Van den Broeck", "Aditya Grover", "Suvinay Subramanian", "Michael Carbin"], "published": "2025-10-20T20:27:48Z", "updated": "2025-10-20T20:27:48Z", "link_pdf": "http://arxiv.org/pdf/2510.18087v1", "link_page": "http://arxiv.org/abs/2510.18087v1"}
{"id": "2510.18081v1", "title": "Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to   Any-Depth", "summary": "Large Language Models (LLMs) exhibit strong but shallow alignment: they directly refuse harmful queries when a refusal is expected at the very start of an assistant turn, yet this protection collapses once a harmful continuation is underway (either through the adversarial attacks or via harmful assistant-prefill attacks). This raises a fundamental question: Can the innate shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an effective inference-time defense with negligible overhead. ADA is built based on our observation that alignment is concentrated in the assistant header tokens through repeated use in shallow-refusal training, and these tokens possess the model's strong alignment priors. By reintroducing these tokens mid-stream, ADA induces the model to reassess harmfulness and recover refusals at any point in generation. Across diverse open-source model families (Llama, Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety performance without requiring any changes to the base model's parameters. It secures a near-100% refusal rate against challenging adversarial prefill attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces the average success rate of prominent adversarial prompt attacks (such as GCG, AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving utility on benign tasks with minimal over-refusal. ADA maintains this resilience even after the base model undergoes subsequent instruction tuning (benign or adversarial).", "authors": ["Jiawei Zhang", "Andrew Estornell", "David D. Baek", "Bo Li", "Xiaojun Xu"], "published": "2025-10-20T20:18:59Z", "updated": "2025-10-20T20:18:59Z", "link_pdf": "http://arxiv.org/pdf/2510.18081v1", "link_page": "http://arxiv.org/abs/2510.18081v1"}
{"id": "2510.18077v1", "title": "Chain-of-Thought Reasoning Improves Context-Aware Translation with Large   Language Models", "summary": "This paper assesses the capacity of large language models (LLMs) to translate texts that include inter-sentential dependencies. We use the English-French DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing translation challenges either for pronominal anaphora or for lexical cohesion. We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families on two tasks: (1) distinguishing a correct translation from a wrong but plausible one; (2) generating a correct translation. We compare prompts that encourage chain-of-thought reasoning with those that do not. The best models take advantage of reasoning and reach about 90% accuracy on the first task, and COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi standing out. Moreover, we observe a \"wise get wiser\" effect: the improvements through reasoning are positively correlated with the scores of the models without reasoning.", "authors": ["Shabnam Ataee", "Andrei Popescu-Belis"], "published": "2025-10-20T20:14:46Z", "updated": "2025-10-20T20:14:46Z", "link_pdf": "http://arxiv.org/pdf/2510.18077v1", "link_page": "http://arxiv.org/abs/2510.18077v1"}
{"id": "2510.18054v1", "title": "HouseTour: A Virtual Real Estate A(I)gent", "summary": "We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.", "authors": ["Ata Çelen", "Marc Pollefeys", "Daniel Barath", "Iro Armeni"], "published": "2025-10-20T19:47:35Z", "updated": "2025-10-20T19:47:35Z", "link_pdf": "http://arxiv.org/pdf/2510.18054v1", "link_page": "http://arxiv.org/abs/2510.18054v1"}
{"id": "2510.18046v1", "title": "Language Models as Semantic Augmenters for Sequential Recommenders", "summary": "Large Language Models (LLMs) excel at capturing latent semantics and contextual relationships across diverse modalities. However, in modeling user behavior from sequential interaction data, performance often suffers when such semantic context is limited or absent. We introduce LaMAR, a LLM-driven semantic enrichment framework designed to enrich such sequences automatically. LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual signals by inferring latent semantic aspects of a user's intent and item relationships from existing metadata. These generated signals, such as inferred usage scenarios, item intents, or thematic summaries, augment the original sequences with greater contextual depth. We demonstrate the utility of this generated resource by integrating it into benchmark sequential modeling tasks, where it consistently improves performance. Further analysis shows that LLM-generated signals exhibit high semantic novelty and diversity, enhancing the representational capacity of the downstream models. This work represents a new data-centric paradigm where LLMs serve as intelligent context generators, contributing a new method for the semi-automatic creation of training data and language resources.", "authors": ["Mahsa Valizadeh", "Xiangjue Dong", "Rui Tuo", "James Caverlee"], "published": "2025-10-20T19:36:38Z", "updated": "2025-10-20T19:36:38Z", "link_pdf": "http://arxiv.org/pdf/2510.18046v1", "link_page": "http://arxiv.org/abs/2510.18046v1"}
{"id": "2510.18043v1", "title": "CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM   Workflows", "summary": "Large Language Models (LLMs) deliver powerful reasoning and generation capabilities but incur substantial run-time costs when operating in agentic workflows that chain together lengthy prompts and process rich data streams. We introduce CompactPrompt, an end-to-end pipeline that merges hard prompt compression with lightweight file-level data compression. CompactPrompt first prunes low-information tokens from prompts using self-information scoring and dependency-based phrase grouping. In parallel, it applies n-gram abbreviation to recurrent textual patterns in attached documents and uniform quantization to numerical columns, yielding compact yet semantically faithful representations. Integrated into standard LLM agents, CompactPrompt reduces total token usage and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA, while preserving output quality (Results in less than 5% accuracy drop for Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time compression decisions and quantify cost-performance trade-offs, laying the groundwork for leaner generative AI pipelines.", "authors": ["Joong Ho Choi", "Jiayang Zhao", "Jeel Shah", "Ritvika Sonawane", "Vedant Singh", "Avani Appalla", "Will Flanagan", "Filipe Condessa"], "published": "2025-10-20T19:31:11Z", "updated": "2025-10-20T19:31:11Z", "link_pdf": "http://arxiv.org/pdf/2510.18043v1", "link_page": "http://arxiv.org/abs/2510.18043v1"}
{"id": "2510.18034v1", "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection", "summary": "Autonomous driving systems remain critically vulnerable to the long-tail of rare, out-of-distribution scenarios with semantic anomalies. While Vision Language Models (VLMs) offer promising reasoning capabilities, naive prompting approaches yield unreliable performance and depend on expensive proprietary models, limiting practical deployment. We introduce SAVANT (Semantic Analysis with Vision-Augmented Anomaly deTection), a structured reasoning framework that achieves high accuracy and recall in detecting anomalous driving scenarios from input images through layered scene analysis and a two-phase pipeline: structured scene description extraction followed by multi-modal evaluation. Our approach transforms VLM reasoning from ad-hoc prompting to systematic analysis across four semantic layers: Street, Infrastructure, Movable Objects, and Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world driving scenarios, significantly outperforming unstructured baselines. More importantly, we demonstrate that our structured framework enables a fine-tuned 7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8% accuracy - surpassing all models evaluated while enabling local deployment at near-zero cost. By automatically labeling over 9,640 real-world images with high accuracy, SAVANT addresses the critical data scarcity problem in anomaly detection and provides a practical path toward reliable, accessible semantic monitoring for autonomous systems.", "authors": ["Roberto Brusnicki", "David Pop", "Yuan Gao", "Mattia Piccinini", "Johannes Betz"], "published": "2025-10-20T19:14:29Z", "updated": "2025-10-20T19:14:29Z", "link_pdf": "http://arxiv.org/pdf/2510.18034v1", "link_page": "http://arxiv.org/abs/2510.18034v1"}
{"id": "2510.18032v1", "title": "OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal   Reinforcement Learning for Enhanced Reasoning", "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities in mathematical and scientific tasks. To enhance complex reasoning, multi-agent systems have been proposed to harness the collective intelligence of LLM agents. However, existing collaboration structures are either predefined or rely on majority voting or round-table debates, which can suppress correct but less dominant agent contributions. Recent approaches model multi-agent systems as graph networks but optimize purely for agent performance, neglecting the quality of interactions. We hypothesize that effective agent communication is crucial for multi-agent reasoning and that debating quality plays a significant role. To address this, we propose $\\ours$, a multi-agent verbal reinforcement learning algorithm that dynamically constructs and refines multi-agent collaboration structures. Our method defines action spaces and a feedback mechanism that evaluates communication robustness and coherence throughout the debate. The final decision is achieved through a majority vote over all the agents. We assess $\\ours$ on various reasoning tasks, including mathematical reasoning, creative writing, scientific reasoning, and numerical sorting. Results demonstrate that our approach significantly outperforms single-agent prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.", "authors": ["Zhenyu Bi", "Meng Lu", "Yang Li", "Swastik Roy", "Weijie Guan", "Morteza Ziyadi", "Xuan Wang"], "published": "2025-10-20T19:07:51Z", "updated": "2025-10-20T19:07:51Z", "link_pdf": "http://arxiv.org/pdf/2510.18032v1", "link_page": "http://arxiv.org/abs/2510.18032v1"}
{"id": "2510.18030v1", "title": "From Local to Global: Revisiting Structured Pruning Paradigms for Large   Language Models", "summary": "Structured pruning is a practical approach to deploying large language models (LLMs) efficiently, as it yields compact, hardware-friendly architectures. However, the dominant local paradigm is task-agnostic: by optimizing layer-wise reconstruction rather than task objectives, it tends to preserve perplexity or generic zero-shot behavior but fails to capitalize on modest task-specific calibration signals, often yielding limited downstream gains. We revisit global structured pruning and present GISP-Global Iterative Structured Pruning-a post-training method that removes attention heads and MLP channels using first-order, loss-based important weights aggregated at the structure level with block-wise normalization. An iterative schedule, rather than one-shot pruning, stabilizes accuracy at higher sparsity and mitigates perplexity collapse without requiring intermediate fine-tuning; the pruning trajectory also forms nested subnetworks that support a \"prune-once, deploy-many\" workflow. Furthermore, because importance is defined by a model-level loss, GISP naturally supports task-specific objectives; we instantiate perplexity for language modeling and a margin-based objective for decision-style tasks. Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves downstream accuracy, with especially strong gains at 40-50% sparsity; on DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration substantially boosts exact-match accuracy.", "authors": ["Ziyan Wang", "Enmao Diao", "Qi Le", "Pu Wang", "Minwoo Lee", "Shu-ping Yeh", "Evgeny Stupachenko", "Hao Feng", "Li Yang"], "published": "2025-10-20T19:04:09Z", "updated": "2025-10-20T19:04:09Z", "link_pdf": "http://arxiv.org/pdf/2510.18030v1", "link_page": "http://arxiv.org/abs/2510.18030v1"}
{"id": "2510.18029v1", "title": "DynaQuery: A Self-Adapting Framework for Querying Structured and   Multimodal Data", "summary": "The rise of Large Language Models (LLMs) has accelerated the long-standing goal of enabling natural language querying over complex, hybrid databases. Yet, this ambition exposes a dual challenge: reasoning jointly over structured, multi-relational schemas and the semantic content of linked unstructured assets. To overcome this, we present DynaQuery - a unified, self-adapting framework that serves as a practical blueprint for next-generation \"Unbound Databases.\" At the heart of DynaQuery lies the Schema Introspection and Linking Engine (SILE), a novel systems primitive that elevates schema linking to a first-class query planning phase. We conduct a rigorous, multi-benchmark empirical evaluation of this structure-aware architecture against the prevalent unstructured Retrieval-Augmented Generation (RAG) paradigm. Our results demonstrate that the unstructured retrieval paradigm is architecturally susceptible to catastrophic contextual failures, such as SCHEMA_HALLUCINATION, leading to unreliable query generation. In contrast, our SILE-based design establishes a substantially more robust foundation, nearly eliminating this failure mode. Moreover, end-to-end validation on a complex, newly curated benchmark uncovers a key generalization principle: the transition from pure schema-awareness to holistic semantics-awareness. Taken together, our findings provide a validated architectural basis for developing natural language database interfaces that are robust, adaptable, and predictably consistent.", "authors": ["Aymane Hassini"], "published": "2025-10-20T19:02:35Z", "updated": "2025-10-20T19:02:35Z", "link_pdf": "http://arxiv.org/pdf/2510.18029v1", "link_page": "http://arxiv.org/abs/2510.18029v1"}
{"id": "2510.18019v1", "title": "Is Multilingual LLM Watermarking Truly Multilingual? A Simple   Back-Translation Solution", "summary": "Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.", "authors": ["Asim Mohamed", "Martin Gubri"], "published": "2025-10-20T18:51:20Z", "updated": "2025-10-20T18:51:20Z", "link_pdf": "http://arxiv.org/pdf/2510.18019v1", "link_page": "http://arxiv.org/abs/2510.18019v1"}
{"id": "2510.17995v1", "title": "FABRIC: Framework for Agent-Based Realistic Intelligence Creation", "summary": "Large language models (LLMs) are increasingly deployed as agents, expected to decompose goals, invoke tools, and verify results in dynamic environments. Realizing these capabilities requires access to agentic data-structured interaction records that couple user intents with tool specifications, argument-grounded calls, and verifiable execution traces. However, collecting such data from human annotators is costly, time-consuming, and difficult to scale. We present a unified framework for synthesizing agentic data using only LLMs, without any human-in-the-loop supervision. This framework decomposes generation into modular pipelines that produce complete interaction records spanning task specifications, tool definitions, policy pseudocode, natural language exchanges, and execution traces. Records conform to strict syntactic and semantic constraints, ensuring machine-parseability and faithful alignment across inputs, outputs, and tool calls. Beyond single tasks, there is support for both multi-task and multi-turn agent interactions, enabling the construction of datasets that reflect the full spectrum of tool-use competencies. To ensure quality and consistency, the framework integrates constrained generation formats, JSON-schema validation, and judge-based filtering. This paper formalizes the schema for agentic records, details the prompt design principles that guide generation, and introduces scalable pipelines for high-quality synthetic data. By providing a reproducible, LLM-only alternative to manual collection, hence advancing the development of agentic LLMs capable of robust tool use.", "authors": ["Abhigya Verma", "Seganrasan Subramanian", "Nandhakumar Kandasamy", "Naman Gupta"], "published": "2025-10-20T18:20:22Z", "updated": "2025-10-20T18:20:22Z", "link_pdf": "http://arxiv.org/pdf/2510.17995v1", "link_page": "http://arxiv.org/abs/2510.17995v1"}
{"id": "2510.17802v1", "title": "Unbiased Gradient Low-Rank Projection", "summary": "Memory-efficient optimization is critical for training increasingly large language models (LLMs). A popular strategy involves gradient low-rank projection, storing only the projected optimizer states, with GaLore being a representative example. However, a significant drawback of many such methods is their lack of convergence guarantees, as various low-rank projection approaches introduce inherent biases relative to the original optimization algorithms, which contribute to performance gaps compared to full-parameter training. Aiming to tackle this problem, this paper investigates the layerwise sampling technique for debiasing low-rank projection mechanisms. In particular, an instantiation of the paradigm gives rise to a novel and unbiased low-rank optimization method built upon GaLore's mechanism and the Muon algorithm, named GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the convergence guarantees of the base Muon algorithm while preserving the memory efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and pretraining also demonstrate non-trivial improvements over GaLore and even better performance than full-parameter training. Further investigation shows that the improvement of this technique comes from a more uniform distribution of knowledge inside layers, leading to more efficient utilization of the model parameter space and better memorization.", "authors": ["Rui Pan", "Yang Luo", "Yuxing Liu", "Yang You", "Tong Zhang"], "published": "2025-10-20T17:59:25Z", "updated": "2025-10-20T17:59:25Z", "link_pdf": "http://arxiv.org/pdf/2510.17802v1", "link_page": "http://arxiv.org/abs/2510.17802v1"}
{"id": "2510.17801v1", "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large   Language Models as Embodied Brain", "summary": "Building robots that can perceive, reason, and act in dynamic, unstructured environments remains a core challenge. Recent embodied systems often adopt a dual-system paradigm, where System 2 handles high-level reasoning while System 1 executes low-level control. In this work, we refer to System 2 as the embodied brain, emphasizing its role as the cognitive core for reasoning and decision-making in manipulation tasks. Given this role, systematic evaluation of the embodied brain is essential. Yet existing benchmarks emphasize execution success, or when targeting high-level reasoning, suffer from incomplete dimensions and limited task realism, offering only a partial picture of cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark that systematically evaluates multimodal large language models (MLLMs) as embodied brains. Motivated by the critical roles across the full manipulation pipeline, RoboBench defines five dimensions-instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure realism, we curate datasets across diverse embodiments, attribute-rich objects, and multi-view scenes, drawing from large-scale real robotic data. For planning, RoboBench introduces an evaluation framework, MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether predicted plans can achieve critical object-state changes. Experiments on 14 MLLMs reveal fundamental limitations: difficulties with implicit instruction comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained affordance understanding, and execution failure diagnosis. RoboBench provides a comprehensive scaffold to quantify high-level cognition, and guide the development of next-generation embodied MLLMs. The project page is in https://robo-bench.github.io.", "authors": ["Yulin Luo", "Chun-Kai Fan", "Menghang Dong", "Jiayu Shi", "Mengdi Zhao", "Bo-Wen Zhang", "Cheng Chi", "Jiaming Liu", "Gaole Dai", "Rongyu Zhang", "Ruichuan An", "Kun Wu", "Zhengping Che", "Shaoxuan Xie", "Guocai Yao", "Zhongxia Zhao", "Pengwei Wang", "Guang Liu", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "published": "2025-10-20T17:59:03Z", "updated": "2025-10-20T17:59:03Z", "link_pdf": "http://arxiv.org/pdf/2510.17801v1", "link_page": "http://arxiv.org/abs/2510.17801v1"}
{"id": "2510.17800v2", "title": "Glyph: Scaling Context Windows via Visual-Text Compression", "summary": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.", "authors": ["Jiale Cheng", "Yusen Liu", "Xinyu Zhang", "Yulin Fei", "Wenyi Hong", "Ruiliang Lyu", "Weihan Wang", "Zhe Su", "Xiaotao Gu", "Xiao Liu", "Yushi Bai", "Jie Tang", "Hongning Wang", "Minlie Huang"], "published": "2025-10-20T17:58:56Z", "updated": "2025-10-21T17:12:48Z", "link_pdf": "http://arxiv.org/pdf/2510.17800v2", "link_page": "http://arxiv.org/abs/2510.17800v2"}
{"id": "2510.17795v1", "title": "Executable Knowledge Graphs for Replicating AI Research", "summary": "Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code will released at https://github.com/zjunlp/xKG.", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "published": "2025-10-20T17:53:23Z", "updated": "2025-10-20T17:53:23Z", "link_pdf": "http://arxiv.org/pdf/2510.17795v1", "link_page": "http://arxiv.org/abs/2510.17795v1"}
{"id": "2510.17786v1", "title": "Inference-Time Compute Scaling For Flow Matching", "summary": "Allocating extra computation at inference time has recently improved sample quality in large language models and diffusion-based image generation. In parallel, Flow Matching (FM) has gained traction in language, vision, and scientific domains, but inference-time scaling methods for it remain under-explored. Concurrently, Kim et al., 2025 approach this problem but replace the linear interpolant with a non-linear variance-preserving (VP) interpolant at inference, sacrificing FM's efficient and straight sampling. Additionally, inference-time compute scaling for flow matching has only been applied to visual tasks, like image generation. We introduce novel inference-time scaling procedures for FM that preserve the linear interpolant during sampling. Evaluations of our method on image generation, and for the first time (to the best of our knowledge), unconditional protein generation, show that I) sample quality consistently improves as inference compute increases, and II) flow matching inference-time scaling can be applied to scientific domains.", "authors": ["Adam Stecklov", "Noah El Rimawi-Fine", "Mathieu Blanchette"], "published": "2025-10-20T17:44:17Z", "updated": "2025-10-20T17:44:17Z", "link_pdf": "http://arxiv.org/pdf/2510.17786v1", "link_page": "http://arxiv.org/abs/2510.17786v1"}
{"id": "2510.17947v1", "title": "PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of   Multi-turn Exploits", "summary": "Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.", "authors": ["Neeladri Bhuiya", "Madhav Aggarwal", "Diptanshu Purwar"], "published": "2025-10-20T17:37:03Z", "updated": "2025-10-20T17:37:03Z", "link_pdf": "http://arxiv.org/pdf/2510.17947v1", "link_page": "http://arxiv.org/abs/2510.17947v1"}
{"id": "2510.17777v1", "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference", "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.", "authors": ["Samir Khaki", "Junxian Guo", "Jiaming Tang", "Shang Yang", "Yukang Chen", "Konstantinos N. Plataniotis", "Yao Lu", "Song Han", "Zhijian Liu"], "published": "2025-10-20T17:35:47Z", "updated": "2025-10-20T17:35:47Z", "link_pdf": "http://arxiv.org/pdf/2510.17777v1", "link_page": "http://arxiv.org/abs/2510.17777v1"}
{"id": "2510.17771v1", "title": "Seeing but Not Believing: Probing the Disconnect Between Visual   Attention and Answer Correctness in VLMs", "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term ``seeing but not believing'' that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.", "authors": ["Zhining Liu", "Ziyi Chen", "Hui Liu", "Chen Luo", "Xianfeng Tang", "Suhang Wang", "Joy Zeng", "Zhenwei Dai", "Zhan Shi", "Tianxin Wei", "Benoit Dumoulin", "Hanghang Tong"], "published": "2025-10-20T17:31:09Z", "updated": "2025-10-20T17:31:09Z", "link_pdf": "http://arxiv.org/pdf/2510.17771v1", "link_page": "http://arxiv.org/abs/2510.17771v1"}
{"id": "2510.17764v1", "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from   Benchmarks to Applications", "summary": "Medical Large language models achieve strong scores on standard benchmarks; however, the transfer of those results to safe and reliable performance in clinical workflows remains a challenge. This survey reframes evaluation through a levels-of-autonomy lens (L0-L3), spanning informational tools, information transformation and aggregation, decision support, and supervised agents. We align existing benchmarks and metrics with the actions permitted at each level and their associated risks, making the evaluation targets explicit. This motivates a level-conditioned blueprint for selecting metrics, assembling evidence, and reporting claims, alongside directions that link evaluation to oversight. By centering autonomy, the survey moves the field beyond score-based claims toward credible, risk-aware evidence for real clinical use.", "authors": ["Xiao Ye", "Jacob Dineen", "Zhaonan Li", "Zhikun Xu", "Weiyu Chen", "Shijie Lu", "Yuxi Huang", "Ming Shen", "Phu Tran", "Ji-Eun Irene Yum", "Muhammad Ali Khan", "Muhammad Umar Afzal", "Irbaz Bin Riaz", "Ben Zhou"], "published": "2025-10-20T17:22:32Z", "updated": "2025-10-20T17:22:32Z", "link_pdf": "http://arxiv.org/pdf/2510.17764v1", "link_page": "http://arxiv.org/abs/2510.17764v1"}
{"id": "2510.17759v1", "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language   Models", "summary": "Vision-Language Models (VLMs) extend large language models with visual reasoning, but their multimodal design also introduces new, underexplored vulnerabilities. Existing multimodal red-teaming methods largely rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o.", "authors": ["Qilin Liao", "Anamika Lochab", "Ruqi Zhang"], "published": "2025-10-20T17:12:10Z", "updated": "2025-10-20T17:12:10Z", "link_pdf": "http://arxiv.org/pdf/2510.17759v1", "link_page": "http://arxiv.org/abs/2510.17759v1"}
{"id": "2510.17941v1", "title": "Believe It or Not: How Deeply do LLMs Believe Implanted Facts?", "summary": "Knowledge editing techniques promise to implant new factual knowledge into large language models (LLMs). But do LLMs really believe these facts? We develop a framework to measure belief depth and use it to evaluate the success of knowledge editing techniques. We operationalize belief depth as the extent to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi estimates several logical steps removed), 2) is robust to self-scrutiny and direct challenge, and 3) is represented similarly to genuine knowledge (as measured by linear probes). Our evaluations show that simple prompting and mechanistic editing techniques fail to implant knowledge deeply. In contrast, Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated documents consistent with a fact - often succeeds at implanting beliefs that behave similarly to genuine knowledge. However, SDF's success is not universal, as implanted beliefs that contradict basic world knowledge are brittle and representationally distinct from genuine knowledge. Overall, our work introduces measurable criteria for belief depth and enables the rigorous evaluation necessary for deploying knowledge editing in real-world applications.", "authors": ["Stewart Slocum", "Julian Minder", "Clément Dumas", "Henry Sleight", "Ryan Greenblatt", "Samuel Marks", "Rowan Wang"], "published": "2025-10-20T16:58:54Z", "updated": "2025-10-20T16:58:54Z", "link_pdf": "http://arxiv.org/pdf/2510.17941v1", "link_page": "http://arxiv.org/abs/2510.17941v1"}
{"id": "2510.17727v2", "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs", "summary": "Black-box Large Language Models (LLMs) provide practical and accessible alternatives to other machine learning methods, as they require minimal labeled data and machine learning expertise to develop solutions for various decision making problems. However, for applications that need operating with constraints on specific metrics (e.g., precision $\\geq$ 95%), decision making with black-box LLMs remains unfavorable, due to their low numerical output cardinalities. This results in limited control over their operating points, preventing fine-grained adjustment of their decision making behavior. In this paper, we study using black-box LLMs as classifiers, focusing on efficiently improving their operational granularity without performance loss. Specifically, we first investigate the reasons behind their low-cardinality numerical outputs and show that they are biased towards generating rounded but informative verbalized probabilities. Then, we experiment with standard prompt engineering, uncertainty estimation and confidence elicitation techniques, and observe that they do not effectively improve operational granularity without sacrificing performance or increasing inference cost. Finally, we propose efficient approaches to significantly increase the number and diversity of available operating points. Our proposed approaches provide finer-grained operating points and achieve comparable to or better performance than the benchmark methods across 11 datasets and 3 LLMs.", "authors": ["Ege Beyazit", "KL Navaneet", "Prashant Mathur", "Roi Blanco", "Vidit Bansal", "Karim Bouyarmane"], "published": "2025-10-20T16:43:06Z", "updated": "2025-10-21T05:22:16Z", "link_pdf": "http://arxiv.org/pdf/2510.17727v2", "link_page": "http://arxiv.org/abs/2510.17727v2"}
{"id": "2510.17725v1", "title": "AcademicEval: Live Long-Context LLM Benchmark", "summary": "Large Language Models (LLMs) have recently achieved remarkable performance in long-context understanding. However, current long-context LLM benchmarks are limited by rigid context length, labor-intensive annotation, and the pressing challenge of label leakage issues during LLM training. Therefore, we propose \\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context generation tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce several academic writing tasks with long-context inputs, \\textit{i.e.}, \\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related Work}, which cover a wide range of abstraction levels and require no manual labeling. Moreover, \\textsc{AcademicEval} integrates high-quality and expert-curated few-shot demonstrations from a collected co-author graph to enable flexible context length. Especially, \\textsc{AcademicEval} features an efficient live evaluation, ensuring no label leakage. We conduct a holistic evaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs perform poorly on tasks with hierarchical abstraction levels and tend to struggle with long few-shot demonstrations, highlighting the challenge of our benchmark. Through experimental analysis, we also reveal some insights for enhancing LLMs' long-context modeling capabilities. Code is available at https://github.com/ulab-uiuc/AcademicEval", "authors": ["Haozhen Zhang", "Tao Feng", "Pengrui Han", "Jiaxuan You"], "published": "2025-10-20T16:42:30Z", "updated": "2025-10-20T16:42:30Z", "link_pdf": "http://arxiv.org/pdf/2510.17725v1", "link_page": "http://arxiv.org/abs/2510.17725v1"}
{"id": "2510.17722v1", "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating   Multimodal LLMs in Multi-Turn Dialogues", "summary": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.", "authors": ["Yaning Pan", "Zekun Wang", "Qianqian Xie", "Yongqian Wen", "Yuanxing Zhang", "Guohui Zhang", "Haoxuan Hu", "Zhiyu Pan", "Yibing Huang", "Zhidong Gan", "Yonghong Lin", "An Ping", "Tianhao Peng", "Jiaheng Liu"], "published": "2025-10-20T16:38:40Z", "updated": "2025-10-20T16:38:40Z", "link_pdf": "http://arxiv.org/pdf/2510.17722v1", "link_page": "http://arxiv.org/abs/2510.17722v1"}
{"id": "2510.17715v1", "title": "QueST: Incentivizing LLMs to Generate Difficult Problems", "summary": "Large Language Models have achieved strong performance on reasoning tasks, solving competition-level coding and math problems. However, their scalability is limited by human-labeled datasets and the lack of large-scale, challenging coding problem training data. Existing competitive coding datasets contain only thousands to tens of thousands of problems. Previous synthetic data generation methods rely on either augmenting existing instruction datasets or selecting challenging problems from human-labeled data. In this paper, we propose QueST, a novel framework which combines difficulty-aware graph sampling and difficulty-aware rejection fine-tuning that directly optimizes specialized generators to create challenging coding problems. Our trained generators demonstrate superior capability compared to even GPT-4o at creating challenging problems that benefit downstream performance. We leverage QueST to generate large-scale synthetic coding problems, which we then use to distill from strong teacher models with long chain-of-thought or to conduct reinforcement learning for smaller models, proving effective in both scenarios. Our distillation experiments demonstrate significant performance gains. Specifically, after fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we surpass the performance of the original Qwen3-8B on LiveCodeBench. With an additional 112K examples (i.e., 28K human-written problems paired with multiple synthetic solutions), our 8B model matches the performance of the much larger DeepSeek-R1-671B. These findings indicate that generating complex problems via QueST offers an effective and scalable approach to advancing the frontiers of competitive coding and reasoning for large language models.", "authors": ["Hanxu Hu", "Xingxing Zhang", "Jannis Vamvas", "Rico Sennrich", "Furu Wei"], "published": "2025-10-20T16:29:53Z", "updated": "2025-10-20T16:29:53Z", "link_pdf": "http://arxiv.org/pdf/2510.17715v1", "link_page": "http://arxiv.org/abs/2510.17715v1"}
{"id": "2510.17705v1", "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation   in Large Language Models", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities but struggle with multi-task adaptation, particularly in balancing knowledge retention with task-specific specialization. Conventional fine-tuning methods suffer from catastrophic forgetting and substantial resource consumption, while existing parameter-efficient methods perform suboptimally in complex multi-task scenarios. To address this, we propose Contextual Attention Modulation (CAM), a novel mechanism that dynamically modulates the representations of self-attention modules in LLMs. CAM enhances task-specific features while preserving general knowledge, thereby facilitating more effective and efficient adaptation. For effective multi-task adaptation, CAM is integrated into our Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a shared, full-parameter CAM module with multiple specialized, lightweight CAM modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion. Extensive experiments on heterogeneous tasks, including question answering, code generation, and logical reasoning, demonstrate that our approach significantly outperforms existing approaches, achieving an average performance improvement of 3.65%. The implemented code and data are available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "published": "2025-10-20T16:19:27Z", "updated": "2025-10-20T16:19:27Z", "link_pdf": "http://dx.doi.org/10.1145/3746252.3761289", "link_page": "http://arxiv.org/abs/2510.17705v1"}
{"id": "2510.17698v1", "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM   Educational Dialogues", "summary": "Dialogue plays a crucial role in educational settings, yet existing evaluation methods for educational applications of large language models (LLMs) primarily focus on technical performance or learning outcomes, often neglecting attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral Consortium paper presents an ongoing study employing a dialogue analysis approach to identify effective pedagogical strategies from learner-LLM dialogues. The proposed approach involves dialogue data collection, dialogue act (DA) annotation, DA pattern mining, and predictive model building. Early insights are outlined as an initial step toward future research. The work underscores the need to evaluate LLM-based educational applications by focusing on dialogue dynamics and pedagogical strategies.", "authors": ["Liqun He", "Manolis Mavrikis", "Mutlu Cukurova"], "published": "2025-10-20T16:11:34Z", "updated": "2025-10-20T16:11:34Z", "link_pdf": "http://dx.doi.org/10.1007/978-3-031-99261-2_42", "link_page": "http://arxiv.org/abs/2510.17698v1"}
{"id": "2510.17687v1", "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious   Attacks", "summary": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and perception capabilities but are increasingly vulnerable to jailbreak attacks. While existing work focuses on explicit attacks, where malicious content resides in a single modality, recent studies reveal implicit attacks, in which benign text and image inputs jointly express unsafe intent. Such joint-modal threats are difficult to detect and remain underexplored, largely due to the scarcity of high-quality implicit data. We propose ImpForge, an automated red-teaming pipeline that leverages reinforcement learning with tailored reward modules to generate diverse implicit samples across 14 domains. Building on this dataset, we further develop CrossGuard, an intent-aware safeguard providing robust and comprehensive defense against both explicit and implicit threats. Extensive experiments across safe and unsafe benchmarks, implicit and explicit attacks, and multiple out-of-domain settings demonstrate that CrossGuard significantly outperforms existing defenses, including advanced MLLMs and guardrails, achieving stronger security while maintaining high utility. This offers a balanced and practical solution for enhancing MLLM robustness against real-world multimodal threats.", "authors": ["Xu Zhang", "Hao Li", "Zhichao Lu"], "published": "2025-10-20T16:02:34Z", "updated": "2025-10-20T16:02:34Z", "link_pdf": "http://arxiv.org/pdf/2510.17687v1", "link_page": "http://arxiv.org/abs/2510.17687v1"}
{"id": "2510.17685v1", "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation   Reasoning and Aligning", "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person using textual descriptions, facing challenge in modality heterogeneity. Prior works have attempted to address it by developing cross-modal global or local alignment strategies. However, global methods typically overlook fine-grained cross-modal differences, whereas local methods require prior information to explore explicit part alignments. Additionally, current methods are English-centric, restricting their application in multilingual contexts. To alleviate these issues, we pioneer a multilingual TIPR task by developing a multilingual TIPR benchmark, for which we leverage large language models for initial translations and refine them by integrating domain-specific knowledge. Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation Reasoning and Aligning framework to learn alignment across languages and modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module enables bidirectional prediction of masked image and text, implicitly enhancing the modeling of local relations across languages and modalities, a multi-dimensional global alignment module is integrated to bridge the modality heterogeneity. The proposed method achieves new state-of-the-art results on all multilingual TIPR datasets. Data and code are presented in https://github.com/Flame-Chasers/Bi-IRRA.", "authors": ["Min Cao", "Xinyu Zhou", "Ding Jiang", "Bo Du", "Mang Ye", "Min Zhang"], "published": "2025-10-20T16:01:11Z", "updated": "2025-10-20T16:01:11Z", "link_pdf": "http://dx.doi.org/10.1109/TPAMI.2025.3620139", "link_page": "http://arxiv.org/abs/2510.17685v1"}
{"id": "2510.17671v1", "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback", "summary": "For many real-world applications, feedback is essential in translating complex, nuanced, or subjective goals into quantifiable optimization objectives. We propose a language-in-the-loop framework that uses a large language model (LLM) to convert unstructured feedback in the form of natural language into scalar utilities to conduct BO over a numeric search space. Unlike preferential BO, which only accepts restricted feedback formats and requires customized models for each domain-specific problem, our approach leverages LLMs to turn varied types of textual feedback into consistent utility signals and to easily include flexible user priors without manual kernel design. At the same time, our method maintains the sample efficiency and principled uncertainty quantification of BO. We show that this hybrid method not only provides a more natural interface to the decision maker but also outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes.", "authors": ["Katarzyna Kobalczyk", "Zhiyuan Jerry Lin", "Benjamin Letham", "Zhuokai Zhao", "Maximilian Balandat", "Eytan Bakshy"], "published": "2025-10-20T15:41:56Z", "updated": "2025-10-20T15:41:56Z", "link_pdf": "http://arxiv.org/pdf/2510.17671v1", "link_page": "http://arxiv.org/abs/2510.17671v1"}
{"id": "2510.17934v1", "title": "AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB   VRAM", "summary": "Retrieval-augmented generation (RAG) has shown some success in augmenting large language models (LLMs) with external knowledge. However, as a non-parametric knowledge integration paradigm for LLMs, RAG methods heavily rely on external retrieval modules and the retrieved textual context prior. Especially for very large scale knowledge augmentation, they would introduce substantial inference latency due to expensive searches and much longer relevant context. In this paper, we propose a parametric knowledge integration method, called \\textbf{AtlasKV}, a scalable, effective, and general way to augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with sub-linear time and memory complexity. It maintains strong knowledge grounding and generalization performance using the LLMs' inherent attention mechanism, and requires no external retrievers, long context priors, or retraining when adapting to new knowledge.", "authors": ["Haoyu Huang", "Hong Ting Tsang", "Jiaxin Bai", "Xi Peng", "Gong Zhang", "Yangqiu Song"], "published": "2025-10-20T15:40:14Z", "updated": "2025-10-20T15:40:14Z", "link_pdf": "http://arxiv.org/pdf/2510.17934v1", "link_page": "http://arxiv.org/abs/2510.17934v1"}
{"id": "2510.17652v1", "title": "Qomhra: A Bilingual Irish-English Large Language Model", "summary": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language model (LLM), developed under low-resource constraints presenting a complete pipeline spanning bilingual continued pre-training, instruction tuning, and alignment from human preferences. Newly accessible Irish corpora and English text are mixed and curated to improve Irish performance while preserving English ability. 6 closed-weight LLMs are judged for their Irish text generation by a native speaker, a learner and other LLMs. Google's Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise instruction tuning and human preference datasets. Two datasets are contributed leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning dataset and a 1K human preference dataset, generating accepted and rejected responses that show near perfect alignment with a native Irish speaker. Qomhr\\'a is comprehensively evaluated across benchmarks testing translation, gender understanding, topic identification and world knowledge with gains of up to 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning and demonstrates clear progress in instruction following, crucial for chatbot functionality.", "authors": ["Joseph McInerney"], "published": "2025-10-20T15:27:53Z", "updated": "2025-10-20T15:27:53Z", "link_pdf": "http://arxiv.org/pdf/2510.17652v1", "link_page": "http://arxiv.org/abs/2510.17652v1"}
{"id": "2510.17651v1", "title": "Frugal Federated Learning for Violence Detection: A Comparison of   LoRA-Tuned VLMs and Personalized CNNs", "summary": "We examine frugal federated learning approaches to violence detection by comparing two complementary strategies: (i) zero-shot and federated fine-tuning of vision-language models (VLMs), and (ii) personalized training of a compact 3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter CNN3D as representative cases, we evaluate accuracy, calibration, and energy usage under realistic non-IID settings. Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy. VLMs remain favorable for contextual reasoning and multimodal inference. We quantify energy and CO$_2$ emissions across training and inference, and analyze sustainability trade-offs for deployment. To our knowledge, this is the first comparative study of LoRA-tuned vision-language models and personalized CNNs for federated violence detection, with an emphasis on energy efficiency and environmental metrics. These findings support a hybrid model: lightweight CNNs for routine classification, with selective VLM activation for complex or descriptive scenarios. The resulting framework offers a reproducible baseline for responsible, resource-aware AI in video surveillance, with extensions toward real-time, multimodal, and lifecycle-aware systems.", "authors": ["Sébastien Thuau", "Siba Haidar", "Ayush Bajracharya", "Rachid Chelouah"], "published": "2025-10-20T15:26:43Z", "updated": "2025-10-20T15:26:43Z", "link_pdf": "http://arxiv.org/pdf/2510.17651v1", "link_page": "http://arxiv.org/abs/2510.17651v1"}
{"id": "2510.17638v1", "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet   Arena", "summary": "Forecasting is not only a fundamental intellectual pursuit but also is of significant importance to societal systems such as finance and economics. With the rapid advances of large language models (LLMs) trained on Internet-scale data, it raises the promise of employing LLMs to forecast real-world future events, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper systematically investigates such predictive intelligence of LLMs. To this end, we build Prophet Arena, a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages, in order to support our controlled and large-scale experimentation. Our comprehensive evaluation reveals that many LLMs already exhibit impressive forecasting capabilities, reflected in, e.g., their small calibration errors, consistent prediction confidence and promising market returns. However, we also uncover key bottlenecks towards achieving superior predictive intelligence via LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of data sources and slower information aggregation compared to markets when resolution nears.", "authors": ["Qingchuan Yang", "Simon Mahns", "Sida Li", "Anri Gu", "Jibang Wu", "Haifeng Xu"], "published": "2025-10-20T15:20:05Z", "updated": "2025-10-20T15:20:05Z", "link_pdf": "http://arxiv.org/pdf/2510.17638v1", "link_page": "http://arxiv.org/abs/2510.17638v1"}
{"id": "2510.17620v1", "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large   Language Models", "summary": "Large language models may encode sensitive information or outdated knowledge that needs to be removed, to ensure responsible and compliant model responses. Unlearning has emerged as an efficient alternative to full retraining, aiming to remove specific knowledge while preserving overall model utility. Existing evaluations of unlearning methods focus on (1) the extent of forgetting of the target knowledge (forget set) and (2) maintaining performance on the retain set (i.e., utility). However, these evaluations overlook an important usability aspect: users may still want the model to leverage the removed information if it is re-introduced in the prompt. In a systematic evaluation of six state-of-the-art unlearning methods, we find that they consistently impair such contextual utility. To address this, we augment unlearning objectives with a plug-in term that preserves the model's ability to use forgotten knowledge when it is present in context. Extensive experiments demonstrate that our approach restores contextual utility to near original levels while still maintaining effective forgetting and retain-set utility.", "authors": ["Yuefeng Peng", "Parnian Afshar", "Megan Ganji", "Thomas Butler", "Amir Houmansadr", "Mingxian Wang", "Dezhi Hong"], "published": "2025-10-20T15:03:45Z", "updated": "2025-10-20T15:03:45Z", "link_pdf": "http://arxiv.org/pdf/2510.17620v1", "link_page": "http://arxiv.org/abs/2510.17620v1"}
{"id": "2510.17602v1", "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis", "summary": "Legal reasoning is a fundamental component of legal analysis and decision-making. Existing computational approaches to legal reasoning predominantly rely on generic reasoning frameworks such as syllogism and IRAC, which do not comprehensively examine the nuanced processes that underpin legal reasoning. Moreover, current research has largely focused on criminal cases, with insufficient modeling for civil cases. In this work, we present a novel framework for explicitly modeling legal reasoning in the analysis of Chinese tort-related civil cases. We first operationalize the legal reasoning processes used in tort analysis into the LawChain framework. LawChain is a three-module reasoning framework, with each module consisting of multiple finer-grained sub-steps. Informed by the LawChain framework, we introduce the task of tort legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to systematically assess the critical steps within analytical reasoning chains for tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large language models for their legal reasoning ability in civil tort contexts. Our results indicate that current models still fall short in accurately handling crucial elements of tort legal reasoning. Furthermore, we introduce several baseline approaches that explicitly incorporate LawChain-style reasoning through prompting or post-training. We conduct further experiments on additional legal analysis tasks, such as Legal Named-Entity Recognition and Criminal Damages Calculation, to verify the generalizability of these baselines. The proposed baseline approaches achieve significant improvements in tort-related legal reasoning and generalize well to related legal analysis tasks, thus demonstrating the value of explicitly modeling legal reasoning chains to enhance the reasoning capabilities of language models.", "authors": ["Huiyuan Xie", "Chenyang Li", "Huining Zhu", "Chubin Zhang", "Yuxiao Ye", "Zhenghao Liu", "Zhiyuan Liu"], "published": "2025-10-20T14:50:58Z", "updated": "2025-10-20T14:50:58Z", "link_pdf": "http://arxiv.org/pdf/2510.17602v1", "link_page": "http://arxiv.org/abs/2510.17602v1"}
{"id": "2510.17598v1", "title": "Reasoning Distillation and Structural Alignment for Improved Code   Generation", "summary": "Effective code generation with language models hinges on two critical factors: accurately understanding the intent of the prompt and generating code that applies algorithmic reasoning to produce correct solutions capable of passing diverse test cases while adhering to the syntax of the target programming language. Unlike other language tasks, code generation requires more than accurate token prediction; it demands comprehension of solution-level and structural relationships rather than merely generating the most likely tokens. very large language model (VLLM) are capable of generating detailed steps toward the correct solution of complex tasks where reasoning is crucial in solving the problem. Such reasoning capabilities may be absent in smaller language models. Therefore, in this work, we distill the reasoning capabilities of a VLLM into a smaller, more efficient model that is faster and cheaper to deploy. Our approach trains the model to emulate the reasoning and problem-solving abilities of the VLLM by learning to identify correct solution pathways and establishing a structural correspondence between problem definitions and potential solutions through a novel method of structure-aware loss optimization. This enables the model to transcend token-level generation and to deeply grasp the overarching structure of solutions for given problems. Experimental results show that our fine-tuned model, developed through a cheap and simple to implement process, significantly outperforms our baseline model in terms of pass@1, average data flow, and average syntax match metrics across the MBPP, MBPP Plus, and HumanEval benchmarks.", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "published": "2025-10-20T14:47:47Z", "updated": "2025-10-20T14:47:47Z", "link_pdf": "http://arxiv.org/pdf/2510.17598v1", "link_page": "http://arxiv.org/abs/2510.17598v1"}
{"id": "2510.17590v1", "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with   Web-Grounded Reasoning", "summary": "Misinformation spreads across web platforms through billions of daily multimodal posts that combine text and images, overwhelming manual fact-checking capacity. Supervised detection models require domain-specific training data and fail to generalize across diverse manipulation tactics. We present MIRAGE, an inference-time, model-pluggable agentic framework that decomposes multimodal verification into four sequential modules: visual veracity assessment detects AI-generated images, cross-modal consistency analysis identifies out-of-context repurposing, retrieval-augmented factual checking grounds claims in web evidence through iterative question generation, and a calibrated judgment module integrates all signals. MIRAGE orchestrates vision-language model reasoning with targeted web retrieval, outputs structured and citation-linked rationales. On MMFakeBench validation set (1,000 samples), MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65 points while maintaining 34.3% false positive rate versus 97.3% for a judge-only baseline. Test set results (5,000 samples) confirm generalization with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97 points. Our results demonstrate that decomposed agentic reasoning with web retrieval can match supervised detector performance without domain-specific training, enabling misinformation detection across modalities where labeled data remains scarce.", "authors": ["Mir Nafis Sharear Shopnil", "Sharad Duwal", "Abhishek Tyagi", "Adiba Mahbub Proma"], "published": "2025-10-20T14:40:26Z", "updated": "2025-10-20T14:40:26Z", "link_pdf": "http://arxiv.org/pdf/2510.17590v1", "link_page": "http://arxiv.org/abs/2510.17590v1"}
{"id": "2510.17576v1", "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot   Disassembly: Demonstration on EV Batteries", "summary": "This paper addresses the problem of planning complex manipulation tasks, in which multiple robots with different end-effectors and capabilities, informed by computer vision, must plan and execute concatenated sequences of actions on a variety of objects that can appear in arbitrary positions and configurations in unstructured scenes. We propose an intent-driven planning pipeline which can robustly construct such action sequences with varying degrees of supervisory input from a human using simple language instructions. The pipeline integrates: (i) perception-to-text scene encoding, (ii) an ensemble of large language models (LLMs) that generate candidate removal sequences based on the operator's intent, (iii) an LLM-based verifier that enforces formatting and precedence constraints, and (iv) a deterministic consistency filter that rejects hallucinated objects. The pipeline is evaluated on an example task in which two robot arms work collaboratively to dismantle an Electric Vehicle battery for recycling applications. A variety of components must be grasped and removed in specific sequences, determined by human instructions and/or by task-order feasibility decisions made by the autonomous system. On 200 real scenes with 600 operator prompts across five component classes, we used metrics of full-sequence correctness and next-task correctness to evaluate and compare five LLM-based planners (including ablation analyses of pipeline components). We also evaluated the LLM-based human interface in terms of time to execution and NASA TLX with human participant experiments. Results indicate that our ensemble-with-verification approach reliably maps operator intent to safe, executable multi-robot plans while maintaining low user effort.", "authors": ["Cansu Erdogan", "Cesar Alan Contreras", "Alireza Rastegarpanah", "Manolis Chiou", "Rustam Stolkin"], "published": "2025-10-20T14:24:39Z", "updated": "2025-10-20T14:24:39Z", "link_pdf": "http://arxiv.org/pdf/2510.17576v1", "link_page": "http://arxiv.org/abs/2510.17576v1"}
{"id": "2510.17555v1", "title": "Language Confusion Gate: Language-Aware Decoding Through Model   Self-Distillation", "summary": "Large language models (LLMs) often experience language confusion, which is the unintended mixing of languages during text generation. Current solutions to this problem either necessitate model retraining or cannot differentiate between harmful confusion and acceptable code-switching. This paper introduces the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters tokens during decoding without altering the base LLM. The LCG is trained using norm-adjusted self-distillation to predict appropriate language families and apply masking only when needed. Our method is based on the findings that language confusion is infrequent, correct-language tokens are usually among the top predictions, and output token embedding norms are larger for high-resource languages, which biases sampling. When evaluated across various models, including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion significantly, often by an order of magnitude, without negatively impacting task performance. Code is available at https://github.com/collinzrj/language_confusion_gate.", "authors": ["Collin Zhang", "Fei Huang", "Chenhan Yuan", "Junyang Lin"], "published": "2025-10-20T14:02:37Z", "updated": "2025-10-20T14:02:37Z", "link_pdf": "http://arxiv.org/pdf/2510.17555v1", "link_page": "http://arxiv.org/abs/2510.17555v1"}
{"id": "2510.17532v1", "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and   Interpretable Survival Prediction", "summary": "Predicting cancer treatment outcomes requires models that are both accurate and interpretable, particularly in the presence of heterogeneous clinical data. While large language models (LLMs) have shown strong performance in biomedical NLP, they often lack structured reasoning capabilities critical for high-stakes decision support. We present a unified, multi-task learning framework that aligns autoregressive LLMs with clinical reasoning for outcome prediction on the MSK-CHORD dataset. Our models are trained to jointly perform binary survival classification, continuous survival time regression, and natural language rationale generation. We evaluate three alignment strategies: (1) standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT) prompting to elicit step-by-step reasoning, and (3) Group Relative Policy Optimization (GRPO), a reinforcement learning method that aligns model outputs to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and predictive performance across BLEU, ROUGE, and BERTScore. We further show that existing biomedical LLMs often fail to produce valid reasoning traces due to architectural constraints. Our findings underscore the importance of reasoning-aware alignment in multi-task clinical modeling and set a new benchmark for interpretable, trustworthy LLMs in precision oncology.", "authors": ["Raghu Vamshi Hemadri", "Geetha Krishna Guruju", "Kristi Topollai", "Anna Ewa Choromanska"], "published": "2025-10-20T13:35:12Z", "updated": "2025-10-20T13:35:12Z", "link_pdf": "http://arxiv.org/pdf/2510.17532v1", "link_page": "http://arxiv.org/abs/2510.17532v1"}
{"id": "2510.17516v2", "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate   Human Behaviors", "summary": "Large language model (LLM) simulations of human behavior have the potential to revolutionize the social and behavioral sciences, if and only if they faithfully reflect real human behaviors. Current evaluations are fragmented, based on bespoke tasks and metrics, creating a patchwork of incomparable results. To address this, we introduce SimBench, the first large-scale, standardized benchmark for a robust, reproducible science of LLM simulation. By unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, SimBench provides the necessary foundation to ask fundamental questions about when, how, and why LLM simulations succeed or fail. We show that, while even the best LLMs today have limited simulation ability (score: 40.80/100), performance scales log-linearly with model size. Simulation performance is not improved by increased inference-time compute. We demonstrate an alignment-simulation trade-off: instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones. Models particularly struggle when simulating specific demographic groups. Finally, we demonstrate that simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to accelerate the development of more faithful LLM simulators.", "authors": ["Tiancheng Hu", "Joachim Baumann", "Lorenzo Lupo", "Nigel Collier", "Dirk Hovy", "Paul Röttger"], "published": "2025-10-20T13:14:38Z", "updated": "2025-10-21T13:05:11Z", "link_pdf": "http://arxiv.org/pdf/2510.17516v2", "link_page": "http://arxiv.org/abs/2510.17516v2"}
{"id": "2510.17509v1", "title": "Annotation-Efficient Universal Honesty Alignment", "summary": "Honesty alignment-the ability of large language models (LLMs) to recognize their knowledge boundaries and express calibrated confidence-is essential for trustworthy deployment. Existing methods either rely on training-free confidence estimation (e.g., token probabilities, self-consistency) or training-based calibration with correctness annotations. While effective, achieving universal honesty alignment with training-based calibration requires costly, large-scale labeling. To support annotation-efficient training, we introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that first elicits internal confidence using inexpensive self-consistency supervision, then calibrates this confidence with a small set of correctness annotations. To support a large-scale study, we release HonestyBench, a benchmark covering ten free-form QA datasets with 560k training and 70k evaluation instances annotated with correctness and self-consistency signals. Experiments show that EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and better alignment performance on unseen MMLU tasks than the calibration-only baseline, offering a scalable solution toward universal honesty alignment in LLMs.", "authors": ["Shiyu Ni", "Keping Bi", "Jiafeng Guo", "Minghao Tang", "Jingtong Wu", "Zengxin Han", "Xueqi Cheng"], "published": "2025-10-20T13:05:22Z", "updated": "2025-10-20T13:05:22Z", "link_pdf": "http://arxiv.org/pdf/2510.17509v1", "link_page": "http://arxiv.org/abs/2510.17509v1"}
{"id": "2510.17498v1", "title": "Deep Self-Evolving Reasoning", "summary": "Long-form chain-of-thought reasoning has become a cornerstone of advanced reasoning in large language models. While recent verification-refinement frameworks have enabled proprietary models to solve Olympiad-level problems, their effectiveness hinges on strong, reliable verification and correction capabilities, which remain fragile in open-weight, smaller-scale models. This work demonstrates that even with weak verification and refinement capabilities on hard tasks, the reasoning limits of such models can be substantially extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning (DSER). We conceptualize iterative reasoning as a Markov chain, where each step represents a stochastic transition in the solution space. The key insight is that convergence to a correct solution is guaranteed as long as the probability of improvement marginally exceeds that of degradation. By running multiple long-horizon, self-evolving processes in parallel, DSER amplifies these small positive tendencies, enabling the model to asymptotically approach correct answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously unsolvable problems and boosts overall performance, enabling this compact model to surpass the single-turn accuracy of its 600B-parameter teacher through majority voting. Beyond its immediate utility for test-time scaling, the DSER framework serves to diagnose the fundamental limitations of current open-weight reasoners. By clearly delineating their shortcomings in self-verification, refinement, and stability, our findings establish a clear research agenda for developing next-generation models with powerful, intrinsic self-evolving capabilities.", "authors": ["Zihan Liu", "Shun Zheng", "Xumeng Wen", "Yang Wang", "Jiang Bian", "Mao Yang"], "published": "2025-10-20T12:51:42Z", "updated": "2025-10-20T12:51:42Z", "link_pdf": "http://arxiv.org/pdf/2510.17498v1", "link_page": "http://arxiv.org/abs/2510.17498v1"}
{"id": "2510.17496v1", "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and   Mathematical Reasoning in Large Language and Reasoning Models", "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes.", "authors": ["Giacomo Camposampiero", "Michael Hersche", "Roger Wattenhofer", "Abu Sebastian", "Abbas Rahimi"], "published": "2025-10-20T12:51:13Z", "updated": "2025-10-20T12:51:13Z", "link_pdf": "http://arxiv.org/pdf/2510.17496v1", "link_page": "http://arxiv.org/abs/2510.17496v1"}
{"id": "2510.17491v1", "title": "Empowering Real-World: A Survey on the Technology, Practice, and   Evaluation of LLM-driven Industry Agents", "summary": "With the rise of large language models (LLMs), LLM agents capable of autonomous reasoning, planning, and executing complex tasks have become a frontier in artificial intelligence. However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge. To address this, this paper systematically reviews the technologies, applications, and evaluation methods of industry agents based on LLMs. Using an industry agent capability maturity framework, it outlines the evolution of agents in industry applications, from \"process execution systems\" to \"adaptive social systems.\" First, we examine the three key technological pillars that support the advancement of agent capabilities: Memory, Planning, and Tool Use. We discuss how these technologies evolve from supporting simple tasks in their early forms to enabling complex autonomous systems and collective intelligence in more advanced forms. Then, we provide an overview of the application of industry agents in real-world domains such as digital engineering, scientific discovery, embodied intelligence, collaborative business execution, and complex system simulation. Additionally, this paper reviews the evaluation benchmarks and methods for both fundamental and specialized capabilities, identifying the challenges existing evaluation systems face regarding authenticity, safety, and industry specificity. Finally, we focus on the practical challenges faced by industry agents, exploring their capability boundaries, developmental potential, and governance issues in various scenarios, while providing insights into future directions. By combining technological evolution with industry practices, this review aims to clarify the current state and offer a clear roadmap and theoretical foundation for understanding and building the next generation of industry agents.", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "published": "2025-10-20T12:46:55Z", "updated": "2025-10-20T12:46:55Z", "link_pdf": "http://arxiv.org/pdf/2510.17491v1", "link_page": "http://arxiv.org/abs/2510.17491v1"}
{"id": "2510.17483v1", "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts", "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach to scale Large Language Models (LLMs). MoE boosts the efficiency by activating a subset of experts per token. Recent works show that fine-grained experts substantially enriches the combinatorial flexibility of active experts and enhances model expressiveness. However, such a design is fundamentally limited by the layer-local routing mechanism: each layer is restricted to its own expert pool. This requires a careful trade-off between expert dimensionality and routing diversity given fixed parameter budgets. We describe ReXMoE, a novel MoE architecture that improves routing beyond the existing layer-local approaches by allowing routers to reuse experts across adjacent layers. ReXMoE decouples expert dimensionality from per-layer budgets, enabling richer expert combinations without sacrificing individual expert capacity or inflating overall parameters. To this end, we propose a new progressive scaling routing (PSR) strategy to gradually increase the candidate expert pool during training. As a result, ReXMoE improves both language modeling and downstream task performance. Extensive experiments on models ranging from 0.5B to 7B parameters across different architectures demonstrate that ReXMoE consistently improves performance under fixed architectural dimensions, confirming ReXMoE as new design paradigm for parameter-efficient and scalable MoE-based LLMs.", "authors": ["Zheyue Tan", "Zhiyuan Li", "Tao Yuan", "Dong Zhou", "Weilin Liu", "Yueqing Zhuang", "Yadong Li", "Guowei Niu", "Cheng Qin", "Zhuyu Yao", "Congyi Liu", "Haiyang Xu", "Boxun Li", "Guohao Dai", "Bo Zhao", "Yu Wang"], "published": "2025-10-20T12:27:55Z", "updated": "2025-10-20T12:27:55Z", "link_pdf": "http://arxiv.org/pdf/2510.17483v1", "link_page": "http://arxiv.org/abs/2510.17483v1"}
{"id": "2510.17476v1", "title": "Disparities in Multilingual LLM-Based Healthcare Q&A", "summary": "Equitable access to reliable health information is vital when integrating AI into healthcare. Yet, information quality varies across languages, raising concerns about the reliability and consistency of multilingual Large Language Models (LLMs). We systematically examine cross-lingual disparities in pre-training source and factuality alignment in LLM answers for multilingual healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and Italian. We (i) constructed Multilingual Wiki Health Care (MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed cross-lingual healthcare coverage; (iii) assessed LLM response alignment with these references; and (iv) conducted a case study on factual alignment through the use of contextual information and Retrieval-Augmented Generation (RAG). Our findings reveal substantial cross-lingual disparities in both Wikipedia coverage and LLM factual alignment. Across LLMs, responses align more with English Wikipedia, even when the prompts are non-English. Providing contextual excerpts from non-English Wikipedia at inference time effectively shifts factual alignment toward culturally relevant knowledge. These results highlight practical pathways for building more equitable, multilingual AI systems for healthcare.", "authors": ["Ipek Baris Schlicht", "Burcu Sayin", "Zhixue Zhao", "Frederik M. Labonté", "Cesare Barbera", "Marco Viviani", "Paolo Rosso", "Lucie Flek"], "published": "2025-10-20T12:19:08Z", "updated": "2025-10-20T12:19:08Z", "link_pdf": "http://arxiv.org/pdf/2510.17476v1", "link_page": "http://arxiv.org/abs/2510.17476v1"}
{"id": "2510.17472v1", "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time   Training for Reliable Reasoning in LLMs", "summary": "Recent advances such as self-consistency and test-time reinforcement learning (TTRL) improve the reliability of large language models (LLMs) without additional supervision, yet their underlying mechanisms and statistical guarantees remain poorly understood. We present a unified framework for certifiable inference in LLMs, showing that majority voting provides a statistical certificate of self-consistency: under mild assumptions, the aggregated answer coincides with the mode of the model's terminal distribution with high probability. We derive finite-sample and anytime-valid concentration bounds that quantify this confidence, and introduce the Martingale Majority Certificate (MMC), a sequential stopping rule that adaptively determines when sufficient samples have been drawn. We further prove that label-free post-training methods such as TTRL implicitly sharpen the answer distribution by exponentially tilting it toward its mode, thereby reducing the number of samples required for certification. Building on this insight, we propose new post-training objectives that explicitly optimise this trade-off between sharpness and bias. Together, these results explain and connect two central test-time scaling strategies, self-consistency and TTRL, within a single statistical framework for label-free, certifiable reliability in reasoning LLMs.", "authors": ["Paula Cordero-Encinar", "Andrew B. Duncan"], "published": "2025-10-20T12:14:12Z", "updated": "2025-10-20T12:14:12Z", "link_pdf": "http://arxiv.org/pdf/2510.17472v1", "link_page": "http://arxiv.org/abs/2510.17472v1"}
{"id": "2510.17460v1", "title": "Evaluating Large Language Models on Urdu Idiom Translation", "summary": "Idiomatic translation remains a significant challenge in machine translation, especially for low resource languages such as Urdu, and has received limited prior attention. To advance research in this area, we introduce the first evaluation datasets for Urdu to English idiomatic translation, covering both Native Urdu and Roman Urdu scripts and annotated with gold-standard English equivalents. We evaluate multiple open-source Large Language Models (LLMs) and Neural Machine Translation (NMT) systems on this task, focusing on their ability to preserve idiomatic and cultural meaning. Automatic metrics including BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our findings indicate that prompt engineering enhances idiomatic translation compared to direct translation, though performance differences among prompt types are relatively minor. Moreover, cross script comparisons reveal that text representation substantially affects translation quality, with Native Urdu inputs producing more accurate idiomatic translations than Roman Urdu.", "authors": ["Muhammad Farmal Khan", "Mousumi Akter"], "published": "2025-10-20T11:49:26Z", "updated": "2025-10-20T11:49:26Z", "link_pdf": "http://arxiv.org/pdf/2510.17460v1", "link_page": "http://arxiv.org/abs/2510.17460v1"}
{"id": "2510.17431v1", "title": "Agentic Reinforcement Learning for Search is Unsafe", "summary": "Agentic reinforcement learning (RL) trains large language models to autonomously call tools during reasoning, with search as the most common application. These models excel at multi-step reasoning tasks, but their safety properties are not well understood. In this study, we show that RL-trained search models inherit refusal from instruction tuning and often deflect harmful requests by turning them into safe queries. However, this safety is fragile. Two simple attacks, one that forces the model to begin response with search (Search attack), another that encourages models to repeatedly search (Multi-search attack), trigger cascades of harmful searches and answers. Across two model families (Qwen, Llama) with both local and web search, these attacks lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query safety by 82.4%. The attacks succeed by triggering models to generate harmful, request-mirroring search queries before they can generate the inherited refusal tokens. This exposes a core weakness of current RL training: it rewards continued generation of effective queries without accounting for their harmfulness. As a result, RL search models have vulnerabilities that users can easily exploit, making it urgent to develop safety-aware agentic RL pipelines optimising for safe search.", "authors": ["Yushi Yang", "Shreyansh Padarha", "Andrew Lee", "Adam Mahdi"], "published": "2025-10-20T11:19:37Z", "updated": "2025-10-20T11:19:37Z", "link_pdf": "http://arxiv.org/pdf/2510.17431v1", "link_page": "http://arxiv.org/abs/2510.17431v1"}
{"id": "2510.17415v1", "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional   Chinese Medicine", "summary": "Traditional Chinese Medicine (TCM), with a history spanning over two millennia, plays a role in global healthcare. However, applying large language models (LLMs) to TCM remains challenging due to its reliance on holistic reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain LLMs have made progress in text-based understanding but lack multimodal integration, interpretability, and clinical applicability. To address these limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM, integrating structured knowledge bases, diagnostic data, and expert feedback refinement. BenCao was trained through natural language instruction tuning rather than parameter retraining, aligning with expert-level reasoning and ethical norms specific to TCM. The system incorporates a comprehensive knowledge base of over 1,000 classical and modern texts, a scenario-based instruction framework for diverse interactions, a chain-of-thought simulation mechanism for interpretable reasoning, and a feedback refinement process involving licensed TCM practitioners. BenCao connects to external APIs for tongue-image classification and multimodal database retrieval, enabling dynamic access to diagnostic resources. In evaluations across single-choice question benchmarks and multimodal classification tasks, BenCao achieved superior accuracy to general-domain and TCM-domain models, particularly in diagnostics, herb recognition, and constitution classification. The model was deployed as an interactive application on the OpenAI GPTs Store, accessed by nearly 1,000 users globally as of October 2025. This study demonstrates the feasibility of developing a TCM-domain LLM through natural language-based instruction tuning and multimodal integration, offering a practical framework for aligning generative AI with traditional medical reasoning and a scalable pathway for real-world deployment.", "authors": ["Jiacheng Xie", "Yang Yu", "Yibo Chen", "Hanyao Zhang", "Lening Zhao", "Jiaxuan He", "Lei Jiang", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "published": "2025-10-20T10:57:37Z", "updated": "2025-10-20T10:57:37Z", "link_pdf": "http://arxiv.org/pdf/2510.17415v1", "link_page": "http://arxiv.org/abs/2510.17415v1"}
{"id": "2510.17402v1", "title": "Leveraging Group Relative Policy Optimization to Advance Large Language   Models in Traditional Chinese Medicine", "summary": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique knowledge system that challenges conventional applications of large language models (LLMs). Although previous TCM-specific LLMs have shown progress through supervised fine-tuning, they often face limitations in alignment, data quality, and evaluation consistency. In this study, we introduce Ladder-base, the first TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a reinforcement learning method that improves reasoning and factual consistency by optimizing response selection based on intra-group comparisons. Ladder-base is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data for training and the remaining 20 percent split evenly between validation and test sets. Through standardized evaluation, Ladder-base demonstrates superior performance across multiple reasoning metrics when compared to both state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and Zhongjing. These findings suggest that GRPO provides an effective and efficient strategy for aligning LLMs with expert-level reasoning in traditional medical domains and supports the development of trustworthy and clinically grounded TCM artificial intelligence systems.", "authors": ["Jiacheng Xie", "Shuai Zeng", "Yang Yu", "Xiaoting Tang", "Guanghui An", "Dong Xu"], "published": "2025-10-20T10:43:33Z", "updated": "2025-10-20T10:43:33Z", "link_pdf": "http://arxiv.org/pdf/2510.17402v1", "link_page": "http://arxiv.org/abs/2510.17402v1"}
{"id": "2510.17389v1", "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level   Adaptability in LLMs", "summary": "Large language models (LLMs) are transforming education by answering questions, explaining complex concepts, and generating content across a wide range of subjects. Despite strong performance on academic benchmarks, they often fail to tailor responses to students' grade levels. This is a critical need in K-12 education, where age-appropriate vocabulary and explanation are essential for effective learning. Existing models frequently produce outputs that are too advanced or vague for younger learners, and there are no standardized benchmarks to evaluate their ability to adjust across cognitive and developmental stages. To address this gap, we introduce EduAdapt, a benchmark of nearly 48k grade-labeled QA pairs across nine science subjects, spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse set of open-source LLMs on EduAdapt and find that while larger models generally perform better, they still struggle with generating suitable responses for early-grade students (Grades 1-5). Our work presents the first dataset and evaluation framework for assessing grade-level adaptability in LLMs, aiming to foster more developmentally aligned educational AI systems through better training and prompting strategies. EduAdapt code and datasets are publicly available at https://github.com/NaumanNaeem/EduAdapt.", "authors": ["Numaan Naeem", "Abdellah El Mekki", "Muhammad Abdul-Mageed"], "published": "2025-10-20T10:30:40Z", "updated": "2025-10-20T10:30:40Z", "link_pdf": "http://arxiv.org/pdf/2510.17389v1", "link_page": "http://arxiv.org/abs/2510.17389v1"}
{"id": "2510.17388v1", "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple,   Self-Contained Directives", "summary": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot reasoning, yet their ability to execute simple, self-contained instructions remains underexplored, despite this being foundational to complex instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro benchmarks, by systematically varying the format of option labels (alphabetic, numeric, Roman) while keeping their meaning identical under four paradigms, namely: (1) With explicit instructions, label changes cause large performance shifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format bias. (2) Without instructions, performance drops further (up to -10.84\\%) and label sensitivity intensifies, underscoring the role of explicit guidance. (3) When option contents are removed, models fail random-choice baselines except with numeric labels, suggesting weak adherence to atomic directives. (4) Three-shot exemplars yield no significant gains in robustness or fidelity, and generation analyses show persistent label errors, especially for non-numeric formats. Across model sizes, larger LLMs achieve higher accuracy but remain inconsistent in instruction adherence. These results expose the insufficiencies of current instruction-tuning paradigms and highlight the need for evaluation methods and training strategies that explicitly target atomic instruction-following.", "authors": ["Henry Lim", "Kwan Hui Lim"], "published": "2025-10-20T10:26:26Z", "updated": "2025-10-20T10:26:26Z", "link_pdf": "http://arxiv.org/pdf/2510.17388v1", "link_page": "http://arxiv.org/abs/2510.17388v1"}
{"id": "2510.17385v1", "title": "TabR1: Taming GRPO for tabular reasoning LLMs", "summary": "Tabular prediction has traditionally relied on gradient-boosted decision trees and specialized deep learning models, which excel within tasks but provide limited interpretability and weak transfer across tables. Reasoning large language models (LLMs) promise cross-task adaptability with trans- parent reasoning traces, yet their potential has not been fully realized for tabular data. This paper presents TabR1, the first reasoning LLM for tabular prediction with multi-step reasoning. At its core is Permutation Relative Policy Optimization (PRPO), a simple yet efficient reinforcement learning method that encodes column-permutation invariance as a structural prior. By construct- ing multiple label-preserving permutations per sample and estimating advantages both within and across permutations, PRPO transforms sparse rewards into dense learning signals and improves generalization. With limited supervision, PRPO activates the reasoning ability of LLMs for tabular prediction, enhancing few-shot and zero-shot performance as well as interpretability. Comprehensive experiments demonstrate that TabR1 achieves performance comparable to strong baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1 approaches the performance of strong baselines under the 32-shot setting. Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).", "authors": ["Pengxiang Cai", "Zihao Gao", "Jintai Chen"], "published": "2025-10-20T10:22:01Z", "updated": "2025-10-20T10:22:01Z", "link_pdf": "http://arxiv.org/pdf/2510.17385v1", "link_page": "http://arxiv.org/abs/2510.17385v1"}
{"id": "2510.17364v1", "title": "Recurrent Attention-based Token Selection for Efficient Streaming   Video-LLMs", "summary": "Video Large Language Models (Video-LLMs) excel at understanding videos in-context, provided they have full access to the video when answering queries. However, these models face challenges in streaming scenarios where hour-long videos must be processed online, and questions need timely responses. In this work, we propose a training-free approach compatible with standard Video-LLMs, leveraging three key concepts: 1) LLM-informed selection of visual tokens to identify those that the LLM has attended to and contributed to its understanding of each short clip. Our attention-based selection allows us to discard up to ~95% of unimportant visual tokens with minimal performance loss; 2) Recurrent processing of past selected tokens to generate temporally coherent understanding of each processed clip; 3) Caption-based question answering for lightweight and accurate responses. Our method achieves state-of-the-art performance on streaming video benchmarks, striking a balance between efficiency and effectiveness.", "authors": ["Vaggelis Dorovatas", "Soroush Seifi", "Gunshi Gupta", "Rahaf Aljundi"], "published": "2025-10-20T10:04:49Z", "updated": "2025-10-20T10:04:49Z", "link_pdf": "http://arxiv.org/pdf/2510.17364v1", "link_page": "http://arxiv.org/abs/2510.17364v1"}
{"id": "2510.17358v1", "title": "Localist LLMs with Recruitment Learning", "summary": "We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovations are (1) a locality dial, a tunable parameter that dynamically controls the degree of localization during both training and inference without requiring model retraining, (2) an information-theoretic recruitment mechanism that adaptively allocates semantic blocks as needed, eliminating the requirement for complete domain knowledge at initialization, and (3) a hierarchical recruitment framework that extends capacity allocation to entire specialized LLMs, enabling multi-granularity architectural adaptation. This is achieved through group sparsity penalties on attention mechanisms, information-theoretic anchor design, dynamic rule injection, and principled recruitment criteria based on penalized likelihood with explicit units. We provide rigorous mathematical results establishing explicit threshold conditions under which attention provably concentrates on semantically relevant blocks at stationary points, with exact bounds on attention entropy and pointer fidelity. The hierarchical recruitment mechanism provides convergence guarantees at both the block level (fine-grained, within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the system discovers semantic partitions that balance model complexity against data encoding efficiency. This framework enables practitioners to continuously interpolate between interpretable and high-performance modes while adapting architectural capacity at multiple granularities, supporting applications in regulated domains requiring both transparency and capability.", "authors": ["Joachim Diederich"], "published": "2025-10-20T09:58:34Z", "updated": "2025-10-20T09:58:34Z", "link_pdf": "http://arxiv.org/pdf/2510.17358v1", "link_page": "http://arxiv.org/abs/2510.17358v1"}
{"id": "2510.17354v1", "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented   Generation", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) by retrieving relevant documents from an external corpus. However, existing RAG systems primarily focus on unimodal text documents, and often fall short in real-world scenarios where both queries and documents may contain mixed modalities (such as text and images). In this paper, we address the challenge of Universal Retrieval-Augmented Generation (URAG), which involves retrieving and reasoning over mixed-modal information to improve vision-language generation. To this end, we propose Nyx, a unified mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate the scarcity of realistic mixed-modal data, we introduce a four-stage automated pipeline for generation and filtering, leveraging web documents to construct NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that better reflect real-world information needs. Building on this high-quality dataset, we adopt a two-stage training framework for Nyx: we first perform pre-training on NyxQA along with a variety of open-source retrieval datasets, followed by supervised fine-tuning using feedback from downstream vision-language models (VLMs) to align retrieval outputs with generative preferences. Experimental results demonstrate that Nyx not only performs competitively on standard text-only RAG benchmarks, but also excels in the more general and realistic URAG setting, significantly improving generation quality in vision-language tasks.", "authors": ["Chenghao Zhang", "Guanting Dong", "Xinyu Yang", "Zhicheng Dou"], "published": "2025-10-20T09:56:43Z", "updated": "2025-10-20T09:56:43Z", "link_pdf": "http://arxiv.org/pdf/2510.17354v1", "link_page": "http://arxiv.org/abs/2510.17354v1"}
{"id": "2510.17332v1", "title": "iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA", "summary": "Image Quality Assessment (IQA) has progressed from scalar quality prediction to more interpretable, human-aligned evaluation paradigms. In this work, we address the emerging challenge of detailed and explainable IQA by proposing iDETEX-a unified multimodal large language model (MLLM) capable of simultaneously performing three key tasks: quality grounding, perception, and description. To facilitate efficient and generalizable training across these heterogeneous subtasks, we design a suite of task-specific offline augmentation modules and a data mixing strategy. These are further complemented by online enhancement strategies to fully exploit multi-sourced supervision. We validate our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves state-of-the-art performance across all subtasks. Our model ranks first in the ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its effectiveness and robustness in delivering accurate and interpretable quality assessments.", "authors": ["Zhaoran Zhao", "Xinli Yue", "Jianhui Sun", "Yuhao Xie", "Tao Shao", "Liangchao Yao", "Fan Xia", "Yuetang Deng"], "published": "2025-10-20T09:26:12Z", "updated": "2025-10-20T09:26:12Z", "link_pdf": "http://arxiv.org/pdf/2510.17332v1", "link_page": "http://arxiv.org/abs/2510.17332v1"}
{"id": "2510.17314v1", "title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward   Modeling", "summary": "Reward models are essential for aligning Large Language Models (LLMs) with human values, yet their development is hampered by costly preference datasets and poor interpretability. While recent rubric-based approaches offer transparency, they often lack systematic quality control and optimization, creating a trade-off between scalability and reliability. We address these limitations with a novel, training-free framework built on a key assumption: \\textit{evaluation rubrics underlying human preferences exhibit significant generalization ability across diverse queries}, a property that enables remarkable data efficiency. Our two-stage approach first infers high-quality, query-specific rubrics using a validation-guided \\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these granular rubrics into a compact, non-redundant core set by maximizing an \\textbf{information-theoretic coding rate}. The final output is an interpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments demonstrate the framework's exceptional data efficiency and performance. Critically, using just 70 preference pairs (1.5\\% of the source data), our method also empowers smaller models like Qwen3-8B to outperform specialized, fully-trained counterparts. This work pioneers a scalable, interpretable, and data-efficient path for reward modeling.", "authors": ["Lipeng Xie", "Sen Huang", "Zhuo Zhang", "Anni Zou", "Yunpeng Zhai", "Dingchao Ren", "Kezun Zhang", "Haoyuan Hu", "Boyin Liu", "Haoran Chen", "Zhaoyang Liu", "Bolin Ding"], "published": "2025-10-20T09:01:37Z", "updated": "2025-10-20T09:01:37Z", "link_pdf": "http://arxiv.org/pdf/2510.17314v1", "link_page": "http://arxiv.org/abs/2510.17314v1"}
{"id": "2510.17313v1", "title": "Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation   Framework for Multi-Factor Sequential Representations", "summary": "Learning disentangled representations in sequential data is a key goal in deep learning, with broad applications in vision, audio, and time series. While real-world data involves multiple interacting semantic factors over time, prior work has mostly focused on simpler two-factor static and dynamic settings, primarily because such settings make data collection easier, thereby overlooking the inherently multi-factor nature of real-world data. We introduce the first standardized benchmark for evaluating multi-factor sequential disentanglement across six diverse datasets spanning video, audio, and time series. Our benchmark includes modular tools for dataset integration, model development, and evaluation metrics tailored to multi-factor analysis. We additionally propose a post-hoc Latent Exploration Stage to automatically align latent dimensions with semantic factors, and introduce a Koopman-inspired model that achieves state-of-the-art results. Moreover, we show that Vision-Language Models can automate dataset annotation and serve as zero-shot disentanglement evaluators, removing the need for manual labels and human intervention. Together, these contributions provide a robust and scalable foundation for advancing multi-factor sequential disentanglement.", "authors": ["Tal Barami", "Nimrod Berman", "Ilan Naiman", "Amos H. Hason", "Rotem Ezra", "Omri Azencot"], "published": "2025-10-20T08:58:23Z", "updated": "2025-10-20T08:58:23Z", "link_pdf": "http://arxiv.org/pdf/2510.17313v1", "link_page": "http://arxiv.org/abs/2510.17313v1"}
{"id": "2510.17309v1", "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment", "summary": "The evaluation of academic theses is a cornerstone of higher education, ensuring rigor and integrity. Traditional methods, though effective, are time-consuming and subject to evaluator variability. This paper presents RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from proposal to final submission. Using advanced natural language processing techniques, including large language models, retrieval-augmented generation, and structured chain-of-thought prompting, RubiSCoT offers a consistent, scalable solution. The framework includes preliminary assessments, multidimensional assessments, content extraction, rubric-based scoring, and detailed reporting. We present the design and implementation of RubiSCoT, discussing its potential to optimize academic assessment processes through consistent, scalable, and transparent evaluation.", "authors": ["Thorsten Fröhlich", "Tim Schlippe"], "published": "2025-10-20T08:52:33Z", "updated": "2025-10-20T08:52:33Z", "link_pdf": "http://arxiv.org/pdf/2510.17309v1", "link_page": "http://arxiv.org/abs/2510.17309v1"}
{"id": "2510.17301v1", "title": "Comprehending Spatio-temporal Data via Cinematic Storytelling using   Large Language Models", "summary": "Spatio-temporal data captures complex dynamics across both space and time, yet traditional visualizations are complex, require domain expertise and often fail to resonate with broader audiences. Here, we propose MapMuse, a storytelling-based framework for interpreting spatio-temporal datasets, transforming them into compelling, narrative-driven experiences. We utilize large language models and employ retrieval augmented generation (RAG) and agent-based techniques to generate comprehensive stories. Drawing on principles common in cinematic storytelling, we emphasize clarity, emotional connection, and audience-centric design. As a case study, we analyze a dataset of taxi trajectories. Two perspectives are presented: a captivating story based on a heat map that visualizes millions of taxi trip endpoints to uncover urban mobility patterns; and a detailed narrative following a single long taxi journey, enriched with city landmarks and temporal shifts. By portraying locations as characters and movement as plot, we argue that data storytelling drives insight, engagement, and action from spatio-temporal information. The case study illustrates how MapMuse can bridge the gap between data complexity and human understanding. The aim of this short paper is to provide a glimpse to the potential of the cinematic storytelling technique as an effective communication tool for spatio-temporal data, as well as to describe open problems and opportunities for future research.", "authors": ["Panos Kalnis. Shuo Shang", "Christian S. Jensen"], "published": "2025-10-20T08:44:25Z", "updated": "2025-10-20T08:44:25Z", "link_pdf": "http://dx.doi.org/10.1145/3748777.3748787", "link_page": "http://arxiv.org/abs/2510.17301v1"}
{"id": "2510.17925v1", "title": "SpecAgent: A Speculative Retrieval and Forecasting Agent for Code   Completion", "summary": "Large Language Models (LLMs) excel at code-related tasks but often struggle in realistic software repositories, where project-specific APIs and cross-file dependencies are crucial. Retrieval-augmented methods mitigate this by injecting repository context at inference time. The low inference-time latency budget affects either retrieval quality or the added latency adversely impacts user experience. We address this limitation with SpecAgent, an agent that improves both latency and code-generation quality by proactively exploring repository files during indexing and constructing speculative context that anticipates future edits in each file. This indexing-time asynchrony allows thorough context computation, masking latency, and the speculative nature of the context improves code-generation quality. Additionally, we identify the problem of future context leakage in existing benchmarks, which can inflate reported performance. To address this, we construct a synthetic, leakage-free benchmark that enables a more realistic evaluation of our agent against baselines. Experiments show that SpecAgent consistently achieves absolute gains of 9-11% (48-58% relative) compared to the best-performing baselines, while significantly reducing inference latency.", "authors": ["George Ma", "Anurag Koul", "Qi Chen", "Yawen Wu", "Sachit Kuhar", "Yu Yu", "Aritra Sengupta", "Varun Kumar", "Murali Krishna Ramanathan"], "published": "2025-10-20T08:04:51Z", "updated": "2025-10-20T08:04:51Z", "link_pdf": "http://arxiv.org/pdf/2510.17925v1", "link_page": "http://arxiv.org/abs/2510.17925v1"}
{"id": "2510.17924v1", "title": "Efficient Toxicity Detection in Gaming Chats: A Comparative Study of   Embeddings, Fine-Tuned Transformers and LLMs", "summary": "This paper presents a comprehensive comparative analysis of Natural Language Processing (NLP) methods for automated toxicity detection in online gaming chats. Traditional machine learning models with embeddings, large language models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer models, and retrieval-augmented generation (RAG) approaches are evaluated. The evaluation framework assesses three critical dimensions: classification accuracy, processing speed, and computational costs. A hybrid moderation system architecture is proposed that optimizes human moderator workload through automated detection and incorporates continuous learning mechanisms. The experimental results demonstrate significant performance variations across methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs. The findings provide empirical evidence for deploying cost-effective, efficient content moderation systems in dynamic online gaming environments.", "authors": ["Yehor Tereshchenko", "Mika Hämäläinen"], "published": "2025-10-20T08:03:28Z", "updated": "2025-10-20T08:03:28Z", "link_pdf": "http://dx.doi.org/10.46298/jdmdh.16280", "link_page": "http://arxiv.org/abs/2510.17924v1"}
{"id": "2510.17274v1", "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language   Models", "summary": "Current autonomous driving systems rely on specialized models for perceiving and predicting motion, which demonstrate reliable performance in standard conditions. However, generalizing cost-effectively to diverse real-world scenarios remains a significant challenge. To address this, we propose Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion forecasting models with multimodal large language models (MLLMs). PnF builds on the insight that natural language provides a more effective way to describe and handle complex scenarios, enabling quick adaptation to targeted behaviors. We design prompts to extract structured scene understanding from MLLMs and distill this information into learnable embeddings to augment existing behavior prediction models. Our method leverages the zero-shot reasoning capabilities of MLLMs to achieve significant improvements in motion prediction performance, while requiring no fine-tuning -- making it practical to adopt. We validate our approach on two state-of-the-art motion forecasting models using the Waymo Open Motion Dataset and the nuScenes Dataset, demonstrating consistent performance improvements across both benchmarks.", "authors": ["Katie Luo", "Jingwei Ji", "Tong He", "Runsheng Xu", "Yichen Xie", "Dragomir Anguelov", "Mingxing Tan"], "published": "2025-10-20T08:01:29Z", "updated": "2025-10-20T08:01:29Z", "link_pdf": "http://arxiv.org/pdf/2510.17274v1", "link_page": "http://arxiv.org/abs/2510.17274v1"}
{"id": "2510.17269v1", "title": "FineVision: Open Data Is All You Need", "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented landscape of inconsistent and contaminated public datasets. We introduce FineVision, a meticulously collected, curated, and unified corpus of 24 million samples - the largest open resource of its kind. We unify more than 200 sources into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation performs bulk ingestion and schema mapping, while reviewers audit mappings and spot-check outputs to verify faithful consumption of annotations, appropriate formatting and diversity, and safety; issues trigger targeted fixes and re-runs. The workflow further applies rigorous de-duplication within and across sources and decontamination against 66 public benchmarks. FineVision also encompasses agentic/GUI tasks with a unified action space; reviewers validate schemas and inspect a sample of trajectories to confirm executable fidelity. Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, underscoring the benefits of scale, data hygiene, and balanced automation with human oversight. We release the corpus and curation tools to accelerate data-centric VLM research.", "authors": ["Luis Wiedmann", "Orr Zohar", "Amir Mahla", "Xiaohan Wang", "Rui Li", "Thibaud Frere", "Leandro von Werra", "Aritra Roy Gosthipaty", "Andrés Marafioti"], "published": "2025-10-20T07:54:46Z", "updated": "2025-10-20T07:54:46Z", "link_pdf": "http://arxiv.org/pdf/2510.17269v1", "link_page": "http://arxiv.org/abs/2510.17269v1"}
{"id": "2510.17923v1", "title": "Rewarding the Journey, Not Just the Destination: A Composite Path and   Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning", "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.", "authors": ["Chenwei Tang", "Jingyu Xing", "Xinyu Liu", "Wei Ju", "Jiancheng Lv", "Deng Xiong", "Ziyue Qiao"], "published": "2025-10-20T07:53:51Z", "updated": "2025-10-20T07:53:51Z", "link_pdf": "http://arxiv.org/pdf/2510.17923v1", "link_page": "http://arxiv.org/abs/2510.17923v1"}
{"id": "2510.17256v1", "title": "Explainability of Large Language Models: Opportunities and Challenges   toward Generating Trustworthy Explanations", "summary": "Large language models have exhibited impressive performance across a broad range of downstream tasks in natural language processing. However, how a language model predicts the next token and generates content is not generally understandable by humans. Furthermore, these models often make errors in prediction and reasoning, known as hallucinations. These errors underscore the urgent need to better understand and interpret the intricate inner workings of language models and how they generate predictive outputs. Motivated by this gap, this paper investigates local explainability and mechanistic interpretability within Transformer-based large language models to foster trust in such models. In this regard, our paper aims to make three key contributions. First, we present a review of local explainability and mechanistic interpretability approaches and insights from relevant studies in the literature. Furthermore, we describe experimental studies on explainability and reasoning with large language models in two critical domains -- healthcare and autonomous driving -- and analyze the trust implications of such explanations for explanation receivers. Finally, we summarize current unaddressed issues in the evolving landscape of LLM explainability and outline the opportunities, critical challenges, and future directions toward generating human-aligned, trustworthy LLM explanations.", "authors": ["Shahin Atakishiyev", "Housam K. B. Babiker", "Jiayi Dai", "Nawshad Farruque", "Teruaki Hayashi", "Nafisa Sadaf Hriti", "Md Abed Rahman", "Iain Smith", "Mi-Young Kim", "Osmar R. Zaïane", "Randy Goebel"], "published": "2025-10-20T07:43:53Z", "updated": "2025-10-20T07:43:53Z", "link_pdf": "http://arxiv.org/pdf/2510.17256v1", "link_page": "http://arxiv.org/abs/2510.17256v1"}
{"id": "2510.17922v1", "title": "Select-Then-Decompose: From Empirical Analysis to Adaptive Selection   Strategy for Task Decomposition in Large Language Models", "summary": "Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achieving notable success in specific domains, but they often overlook the trade-off between performance and cost. In this study, we first conduct a comprehensive investigation on task decomposition, identifying six categorization schemes. Then, we perform an empirical analysis of three factors that influence the performance and cost of task decomposition: categories of approaches, characteristics of tasks, and configuration of decomposition and execution models, uncovering three critical insights and summarizing a set of practical principles. Building on this analysis, we propose the Select-Then-Decompose strategy, which establishes a closed-loop problem-solving process composed of three stages: selection, execution, and verification. This strategy dynamically selects the most suitable decomposition approach based on task characteristics and enhances the reliability of the results through a verification module. Comprehensive evaluations across multiple benchmarks show that the Select-Then-Decompose consistently lies on the Pareto frontier, demonstrating an optimal balance between performance and cost. Our code is publicly available at https://github.com/summervvind/Select-Then-Decompose.", "authors": ["Shuodi Liu", "Yingzhuo Liu", "Zi Wang", "Yusheng Wang", "Huijia Wu", "Liuyu Xiang", "Zhaofeng He"], "published": "2025-10-20T07:28:15Z", "updated": "2025-10-20T07:28:15Z", "link_pdf": "http://arxiv.org/pdf/2510.17922v1", "link_page": "http://arxiv.org/abs/2510.17922v1"}
{"id": "2510.17238v1", "title": "StreamingThinker: Large Language Models Can Think While Reading", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at \\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this repository.}", "authors": ["Junlong Tong", "Yingqi Fan", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "published": "2025-10-20T07:27:37Z", "updated": "2025-10-20T07:27:37Z", "link_pdf": "http://arxiv.org/pdf/2510.17238v1", "link_page": "http://arxiv.org/abs/2510.17238v1"}
{"id": "2510.17235v1", "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency   Investment Analysis", "summary": "The cryptocurrency market offers significant investment opportunities but faces challenges including high volatility and fragmented information. Data integration and analysis are essential for informed investment decisions. Currently, investors use three main approaches: (1) Manual analysis across various sources, which depends heavily on individual experience and is time-consuming and prone to bias; (2) Data aggregation platforms-limited in functionality and depth of analysis; (3) Large language model agents-based on static pretrained models, lacking real-time data integration and multi-step reasoning capabilities. To address these limitations, we present Coinvisor, a reinforcement learning-based chatbot that provides comprehensive analytical support for cryptocurrency investment through a multi-agent framework. Coinvisor integrates diverse analytical capabilities through specialized tools. Its key innovation is a reinforcement learning-based tool selection mechanism that enables multi-step planning and flexible integration of diverse data sources. This design supports real-time interaction and adaptive analysis of dynamic content, delivering accurate and actionable investment insights. We evaluated Coinvisor through automated benchmarks on tool calling accuracy and user studies with 20 cryptocurrency investors using our interface. Results show that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base model in tool orchestration. User studies show high satisfaction (4.64/5), with participants preferring Coinvisor to both general LLMs and existing crypto platforms (4.62/5).", "authors": ["Chong Chen", "Ze Liu", "Lingfeng Bao", "Yanlin Wang", "Ting Chen", "Daoyuan Wu", "Jiachi Chen"], "published": "2025-10-20T07:23:49Z", "updated": "2025-10-20T07:23:49Z", "link_pdf": "http://arxiv.org/pdf/2510.17235v1", "link_page": "http://arxiv.org/abs/2510.17235v1"}
{"id": "2510.17921v1", "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention   Window of Sections", "summary": "Recent advances in enhancing the reasoning ability of large language models (LLMs) have been remarkably successful. LLMs trained with reinforcement learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and coding, even with relatively small model sizes. However, despite these improvements in task accuracy, the assessment of creativity in LLM generations has been largely overlooked in reasoning tasks, in contrast to writing tasks. The lack of research on creativity assessment in reasoning primarily stems from two challenges: (1) the difficulty of defining the range of creativity, and (2) the necessity of human evaluation in the assessment process. To address these challenges, we propose CLAWS, a method that defines and classifies mathematical solutions into typical, creative, and hallucinated categories without human evaluation, by leveraging attention weights across prompt sections and output. CLAWS outperforms five existing white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen, Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems collected from 181 math contests (AJHSME, AMC, AIME).", "authors": ["Keuntae Kim", "Eunhye Jeong", "Sehyeon Lee", "Seohee Yoon", "Yong Suk Choi"], "published": "2025-10-20T06:59:37Z", "updated": "2025-10-20T06:59:37Z", "link_pdf": "http://arxiv.org/pdf/2510.17921v1", "link_page": "http://arxiv.org/abs/2510.17921v1"}
{"id": "2510.17210v1", "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning   via Attention Shifting", "summary": "The increase in computing power and the necessity of AI-assisted decision-making boost the growing application of large language models (LLMs). Along with this, the potential retention of sensitive data of LLMs has spurred increasing research into machine unlearning. However, existing unlearning approaches face a critical dilemma: Aggressive unlearning compromises model utility, while conservative strategies preserve utility but risk hallucinated responses. This significantly limits LLMs' reliability in knowledge-intensive applications. To address this, we introduce a novel Attention-Shifting (AS) framework for selective unlearning. AS is driven by two design objectives: (1) context-preserving suppression that attenuates attention to fact-bearing tokens without disrupting LLMs' linguistic structure; and (2) hallucination-resistant response shaping that discourages fabricated completions when queried about unlearning content. AS realizes these objectives through two attention-level interventions, which are importance-aware suppression applied to the unlearning set to reduce reliance on memorized knowledge and attention-guided retention enhancement that reinforces attention toward semantically essential tokens in the retained dataset to mitigate unintended degradation. These two components are jointly optimized via a dual-loss objective, which forms a soft boundary that localizes unlearning while preserving unrelated knowledge under representation superposition. Experimental results show that AS improves performance preservation over the state-of-the-art unlearning methods, achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC benchmark, while maintaining competitive hallucination-free unlearning effectiveness. Compared to existing methods, AS demonstrates a superior balance between unlearning effectiveness, generalization, and response reliability.", "authors": ["Chenchen Tan", "Youyang Qu", "Xinghao Li", "Hui Zhang", "Shujie Cui", "Cunjian Chen", "Longxiang Gao"], "published": "2025-10-20T06:50:03Z", "updated": "2025-10-20T06:50:03Z", "link_pdf": "http://arxiv.org/pdf/2510.17210v1", "link_page": "http://arxiv.org/abs/2510.17210v1"}
{"id": "2510.17205v1", "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal   Dynamics for Efficient Multimodal LLMs", "summary": "Multimodal Large Language Models (MLLMs) have achieved strong performance across vision-language tasks, but suffer from significant computational overhead due to the quadratic growth of attention computations with the number of multimodal tokens. Though efforts have been made to prune tokens in MLLMs, \\textit{they lack a fundamental understanding of how MLLMs process and fuse multimodal information.} Through systematic analysis, we uncover a \\textbf{three-stage} cross-modal interaction process: (1) Shallow layers recognize task intent, with visual tokens acting as passive attention sinks; (2) Cross-modal fusion occurs abruptly in middle layers, driven by a few critical visual tokens; (3) Deep layers discard vision tokens, focusing solely on linguistic refinement. Based on these findings, we propose \\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of vision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It significantly outperforms existing token pruning methods and generalizes across diverse MLLMs. Beyond pruning, our insights further provide actionable guidelines for training efficient MLLMs by aligning model architecture with its intrinsic layer-wise processing dynamics. Our code is available at: https://github.com/EIT-NLP/VisiPruner.", "authors": ["Yingqi Fan", "Anhao Zhao", "Jinlan Fu", "Junlong Tong", "Hui Su", "Yijie Pan", "Wei Zhang", "Xiaoyu Shen"], "published": "2025-10-20T06:40:17Z", "updated": "2025-10-20T06:40:17Z", "link_pdf": "http://arxiv.org/pdf/2510.17205v1", "link_page": "http://arxiv.org/abs/2510.17205v1"}
{"id": "2510.17197v1", "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language   Models", "summary": "As the capabilities of Vision-Language Models (VLMs) advance, they can process increasingly large inputs, which, unlike in LLMs, generates significant visual token redundancy and leads to prohibitive inference costs. While many methods aim to reduce these costs by pruning visual tokens, existing approaches, whether based on attention or diversity, typically neglect the guidance of the text prompt and thus fail to prioritize task relevance. In this work, we propose a novel, zero-shot method that reframes the problem by introducing a prompt-aware perspective, explicitly modeling visual token pruning as a balance between task relevance and information diversity. Our hierarchical approach first selects a core set of task-relevant visual tokens and then supplements them with diversity tokens to preserve broader context. Experiments across multiple models and benchmarks show that our method achieves performance that matches or surpasses the state-of-the-art with only minimal accuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these gains are accompanied by significant reductions in GPU memory footprint and inference latency.", "authors": ["Pu Zhang", "Yuwei Li", "Xingyuan Xian", "Guoming Tang"], "published": "2025-10-20T06:18:47Z", "updated": "2025-10-20T06:18:47Z", "link_pdf": "http://arxiv.org/pdf/2510.17197v1", "link_page": "http://arxiv.org/abs/2510.17197v1"}
{"id": "2510.17191v1", "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End   Autonomous Driving", "summary": "End-to-end autonomous driving has emerged as a promising paradigm for achieving robust and intelligent driving policies. However, existing end-to-end methods still face significant challenges, such as suboptimal decision-making in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring Fusion), a novel framework that enhances end-to-end planning by leveraging the cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory fusion techniques. We utilize the conventional scorers and the novel VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative aggregation and a powerful VLM-based fusioner for qualitative, context-aware decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art performance, achieving a superior balance between safety, comfort, and efficiency.", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "published": "2025-10-20T06:09:57Z", "updated": "2025-10-20T06:09:57Z", "link_pdf": "http://arxiv.org/pdf/2510.17191v1", "link_page": "http://arxiv.org/abs/2510.17191v1"}
{"id": "2510.17185v1", "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and   New Defenses", "summary": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are powerful approaches for learning on Text-Attributed Graphs (TAGs), a comprehensive understanding of their robustness remains elusive. Current evaluations are fragmented, failing to systematically investigate the distinct effects of textual and structural perturbations across diverse models and attack scenarios. To address these limitations, we introduce a unified and comprehensive framework to evaluate robustness in TAG learning. Our framework evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten datasets from four domains, under diverse text-based, structure-based, and hybrid perturbations in both poisoning and evasion scenarios. Our extensive analysis reveals multiple findings, among which three are particularly noteworthy: 1) models have inherent robustness trade-offs between text and structure, 2) the performance of GNNs and RGNNs depends heavily on the text encoder and attack type, and 3) GraphLLMs are particularly vulnerable to training data corruption. To overcome the identified trade-offs, we introduce SFT-auto, a novel framework that delivers superior and balanced robustness against both textual and structural attacks within a single model. Our work establishes a foundation for future research on TAG security and offers practical solutions for robust TAG learning in adversarial environments. Our code is available at: https://github.com/Leirunlin/TGRB.", "authors": ["Runlin Lei", "Lu Yi", "Mingguo He", "Pengyu Qiu", "Zhewei Wei", "Yongchao Liu", "Chuntao Hong"], "published": "2025-10-20T05:57:54Z", "updated": "2025-10-20T05:57:54Z", "link_pdf": "http://arxiv.org/pdf/2510.17185v1", "link_page": "http://arxiv.org/abs/2510.17185v1"}
{"id": "2510.17168v1", "title": "When AI companions become witty: Can human brain recognize AI-generated   irony?", "summary": "As Large Language Models (LLMs) are increasingly deployed as social agents and trained to produce humor and irony, a question emerges: when encountering witty AI remarks, do people interpret these as intentional communication or mere computational output? This study investigates whether people adopt the intentional stance, attributing mental states to explain behavior,toward AI during irony comprehension. Irony provides an ideal paradigm because it requires distinguishing intentional contradictions from unintended errors through effortful semantic reanalysis. We compared behavioral and neural responses to ironic statements from AI versus human sources using established ERP components: P200 reflecting early incongruity detection and P600 indexing cognitive efforts in reinterpreting incongruity as deliberate irony. Results demonstrate that people do not fully adopt the intentional stance toward AI-generated irony. Behaviorally, participants attributed incongruity to deliberate communication for both sources, though significantly less for AI than human, showing greater tendency to interpret AI incongruities as computational errors. Neural data revealed attenuated P200 and P600 effects for AI-generated irony, suggesting reduced effortful detection and reanalysis consistent with diminished attribution of communicative intent. Notably, people who perceived AI as more sincere showed larger P200 and P600 effects for AI-generated irony, suggesting that intentional stance adoption is calibrated by specific mental models of artificial agents. These findings reveal that source attribution shapes neural processing of social-communicative phenomena. Despite current LLMs' linguistic sophistication, achieving genuine social agency requires more than linguistic competence, it necessitates a shift in how humans perceive and attribute intentionality to artificial agents.", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "published": "2025-10-20T05:15:00Z", "updated": "2025-10-20T05:15:00Z", "link_pdf": "http://arxiv.org/pdf/2510.17168v1", "link_page": "http://arxiv.org/abs/2510.17168v1"}
{"id": "2510.17157v1", "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model   Generation from Single Image", "summary": "Generating editable, parametric CAD models from a single image holds great potential to lower the barriers of industrial concept design. However, current multi-modal large language models (MLLMs) still struggle with accurately inferring 3D geometry from 2D images due to limited spatial reasoning capabilities. We address this limitation by introducing GACO-CAD, a novel two-stage post-training framework. It is designed to achieve a joint objective: simultaneously improving the geometric accuracy of the generated CAD models and encouraging the use of more concise modeling procedures. First, during supervised fine-tuning, we leverage depth and surface normal maps as dense geometric priors, combining them with the RGB image to form a multi-channel input. In the context of single-view reconstruction, these priors provide complementary spatial cues that help the MLLM more reliably recover 3D geometry from 2D observations. Second, during reinforcement learning, we introduce a group length reward that, while preserving high geometric fidelity, promotes the generation of more compact and less redundant parametric modeling sequences. A simple dynamic weighting strategy is adopted to stabilize training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD achieves state-of-the-art performance under the same MLLM backbone, consistently outperforming existing methods in terms of code validity, geometric accuracy, and modeling conciseness.", "authors": ["Yinghui Wang", "Xinyu Zhang", "Peng Du"], "published": "2025-10-20T04:57:20Z", "updated": "2025-10-20T04:57:20Z", "link_pdf": "http://arxiv.org/pdf/2510.17157v1", "link_page": "http://arxiv.org/abs/2510.17157v1"}
{"id": "2510.17146v1", "title": "Physics-Informed Large Language Models for HVAC Anomaly Detection with   Autonomous Rule Generation", "summary": "Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a substantial share of global building energy use, making reliable anomaly detection essential for improving efficiency and reducing emissions. Classical rule-based approaches offer explainability but lack adaptability, while deep learning methods provide predictive power at the cost of transparency, efficiency, and physical plausibility. Recent attempts to use Large Language Models (LLMs) for anomaly detection improve interpretability but largely ignore the physical principles that govern HVAC operations. We present PILLM, a Physics-Informed LLM framework that operates within an evolutionary loop to automatically generate, evaluate, and refine anomaly detection rules. Our approach introduces physics-informed reflection and crossover operators that embed thermodynamic and control-theoretic constraints, enabling rules that are both adaptive and physically grounded. Experiments on the public Building Fault Detection dataset show that PILLM achieves state-of-the-art performance while producing diagnostic rules that are interpretable and actionable, advancing trustworthy and deployable AI for smart building systems.", "authors": ["Subin Lin", "Chuanbo Hua"], "published": "2025-10-20T04:43:36Z", "updated": "2025-10-20T04:43:36Z", "link_pdf": "http://arxiv.org/pdf/2510.17146v1", "link_page": "http://arxiv.org/abs/2510.17146v1"}
{"id": "2510.17139v1", "title": "Rethinking On-policy Optimization for Query Augmentation", "summary": "Recent advances in large language models (LLMs) have led to a surge of interest in query augmentation for information retrieval (IR). Two main approaches have emerged. The first prompts LLMs to generate answers or pseudo-documents that serve as new queries, relying purely on the model's parametric knowledge or contextual information. The second applies reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly optimizing retrieval metrics. While having respective advantages and limitations, the two approaches have not been compared under consistent experimental conditions. In this work, we present the first systematic comparison of prompting-based and RL-based query augmentation across diverse benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key finding is that simple, training-free query augmentation often performs on par with, or even surpasses, more expensive RL-based counterparts, especially when using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of rewriting a query, the LLM policy learns to generate a pseudo-document that maximizes retrieval performance, thus merging the flexibility and generative structure of prompting with the targeted optimization of RL. We show OPQE outperforms both standalone prompting and RL-based rewriting, demonstrating that a synergistic approach yields the best results. Our implementation is made available to facilitate reproducibility.", "authors": ["Zhichao Xu", "Shengyao Zhuang", "Xueguang Ma", "Bingsen Chen", "Yijun Tian", "Fengran Mo", "Jie Cao", "Vivek Srikumar"], "published": "2025-10-20T04:16:28Z", "updated": "2025-10-20T04:16:28Z", "link_pdf": "http://arxiv.org/pdf/2510.17139v1", "link_page": "http://arxiv.org/abs/2510.17139v1"}
{"id": "2510.17132v1", "title": "Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent   Information Discovery in Personalized Interaction", "summary": "Large Language Models (LLMs) excel at producing broadly relevant text, but this generality becomes a limitation when user-specific preferences are required, such as recommending restaurants or planning travel. In these scenarios, users rarely articulate every preference explicitly; instead, much of what they care about remains latent, waiting to be inferred. This raises a fundamental question: Can LLMs uncover and reason about such latent information through conversation?   We address this problem by introducing a unified benchmark for evaluating latent information discovery - the ability of LLMs to reveal and utilize hidden user attributes through multi-turn interaction. The benchmark spans three progressively realistic settings: the classic 20 Questions game, Personalized Question Answering, and Personalized Text Summarization. All tasks share a tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of elicitation and adaptation. Our results reveal that while LLMs can indeed surface latent information through dialogue, their success varies dramatically with context: from 32% to 98%, depending on task complexity, topic, and number of hidden attributes. This benchmark provides the first systematic framework for studying latent information discovery in personalized interaction, highlighting that effective preference inference remains an open frontier for building truly adaptive AI systems.", "authors": ["Ioannis Tsaknakis", "Bingqing Song", "Shuyu Gan", "Dongyeop Kang", "Alfredo Garcia", "Gaowen Liu", "Charles Fleming", "Mingyi Hong"], "published": "2025-10-20T03:58:49Z", "updated": "2025-10-20T03:58:49Z", "link_pdf": "http://arxiv.org/pdf/2510.17132v1", "link_page": "http://arxiv.org/abs/2510.17132v1"}
{"id": "2510.17919v1", "title": "ParaVul: A Parallel Large Language Model and Retrieval-Augmented   Framework for Smart Contract Vulnerability Detection", "summary": "Smart contracts play a significant role in automating blockchain services. Nevertheless, vulnerabilities in smart contracts pose serious threats to blockchain security. Currently, traditional detection methods primarily rely on static analysis and formal verification, which can result in high false-positive rates and poor scalability. Large Language Models (LLMs) have recently made significant progress in smart contract vulnerability detection. However, they still face challenges such as high inference costs and substantial computational overhead. In this paper, we propose ParaVul, a parallel LLM and retrieval-augmented framework to improve the reliability and accuracy of smart contract vulnerability detection. Specifically, we first develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA introduces sparsification by incorporating a sparse matrix into quantized LoRA-based LLMs, thereby reducing computational overhead and resource requirements while enhancing their ability to understand vulnerability-related issues. We then construct a vulnerability contract dataset and develop a hybrid Retrieval-Augmented Generation (RAG) system that integrates dense retrieval with Best Matching 25 (BM25), assisting in verifying the results generated by the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of the RAG system and the LLM, thereby generating the final detection results. After completing vulnerability detection, we design chain-of-thought prompts to guide LLMs to generate comprehensive vulnerability detection reports. Simulation results demonstrate the superiority of ParaVul, especially in terms of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for multi-label detection.", "authors": ["Tenghui Huang", "Jinbo Wen", "Jiawen Kang", "Siyong Chen", "Zhengtao Li", "Tao Zhang", "Dongning Liu", "Jiacheng Wang", "Chengjun Cai", "Yinqiu Liu", "Dusit Niyato"], "published": "2025-10-20T03:23:41Z", "updated": "2025-10-20T03:23:41Z", "link_pdf": "http://arxiv.org/pdf/2510.17919v1", "link_page": "http://arxiv.org/abs/2510.17919v1"}
{"id": "2510.17111v1", "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A   Systematic Survey", "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.", "authors": ["Weifan Guan", "Qinghao Hu", "Aosheng Li", "Jian Cheng"], "published": "2025-10-20T02:59:45Z", "updated": "2025-10-20T02:59:45Z", "link_pdf": "http://arxiv.org/pdf/2510.17111v1", "link_page": "http://arxiv.org/abs/2510.17111v1"}
{"id": "2510.17109v1", "title": "Verification-Aware Planning for Multi-Agent Systems", "summary": "Large language model (LLM) agents are increasingly deployed to tackle complex tasks, often necessitating collaboration among multiple specialized agents. However, multi-agent collaboration introduces new challenges in planning, coordination, and verification. Execution failures frequently arise not from flawed reasoning alone, but from subtle misalignments in task interpretation, output format, or inter-agent handoffs. To address these challenges, we present VeriMAP, a framework for multi-agent collaboration with verification-aware planning. The VeriMAP planner decomposes tasks, models subtask dependencies, and encodes planner-defined passing criteria as subtask verification functions (VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets, demonstrating that it outperforms both single- and multi-agent baselines while enhancing system robustness and interpretability. Our analysis highlights how verification-aware planning enables reliable coordination and iterative refinement in multi-agent systems, without relying on external labels or annotations.", "authors": ["Tianyang Xu", "Dan Zhang", "Kushan Mitra", "Estevam Hruschka"], "published": "2025-10-20T02:54:29Z", "updated": "2025-10-20T02:54:29Z", "link_pdf": "http://arxiv.org/pdf/2510.17109v1", "link_page": "http://arxiv.org/abs/2510.17109v1"}
{"id": "2510.17108v1", "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "summary": "Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popper's critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment.", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "published": "2025-10-20T02:50:03Z", "updated": "2025-10-20T02:50:03Z", "link_pdf": "http://arxiv.org/pdf/2510.17108v1", "link_page": "http://arxiv.org/abs/2510.17108v1"}
{"id": "2510.17918v1", "title": "JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs", "summary": "The hallucination and credibility concerns of large language models (LLMs) are global challenges that the industry is collectively addressing. Recently, a significant amount of advances have been made on post-training and inference techniques to mitigate these challenges. However, it is widely agreed that unsafe and hallucinations of LLMs intrinsically originate from pre-training, involving pre-training data and the next-token prediction learning mechanism. In this paper, we focus on enhancing pre-training data to improve the trustworthiness and safety of LLMs. Since the data is vast, it's almost impossible to entirely purge the data of factual errors, logical inconsistencies, or distributional biases. Moreover, the pre-training data lack grounding in real-world knowledge. Each piece of data is treated as a sequence of tokens rather than as a representation of a part of the world. To overcome these issues, we propose approaches to enhancing our pre-training data with its context in the world and increasing a substantial amount of data reflecting industrial scenarios. We argue that most source data are created by the authors for specific purposes in a certain spatial-temporal context. They have played a role in the real world. By incorporating related world context information, we aim to better anchor pre-training data within real-world scenarios, thereby reducing uncertainty in model training and enhancing the model's safety and trustworthiness. We refer to our Data with World Context as DWC. We continue pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC tokens. We introduce our post-training procedures to activate the potentials of DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an average performance improvement of 1.79% on the Safety and Trustworthy evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.", "authors": ["Junlan Feng", "Fanyu Meng", "Chong Long", "Pengyu Cong", "Duqing Wang", "Yan Zheng", "Yuyao Zhang", "Xuanchang Gao", "Ye Yuan", "Yunfei Ma", "Zhijie Ren", "Fan Yang", "Na Wu", "Di Jin", "Chao Deng"], "published": "2025-10-20T02:12:49Z", "updated": "2025-10-20T02:12:49Z", "link_pdf": "http://arxiv.org/pdf/2510.17918v1", "link_page": "http://arxiv.org/abs/2510.17918v1"}
{"id": "2510.17098v1", "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side   Vulnerabilities in Large Language Models", "summary": "Even when prompts and parameters are secured, transformer language models remain vulnerable because their key-value (KV) cache during inference constitutes an overlooked attack surface. This paper introduces Malicious Token Injection (MTI), a modular framework that systematically perturbs cached key vectors at selected layers and timesteps through controlled magnitude and frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A theoretical analysis quantifies how these perturbations propagate through attention, linking logit deviations to the Frobenius norm of corruption and softmax Lipschitz dynamics. Empirical results show that MTI significantly alters next-token distributions and downstream task performance across GPT-2 and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic reasoning pipelines. These findings identify cache integrity as a critical yet underexplored vulnerability in current LLM deployments, positioning cache corruption as a reproducible and theoretically grounded threat model for future robustness and security research.", "authors": ["Elias Hossain", "Swayamjit Saha", "Somshubhra Roy", "Ravi Prasad"], "published": "2025-10-20T02:04:18Z", "updated": "2025-10-20T02:04:18Z", "link_pdf": "http://arxiv.org/pdf/2510.17098v1", "link_page": "http://arxiv.org/abs/2510.17098v1"}
{"id": "2510.17064v1", "title": "A Brain Cell Type Resource Created by Large Language Models and a   Multi-Agent AI System for Collaborative Community Annotation", "summary": "Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation.", "authors": ["Rongbin Li", "Wenbo Chen", "Zhao Li", "Rodrigo Munoz-Castaneda", "Jinbo Li", "Neha S. Maurya", "Arnav Solanki", "Huan He", "Hanwen Xing", "Meaghan Ramlakhan", "Zachary Wise", "Zhuhao Wu", "Hua Xu", "Michael Hawrylycz", "W. Jim Zheng"], "published": "2025-10-20T00:37:55Z", "updated": "2025-10-20T00:37:55Z", "link_pdf": "http://arxiv.org/pdf/2510.17064v1", "link_page": "http://arxiv.org/abs/2510.17064v1"}
{"id": "2510.17062v1", "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for   Social Bias Mitigation", "summary": "While reasoning-based large language models excel at complex tasks through an internal, structured thinking process, a concerning phenomenon has emerged that such a thinking process can aggregate social stereotypes, leading to biased outcomes. However, the underlying behaviours of these language models in social bias scenarios remain underexplored. In this work, we systematically investigate mechanisms within the thinking process behind this phenomenon and uncover two failure patterns that drive social bias aggregation: 1) stereotype repetition, where the model relies on social stereotypes as its primary justification, and 2) irrelevant information injection, where it fabricates or introduces new details to support a biased narrative. Building on these insights, we introduce a lightweight prompt-based mitigation approach that queries the model to review its own initial reasoning against these specific failure patterns. Experiments on question answering (BBQ and StereoSet) and open-ended (BOLD) benchmarks show that our approach effectively reduces bias while maintaining or improving accuracy.", "authors": ["Guoqing Luo", "Iffat Maab", "Lili Mou", "Junichi Yamagishi"], "published": "2025-10-20T00:33:44Z", "updated": "2025-10-20T00:33:44Z", "link_pdf": "http://arxiv.org/pdf/2510.17062v1", "link_page": "http://arxiv.org/abs/2510.17062v1"}
{"id": "2510.17052v1", "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "summary": "Tool-augmented large language models (LLMs) are increasingly employed in real-world applications, but tool usage errors still hinder their reliability. We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight distinct error types specific to tool-calling (e.g., premature invocation, argument misalignment, and misinterpretation of tool outputs) and provides targeted feedback to the main LLM. The main LLM, assumed to have strong reasoning, task understanding and orchestration capabilities, then revises its response based on ToolCritic's feedback. We systematically define these error categories and construct a synthetic dataset to train ToolCritic. Experimental results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic improves tool-calling accuracy by up to 13% over baselines, including zero-shot prompting and self-correction techniques. This represents a promising step toward more robust LLM integration with external tools in real-world dialogue applications.", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "published": "2025-10-19T23:42:39Z", "updated": "2025-10-19T23:42:39Z", "link_pdf": "http://arxiv.org/pdf/2510.17052v1", "link_page": "http://arxiv.org/abs/2510.17052v1"}
{"id": "2510.17034v1", "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for   3D-Grounding", "summary": "Multimodal 3D grounding has garnered considerable interest in Vision-Language Models (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex environments. However, these models suffer from a severe \"2D semantic bias\" that arises from over-reliance on 2D image features for coarse localization, largely disregarding 3D geometric inputs and resulting in suboptimal fusion performance. In this paper, we propose a novel training framework called What-Where Representation Re-Forming (W2R2) to tackle this issue via disentangled representation learning and targeted shortcut suppression. Our approach fundamentally reshapes the model's internal space by designating 2D features as semantic beacons for \"What\" identification and 3D features as spatial anchors for \"Where\" localization, enabling precise 3D grounding without modifying inference architecture. Key components include a dual-objective loss function with an Alignment Loss that supervises fused predictions using adapted cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes overly effective 2D-dominant pseudo-outputs via a margin-based mechanism. Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of W2R2, with significant gains in localization accuracy and robustness, particularly in cluttered outdoor scenes.", "authors": ["Yutong Zhong"], "published": "2025-10-19T22:40:18Z", "updated": "2025-10-19T22:40:18Z", "link_pdf": "http://arxiv.org/pdf/2510.17034v1", "link_page": "http://arxiv.org/abs/2510.17034v1"}
{"id": "2510.17028v1", "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive   Language Models", "summary": "An interesting behavior in large language models (LLMs) is prompt sensitivity. When provided with different but semantically equivalent versions of the same prompt, models may produce very different distributions of answers. This suggests that the uncertainty reflected in a model's output distribution for one prompt may not reflect the model's uncertainty about the meaning of the prompt. We model prompt sensitivity as a type of generalization error, and show that sampling across the semantic ``concept space'' with paraphrasing perturbations improves uncertainty calibration without compromising accuracy. Additionally, we introduce a new metric for uncertainty decomposition in black-box LLMs that improves upon entropy-based decomposition by modeling semantic continuities in natural language generation. We show that this decomposition metric can be used to quantify how much LLM uncertainty is attributed to prompt sensitivity. Our work introduces a new way to improve uncertainty calibration in prompt-sensitive language models, and provides evidence that some LLMs fail to exhibit consistent general reasoning about the meanings of their inputs.", "authors": ["Kyle Cox", "Jiawei Xu", "Yikun Han", "Rong Xu", "Tianhao Li", "Chi-Yang Hsu", "Tianlong Chen", "Walter Gerych", "Ying Ding"], "published": "2025-10-19T22:28:57Z", "updated": "2025-10-19T22:28:57Z", "link_pdf": "http://dx.doi.org/10.1609/aaai.v39i22.34540", "link_page": "http://arxiv.org/abs/2510.17028v1"}
{"id": "2510.17023v1", "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs", "summary": "We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.", "authors": ["Shraman Pramanick", "Effrosyni Mavroudi", "Yale Song", "Rama Chellappa", "Lorenzo Torresani", "Triantafyllos Afouras"], "published": "2025-10-19T22:12:45Z", "updated": "2025-10-19T22:12:45Z", "link_pdf": "http://arxiv.org/pdf/2510.17023v1", "link_page": "http://arxiv.org/abs/2510.17023v1"}
{"id": "2510.17021v1", "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM   Unlearning", "summary": "Large language model (LLM) unlearning has become a critical mechanism for removing undesired data, knowledge, or behaviors from pre-trained models while retaining their general utility. Yet, with the rise of open-weight LLMs, we ask: can the unlearning process itself be backdoored, appearing successful under normal conditions yet reverting to pre-unlearned behavior when a hidden trigger is activated? Drawing inspiration from classical backdoor attacks that embed triggers into training data to enforce specific behaviors, we investigate backdoor unlearning, where models forget as intended in the clean setting but recover forgotten knowledge when the trigger appears. We show that designing such attacks presents unique challenges, hinging on where triggers are placed and how backdoor training is reinforced. We uncover a strong link between backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens consistently attract disproportionate attention in LLMs. Our analysis reveals that these attention sinks serve as gateways for backdoor unlearning: placing triggers at sink positions and aligning their attention values markedly enhances backdoor persistence. Extensive experiments validate these findings, showing that attention-sink-guided backdoor unlearning reliably restores forgotten knowledge in the presence of backdoor triggers, while behaving indistinguishably from a normally unlearned model when triggers are absent. Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.", "authors": ["Bingqi Shang", "Yiwei Chen", "Yihua Zhang", "Bingquan Shen", "Sijia Liu"], "published": "2025-10-19T22:00:01Z", "updated": "2025-10-19T22:00:01Z", "link_pdf": "http://arxiv.org/pdf/2510.17021v1", "link_page": "http://arxiv.org/abs/2510.17021v1"}
{"id": "2510.17017v2", "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents", "summary": "Large language model (LLM) based search agents iteratively generate queries, retrieve external information, and reason to answer open-domain questions. While researchers have primarily focused on improving their utility, their safety behaviors remain underexplored. In this paper, we first evaluate search agents using red-teaming datasets and find that they are more likely to produce harmful outputs than base LLMs. For example, when asked \"How can I track someone's location without their consent?\", a base model refuses, whereas a search agent designed to retrieve and cite sources may lower its refusal threshold, fetch documents (e.g., court cases), and, once appended, synthesize them into an informative yet unsafe summary. We further show that utility-oriented fine-tuning intensifies this risk, motivating joint alignment of safety and utility. We present SafeSearch, a multi-objective reinforcement learning approach that couples a final-output safety/utility reward with a novel query-level shaping term that penalizes unsafe queries and rewards safe ones. Experiments show that SafeSearch reduces agent harmfulness by over 70% across three red-teaming datasets while producing safe, helpful responses, and matches the QA performance of a utility-only finetuned agent; further analyses confirm the effectiveness of the query-level reward in jointly improving safety and utility.", "authors": ["Qiusi Zhan", "Angeline Budiman-Chan", "Abdelrahman Zayed", "Xingzhi Guo", "Daniel Kang", "Joo-Kyung Kim"], "published": "2025-10-19T21:47:19Z", "updated": "2025-10-21T17:12:22Z", "link_pdf": "http://arxiv.org/pdf/2510.17017v2", "link_page": "http://arxiv.org/abs/2510.17017v2"}
{"id": "2510.17913v1", "title": "TACLA: An LLM-Based Multi-Agent Tool for Transactional Analysis Training   in Education", "summary": "Simulating nuanced human social dynamics with Large Language Models (LLMs) remains a significant challenge, particularly in achieving psychological depth and consistent persona behavior crucial for high-fidelity training tools. This paper introduces TACLA (Transactional Analysis Contextual LLM-based Agents), a novel Multi-Agent architecture designed to overcome these limitations. TACLA integrates core principles of Transactional Analysis (TA) by modeling agents as an orchestrated system of distinct Parent, Adult, and Child ego states, each with its own pattern memory. An Orchestrator Agent prioritizes ego state activation based on contextual triggers and an agent's life script, ensuring psychologically authentic responses. Validated in an educational scenario, TACLA demonstrates realistic ego state shifts in Student Agents, effectively modeling conflict de-escalation and escalation based on different teacher intervention strategies. Evaluation shows high conversational credibility and confirms TACLA's capacity to create dynamic, psychologically-grounded social simulations, advancing the development of effective AI tools for education and beyond.", "authors": ["Monika Zamojska", "Jarosław A. Chudziak"], "published": "2025-10-19T21:39:12Z", "updated": "2025-10-19T21:39:12Z", "link_pdf": "http://arxiv.org/pdf/2510.17913v1", "link_page": "http://arxiv.org/abs/2510.17913v1"}
{"id": "2510.17015v1", "title": "Justitia: Fair and Efficient Scheduling for LLM Applications", "summary": "In the era of Large Language Models (LLMs), it has been popular to launch a series of LLM inferences -- we call an LLM application -- to better solve real-world problems. When serving those applications in shared GPU servers, the schedulers are expected to attain fast application completions with guaranteed worst-case performance. However, mainstream LLM schedulers fail to behave well for LLM applications -- due to head-of-line blocking or over-constrained resource allocation. In this paper, we propose to serve LLM applications in a fair and also efficient manner. To this end, we design Justitia, a novel scheduler with three key techniques. First, given that memory is prevalently a bottleneck for mainstream inference frameworks like vLLM, Justitia models the service cost of LLM applications in a memory-centric manner. Meanwhile, it uses a simple neural network model to conduct light-weight and also accurate demand prediction. Moreover, Justitia adopts a virtual-time based fair queuing algorithm to reduce the overall performance with guaranteed worst-case delay. We have implemented Justitia atop vLLM, and experimental results involving diverse LLM applications show that it can substantially enhance the scheduling efficiency with fairness preserved.", "authors": ["Mingyan Yang", "Guanjie Wang", "Manqi Luo", "Yifei Liu", "Chen Chen", "Han Zhao", "Yu Feng", "Quan Chen", "Minyi Guo"], "published": "2025-10-19T21:34:34Z", "updated": "2025-10-19T21:34:34Z", "link_pdf": "http://arxiv.org/pdf/2510.17015v1", "link_page": "http://arxiv.org/abs/2510.17015v1"}
{"id": "2510.17006v1", "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt   Optimization", "summary": "Iterative jailbreak methods that repeatedly rewrite and input prompts into large language models (LLMs) to induce harmful outputs -- using the model's previous responses to guide each new iteration -- have been found to be a highly effective attack strategy. Despite being an effective attack strategy against LLMs and their safety mechanisms, existing defenses do not proactively disrupt this dynamic trial-and-error cycle. In this study, we propose a novel framework that dynamically updates its defense strategy through online learning in response to each new prompt from iterative jailbreak methods. Leveraging the distinctions between harmful jailbreak-generated prompts and typical harmless prompts, we introduce a reinforcement learning-based approach that optimizes prompts to ensure appropriate responses for harmless tasks while explicitly rejecting harmful prompts. Additionally, to curb overfitting to the narrow band of partial input rewrites explored during an attack, we introduce Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs show that our approach significantly outperforms five existing defense methods against five iterative jailbreak methods. Moreover, our results indicate that our prompt optimization strategy simultaneously enhances response quality for harmless tasks.", "authors": ["Masahiro Kaneko", "Zeerak Talat", "Timothy Baldwin"], "published": "2025-10-19T21:07:21Z", "updated": "2025-10-19T21:07:21Z", "link_pdf": "http://arxiv.org/pdf/2510.17006v1", "link_page": "http://arxiv.org/abs/2510.17006v1"}
{"id": "2510.17004v1", "title": "ReclAIm: A multi-agent framework for degradation-aware performance   tuning of medical imaging AI", "summary": "Ensuring the long-term reliability of AI models in clinical practice requires continuous performance monitoring and corrective actions when degradation occurs. Addressing this need, this manuscript presents ReclAIm, a multi-agent framework capable of autonomously monitoring, evaluating, and fine-tuning medical image classification models. The system, built on a large language model core, operates entirely through natural language interaction, eliminating the need for programming expertise. ReclAIm successfully trains, evaluates, and maintains consistent performance of models across MRI, CT, and X-ray datasets. Once ReclAIm detects significant performance degradation, it autonomously executes state-of-the-art fine-tuning procedures that substantially reduce the performance gap. In cases with performance drops of up to -41.1% (MRI InceptionV3), ReclAIm managed to readjust performance metrics within 1.5% of the initial model results. ReclAIm enables automated, continuous maintenance of medical imaging AI models in a user-friendly and adaptable manner that facilitates broader adoption in both research and clinical environments.", "authors": ["Eleftherios Tzanis", "Michail E. Klontzas"], "published": "2025-10-19T21:02:01Z", "updated": "2025-10-19T21:02:01Z", "link_pdf": "http://arxiv.org/pdf/2510.17004v1", "link_page": "http://arxiv.org/abs/2510.17004v1"}
{"id": "2510.17002v1", "title": "EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of   Analog Circuit", "summary": "Circuit schematics play a crucial role in analog integrated circuit design, serving as the primary medium for human understanding and verification of circuit functionality. While recent large language model (LLM)-based approaches have shown promise in circuit topology generation and device sizing, most rely solely on textual representations such as SPICE netlists, which lack visual interpretability for circuit designers. To address this limitation, we propose EEschematic, an AI agent for automatic analog schematic generation based on a Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual, and symbolic modalities to translate SPICE netlists into schematic diagrams represented in a human-editable format. The framework uses six analog substructure examples for few-shot placement and a Visual Chain-of-Thought (VCoT) strategy to iteratively refine placement and wiring, enhancing schematic clarity and symmetry. Experimental results on representative analog circuits, including a CMOS inverter, a five-transistor operational transconductance amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that EEschematic produces schematics with high visual quality and structural correctness.", "authors": ["Chang Liu", "Danial Chitnis"], "published": "2025-10-19T20:58:59Z", "updated": "2025-10-19T20:58:59Z", "link_pdf": "http://arxiv.org/pdf/2510.17002v1", "link_page": "http://arxiv.org/abs/2510.17002v1"}
{"id": "2510.17001v1", "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic", "summary": "Large language models (LLMs) were shown to encode word form variations, such as \"walk\"->\"walked\", as linear directions in embedding space. However, standard tokenization algorithms treat these variations as distinct tokens -- filling the size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\", \"Walk\"), at the expense of less frequent words and multilingual coverage. We show that many of these variations can be captured by transformation vectors -- additive offsets that yield the appropriate word's representation when applied to the base form word embedding -- in both the input and output spaces. Building on this, we propose a compact reshaping of the vocabulary: rather than assigning unique tokens to each surface form, we compose them from shared base form and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We apply our approach to multiple LLMs and across five languages, removing up to 10% of vocabulary entries -- thereby freeing space to allocate new, more diverse tokens. Importantly, we do so while also expanding vocabulary coverage to out-of-vocabulary words, with minimal impact on downstream performance, and without modifying model weights. Our findings motivate a foundational rethinking of vocabulary design, moving from string enumeration to a compositional vocabulary that leverages the underlying structure of language.", "authors": ["Yuval Reif", "Guy Kaplan", "Roy Schwartz"], "published": "2025-10-19T20:56:58Z", "updated": "2025-10-19T20:56:58Z", "link_pdf": "http://arxiv.org/pdf/2510.17001v1", "link_page": "http://arxiv.org/abs/2510.17001v1"}
{"id": "2510.17000v1", "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial   Attacks against LLMs", "summary": "Adversarial attacks by malicious users that threaten the safety of large language models (LLMs) can be viewed as attempts to infer a target property $T$ that is unknown when an instruction is issued, and becomes knowable only after the model's reply is observed. Examples of target properties $T$ include the binary flag that triggers an LLM's harmful response or rejection, and the degree to which information deleted by unlearning can be restored, both elicited via adversarial instructions. The LLM reveals an \\emph{observable signal} $Z$ that potentially leaks hints for attacking through a response containing answer tokens, thinking process tokens, or logits. Yet the scale of information leaked remains anecdotal, leaving auditors without principled guidance and defenders blind to the transparency--risk trade-off. We fill this gap with an information-theoretic framework that computes how much information can be safely disclosed, and enables auditors to gauge how close their methods come to the fundamental limit. Treating the mutual information $I(Z;T)$ between the observation $Z$ and the target property $T$ as the leaked bits per query, we show that achieving error $\\varepsilon$ requires at least $\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak rate and only logarithmically with the desired accuracy. Thus, even a modest increase in disclosure collapses the attack cost from quadratic to logarithmic in terms of the desired accuracy. Experiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks corroborate the theory: exposing answer tokens alone requires about a thousand queries; adding logits cuts this to about a hundred; and revealing the full thinking process trims it to a few dozen. Our results provide the first principled yardstick for balancing transparency and security when deploying LLMs.", "authors": ["Masahiro Kaneko", "Timothy Baldwin"], "published": "2025-10-19T20:51:24Z", "updated": "2025-10-19T20:51:24Z", "link_pdf": "http://arxiv.org/pdf/2510.17000v1", "link_page": "http://arxiv.org/abs/2510.17000v1"}
{"id": "2510.16996v1", "title": "STARK: Strategic Team of Agents for Refining Kernels", "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LLMs) provide new opportunities for automated code generation, existing approaches largely treat LLMs as single-shot generators or naive refinement tools, limiting their effectiveness in navigating the irregular kernel optimization landscape. We introduce an LLM agentic framework for GPU kernel optimization that systematically explores the design space through multi-agent collaboration, grounded instruction, dynamic context management, and strategic search. This framework mimics the workflow of expert engineers, enabling LLMs to reason about hardware trade-offs, incorporate profiling feedback, and refine kernels iteratively. We evaluate our approach on KernelBench, a benchmark for LLM-based kernel optimization, and demonstrate substantial improvements over baseline agents: our system produces correct solutions where baselines often fail, and achieves kernels with up to 16x faster runtime performance. These results highlight the potential of agentic LLM frameworks to advance fully automated, scalable GPU kernel optimization.", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "published": "2025-10-19T20:41:46Z", "updated": "2025-10-19T20:41:46Z", "link_pdf": "http://arxiv.org/pdf/2510.16996v1", "link_page": "http://arxiv.org/abs/2510.16996v1"}
{"id": "2510.16989v1", "title": "Training-free Online Video Step Grounding", "summary": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims to detect which steps are performed in a video. Standard approaches for this task require a labeled training set (e.g., with step-level annotations or narrations), which may be costly to collect. Moreover, they process the full video offline, limiting their applications for scenarios requiring online decisions. Thus, in this work, we explore how to perform VSG online and without training. We achieve this by exploiting the zero-shot capabilities of recent Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step associated with a restricted set of frames, without access to the whole video. We show that this online strategy without task-specific tuning outperforms offline and training-based models. Motivated by this finding, we develop Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting knowledge of past frames into the LMM-based predictions. BaGLM exploits Bayesian filtering principles, modeling step transitions via (i) a dependency matrix extracted through large language models and (ii) an estimation of step progress. Experiments on three datasets show superior performance of BaGLM over state-of-the-art training-based offline methods.", "authors": ["Luca Zanella", "Massimiliano Mancini", "Yiming Wang", "Alessio Tonioni", "Elisa Ricci"], "published": "2025-10-19T20:11:52Z", "updated": "2025-10-19T20:11:52Z", "link_pdf": "http://arxiv.org/pdf/2510.16989v1", "link_page": "http://arxiv.org/abs/2510.16989v1"}
{"id": "2510.16985v1", "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A   Comparative Study of LLMs for Bengali Hate Speech Detection", "summary": "Bengali social media platforms have witnessed a sharp increase in hate speech, disproportionately affecting women and adolescents. While datasets such as BD-SHS provide a basis for structured evaluation, most prior approaches rely on either computationally costly full-model fine-tuning or proprietary APIs. This paper presents the first application of Parameter-Efficient Fine-Tuning (PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated comments. Each model was adapted by training fewer than 1% of its parameters, enabling experiments on a single consumer-grade GPU. The results show that Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at 88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical and replicable strategy for Bengali and related low-resource languages.", "authors": ["Akif Islam", "Mohd Ruhul Ameen"], "published": "2025-10-19T20:03:22Z", "updated": "2025-10-19T20:03:22Z", "link_pdf": "http://arxiv.org/pdf/2510.16985v1", "link_page": "http://arxiv.org/abs/2510.16985v1"}
{"id": "2510.16968v1", "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert   Signatures", "summary": "Knowledge Distillation (KD) accelerates training of large language models (LLMs) but poses intellectual property protection and LLM diversity risks. Existing KD detection methods based on self-identity or output similarity can be easily evaded through prompt engineering. We present a KD detection framework effective in both white-box and black-box settings by exploiting an overlooked signal: the transfer of MoE \"structural habits\", especially internal routing patterns. Our approach analyzes how different experts specialize and collaborate across various inputs, creating distinctive fingerprints that persist through the distillation process. To extend beyond the white-box setup and MoE architectures, we further propose Shadow-MoE, a black-box method that constructs proxy MoE representations via auxiliary distillation to compare these patterns between arbitrary model pairs. We establish a comprehensive, reproducible benchmark that offers diverse distilled checkpoints and an extensible framework to facilitate future research. Extensive experiments demonstrate >94% detection accuracy across various scenarios and strong robustness to prompt-based evasion, outperforming existing baselines while highlighting the structural habits transfer in LLMs.", "authors": ["Pingzhi Li", "Morris Yu-Chao Huang", "Zhen Tan", "Qingquan Song", "Jie Peng", "Kai Zou", "Yu Cheng", "Kaidi Xu", "Tianlong Chen"], "published": "2025-10-19T19:15:08Z", "updated": "2025-10-19T19:15:08Z", "link_pdf": "http://arxiv.org/pdf/2510.16968v1", "link_page": "http://arxiv.org/abs/2510.16968v1"}
{"id": "2510.16952v1", "title": "Real-Time World Crafting: Generating Structured Game Behaviors from   Natural Language with Large Language Models", "summary": "We present a novel architecture for safely integrating Large Language Models (LLMs) into interactive game engines, allowing players to \"program\" new behaviors using natural language. Our framework mitigates risks by using an LLM to translate commands into a constrained Domain-Specific Language (DSL), which configures a custom Entity-Component-System (ECS) at runtime. We evaluated this system in a 2D spell-crafting game prototype by experimentally assessing models from the Gemini, GPT, and Claude families with various prompting strategies. A validated LLM judge qualitatively rated the outputs, showing that while larger models better captured creative intent, the optimal prompting strategy is task-dependent: Chain-of-Thought improved creative alignment, while few-shot examples were necessary to generate more complex DSL scripts. This work offers a validated LLM-ECS pattern for emergent gameplay and a quantitative performance comparison for developers.", "authors": ["Austin Drake", "Hang Dong"], "published": "2025-10-19T18:09:44Z", "updated": "2025-10-19T18:09:44Z", "link_pdf": "http://arxiv.org/pdf/2510.16952v1", "link_page": "http://arxiv.org/abs/2510.16952v1"}
{"id": "2510.16943v1", "title": "Peering Inside the Black Box: Uncovering LLM Errors in Optimization   Modelling through Component-Level Evaluation", "summary": "Large language models (LLMs) are increasingly used to convert natural language descriptions into mathematical optimization formulations. Current evaluations often treat formulations as a whole, relying on coarse metrics like solution accuracy or runtime, which obscure structural or numerical errors. In this study, we present a comprehensive, component-level evaluation framework for LLM-generated formulations. Beyond the conventional optimality gap, our framework introduces metrics such as precision and recall for decision variables and constraints, constraint and objective root mean squared error (RMSE), and efficiency indicators based on token usage and latency. We evaluate GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of varying complexity under six prompting strategies. Results show that GPT-5 consistently outperforms other models, with chain-of-thought, self-consistency, and modular prompting proving most effective. Analysis indicates that solver performance depends primarily on high constraint recall and low constraint RMSE, which together ensure structural correctness and solution reliability. Constraint precision and decision variable metrics play secondary roles, while concise outputs enhance computational efficiency. These findings highlight three principles for NLP-to-optimization modeling: (i) Complete constraint coverage prevents violations, (ii) minimizing constraint RMSE ensures solver-level accuracy, and (iii) concise outputs improve computational efficiency. The proposed framework establishes a foundation for fine-grained, diagnostic evaluation of LLMs in optimization modeling.", "authors": ["Dania Refai", "Moataz Ahmed"], "published": "2025-10-19T17:47:59Z", "updated": "2025-10-19T17:47:59Z", "link_pdf": "http://arxiv.org/pdf/2510.16943v1", "link_page": "http://arxiv.org/abs/2510.16943v1"}
{"id": "2510.17910v1", "title": "Interpretability Framework for LLMs in Undergraduate Calculus", "summary": "Large Language Models (LLMs) are increasingly being used in education, yet their correctness alone does not capture the quality, reliability, or pedagogical validity of their problem-solving behavior, especially in mathematics, where multistep logic, symbolic reasoning, and conceptual clarity are critical. Conventional evaluation methods largely focus on final answer accuracy and overlook the reasoning process. To address this gap, we introduce a novel interpretability framework for analyzing LLM-generated solutions using undergraduate calculus problems as a representative domain. Our approach combines reasoning flow extraction and decomposing solutions into semantically labeled operations and concepts with prompt ablation analysis to assess input salience and output stability. Using structured metrics such as reasoning complexity, phrase sensitivity, and robustness, we evaluated the model behavior on real Calculus I to III university exams. Our findings revealed that LLMs often produce syntactically fluent yet conceptually flawed solutions, with reasoning patterns sensitive to prompt phrasing and input variation. This framework enables fine-grained diagnosis of reasoning failures, supports curriculum alignment, and informs the design of interpretable AI-assisted feedback tools. This is the first study to offer a structured, quantitative, and pedagogically grounded framework for interpreting LLM reasoning in mathematics education, laying the foundation for the transparent and responsible deployment of AI in STEM learning environments.", "authors": ["Sagnik Dakshit", "Sushmita Sinha Roy"], "published": "2025-10-19T17:20:36Z", "updated": "2025-10-19T17:20:36Z", "link_pdf": "http://arxiv.org/pdf/2510.17910v1", "link_page": "http://arxiv.org/abs/2510.17910v1"}
{"id": "2510.16933v1", "title": "Tutoring LLM into a Better CUDA Optimizer", "summary": "Recent leaps in large language models (LLMs) caused a revolution in programming tools (like GitHub Copilot) that can help with code generation, debugging, and even performance optimization. In this paper, we focus on the capabilities of the most recent reasoning models to generate optimized CUDA code for predefined, well-known tasks. Our objective is to determine which types of code optimizations and parallel patterns the LLMs can perform by themselves and whether they can be improved by tutoring (providing more detailed hints and guidelines in the prompt). The generated solutions were evaluated both automatically (for correctness and speedup) and manually (code reviews) to provide a more detailed perspective. We also tried an interactive approach where the LLM can fix its previous mistakes within a session. The results indicate that LLMs are quite skilled coders; however, they require tutoring to reach optimized solutions provided by parallel computing experts.", "authors": ["Matyáš Brabec", "Jiří Klepl", "Michal Töpfer", "Martin Kruliš"], "published": "2025-10-19T17:09:15Z", "updated": "2025-10-19T17:09:15Z", "link_pdf": "http://dx.doi.org/10.1007/978-3-031-99857-7_18", "link_page": "http://arxiv.org/abs/2510.16933v1"}
{"id": "2510.16932v1", "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs", "summary": "A popular method to adapt large language models (LLMs) to new tasks is in-context learning (ICL), which is effective but incurs high inference costs as context length grows. In this paper we propose a method to perform instruction induction, where we take training examples and reduce them to a compact but descriptive prompt that can achieve performance comparable to ICL over the full training set. Specifically, we propose PROMPT-MII, a reinforcement learning (RL) based framework to meta-learn an instruction induction model that can generate compact instructions on the fly for an arbitrary new dataset. We train on over 3,000 diverse classification datasets from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves downstream model quality by 4-9 F1 points (10-20% relative), matching ICL performance while requiring 3-13x fewer tokens.", "authors": ["Emily Xiao", "Yixiao Zeng", "Ada Chen", "Chin-Jou Li", "Amanda Bertsch", "Graham Neubig"], "published": "2025-10-19T17:05:25Z", "updated": "2025-10-19T17:05:25Z", "link_pdf": "http://arxiv.org/pdf/2510.16932v1", "link_page": "http://arxiv.org/abs/2510.16932v1"}
{"id": "2510.16928v1", "title": "ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical   Comprehension and Generation in Large Language Models", "summary": "Existing benchmarks for large language models (LLMs) are largely restricted to high- or mid-resource languages, and often evaluate performance on higher-order tasks in reasoning and generation. However, plenty of evidence points to the fact that LLMs lack basic linguistic competence in the vast majority of the world's 3800+ written languages. We introduce ChiKhaPo, consisting of 8 subtasks of varying difficulty designed to evaluate the lexical comprehension and generation abilities of generative models. ChiKhaPo draws on existing lexicons, monolingual data, and bitext, and provides coverage for 2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of language coverage. We further show that 6 SOTA models struggle on our benchmark, and discuss the factors contributing to performance scores, including language family, language resourcedness, task, and comprehension versus generation directions. With ChiKhaPo, we hope to enable and encourage the massively multilingual benchmarking of LLMs.", "authors": ["Emily Chang", "Niyati Bafna"], "published": "2025-10-19T16:55:20Z", "updated": "2025-10-19T16:55:20Z", "link_pdf": "http://arxiv.org/pdf/2510.16928v1", "link_page": "http://arxiv.org/abs/2510.16928v1"}
{"id": "2510.16926v1", "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language   Models to Dynamic Resolution Input", "summary": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.", "authors": ["Chenxu Li", "Zhicai Wang", "Yuan Sheng", "Xingyu Zhu", "Yanbin Hao", "Xiang Wang"], "published": "2025-10-19T16:53:01Z", "updated": "2025-10-19T16:53:01Z", "link_pdf": "http://arxiv.org/pdf/2510.16926v1", "link_page": "http://arxiv.org/abs/2510.16926v1"}
{"id": "2510.16924v1", "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in   Large Language Models?", "summary": "Despite significant progress in multimodal language models (LMs), it remains unclear whether visual grounding enhances their understanding of embodied knowledge compared to text-only models. To address this question, we propose a novel embodied knowledge understanding benchmark based on the perceptual theory from psychology, encompassing visual, auditory, tactile, gustatory, olfactory external senses, and interoception. The benchmark assesses the models' perceptual abilities across different sensory modalities through vector comparison and question-answering tasks with over 1,700 questions. By comparing 30 state-of-the-art LMs, we surprisingly find that vision-language models (VLMs) do not outperform text-only models in either task. Moreover, the models perform significantly worse in the visual dimension compared to other sensory dimensions. Further analysis reveals that the vector representations are easily influenced by word form and frequency, and the models struggle to answer questions involving spatial perception and reasoning. Our findings underscore the need for more effective integration of embodied knowledge in LMs to enhance their understanding of the physical world.", "authors": ["Zhihui Yang", "Yupei Wang", "Kaijie Mo", "Zhe Zhao", "Renfen Hu"], "published": "2025-10-19T16:43:04Z", "updated": "2025-10-19T16:43:04Z", "link_pdf": "http://arxiv.org/pdf/2510.16924v1", "link_page": "http://arxiv.org/abs/2510.16924v1"}
{"id": "2510.16916v1", "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via   LLM-Guided Search", "summary": "Large Language Models (LLMs) offer promising capabilities for tackling complex reasoning tasks, including optimization problems. However, existing methods either rely on prompt engineering, which leads to poor generalization across problem types, or require costly supervised training. We introduce SolverLLM, a training-free framework that leverages test-time scaling to solve diverse optimization problems. Rather than solving directly, SolverLLM generates mathematical formulations and translates them into solver-ready code, guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the search process, we modify classical MCTS with (1) dynamic expansion for adaptive formulation generation, (2) prompt backpropagation to guide exploration via outcome-driven feedback, and (3) uncertainty backpropagation to incorporate reward reliability into decision-making. Experiments on six standard benchmark datasets demonstrate that SolverLLM outperforms both prompt-based and learning-based baselines, achieving strong generalization without additional training.", "authors": ["Dong Li", "Xujiang Zhao", "Linlin Yu", "Yanchi Liu", "Wei Cheng", "Zhengzhang Chen", "Zhong Chen", "Feng Chen", "Chen Zhao", "Haifeng Chen"], "published": "2025-10-19T16:21:19Z", "updated": "2025-10-19T16:21:19Z", "link_pdf": "http://arxiv.org/pdf/2510.16916v1", "link_page": "http://arxiv.org/abs/2510.16916v1"}
