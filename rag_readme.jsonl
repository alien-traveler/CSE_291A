{"id": "935963004_MAET_README.md", "paper_id": "935963004_MAET_README", "text": "# MAET\nThis is the official repository of our paper for ACM MM 2023: [Multimodal Adaptive Emotion Transformer with Flexible Modality Inputs on A Novel Dataset with Continuous Labels](https://dl.acm.org/doi/10.1145/3581783.3613797).\n\nNEWS: The extended version of this conference paper has been accepted by IEEE Transactions on Affective Computing. Please refer to [SEED-VII: A Multimodal Dataset of Six Basic Emotions with Continuous Labels for Emotion Recognition](https://xploreqa.ieee.org/document/10731546). The dataset is now available.\n\n## Abstract\nEmotion recognition from physiological signals is a topic of widespread interest, and researchers continue to develop novel techniques for perceiving emotions. However, the emergence of deep learning has highlighted the need for high-quality emotional datasets to accurately decode human emotions. In this study, we present a novel multimodal emotion dataset that incorporates electroencephalography (EEG) and eye movement signals to systematically explore human emotions. Seven basic emotions (happy, sad, fear, disgust, surprise, anger, and neutral) are elicited by a large number of 80 videos and fully investigated with continuous labels that indicate the intensity of the corresponding emotions. Additionally, we propose a novel Multimodal Adaptive Emotion Transformer (MAET), that can flexibly process both unimodal and multimodal inputs. Adversarial training is utilized in MAET to mitigate subject discrepancy, which enhances domain generalization. Our extensive experiments, encompassing both subject-dependent and cross-subject conditions, demonstrate MAET's superior performance in handling various inputs. The filtering of data for high emotional evocation using continuous labels proved to be effective in the experiments. Furthermore, the complementary properties between EEG and eye movements are observed.\n\n## Dataset\nThe SEED-VII dataset is publicly available on [here](https://bcmi.sjtu.edu.cn/home/seed/seed-vii.html).\n\n## Extract Features\n- EEG_feature_extractor/extract_EEG_features.py: Extract EEG features\n- Eye_feature_extractor/extract_eye_features.py: Extract eye movement features\n\n## Requirements\n* python==3.10.9\n* pytorch==2.0.0\n* timm==0.4.12\n\n## Example\nExample code for the use of MAET:\n```python\nimport torch\nfrom torch import nn\nfrom model import MAET\nfrom functools import partial\n\nmodel = MAET(embed_dim=32, num_classes=7, eeg_seq_len=5, eye_seq_len=5, eeg_dim=310, eye_dim=33, depth=3, num_heads=4, qkv_bias=True, mixffn_start_layer_index=2, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n\ninput_eeg = torch.randn(64, 310)\ninput_eye = torch.randn(64, 33)\n\n# single EEG input\nout_eeg = model(eeg=input_eeg)\n\n# single eye movements input\nout_eye = model(eye=input_eye)\n\n# multimodal input\nout_mul = model(eeg=input_eeg, eye=input_eye)\n```\nExample code for cross-subject training:\n```python\nimport torch\nimport torch.nn.functional as F\nfrom model import MAET\nfrom functools import partial\nfrom sklearn.metrics import accuracy_score\nimport math\n\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, epsilon: float = 0.1, reduction='mean'):\n        super().__init__()\n        self.epsilon = epsilon\n        self.reduction = reduction\n\n    def forward(self, preds, target):\n        n = preds.size()[-1]\n        log_preds = F.log_softmax(preds, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        return linear_combination(loss / n, nll, self.epsilon)\n\n\ndef train(optimizer, train_dataloader, local_rank, epochs, num_domains):\n    model = MAET(embed_dim=32, num_classes=7, eeg_seq_len=5, eye_seq_len=5, eeg_dim=310, eye_dim=33, depth=3, num_heads=4, qkv_bias=True, mixffn_start_layer_index=2, norm_layer=partial(nn.LayerNorm, eps=1e-6), domain_generalization=True, num_domains=num_domains)\n    criterion = LabelSmoothingCrossEntropy()\n    for epoch in range(epochs):\n        model.train()\n        alpha = 2 / (1 + math.exp(-10 * epoch / epochs)) - 1\n        label_smoothing = (num_domains - 1) / num_domains * epoch / epochs\n        criterion.epsilon = label_smoothing\n        loss_all = 0\n        preds = []\n        labels = []\n        for eeg, label, domain_label in train_dataloader:\n            label = label.to(local_rank, non_blocking=True)\n            eeg = eeg.to(local_rank, non_blocking=True)\n            domain_label = domain_label.to(local_rank, non_blocking=True)\n            outputs, domain_output = model(eeg=eeg, alpha_=alpha)\n            loss_ce = F.cross_entropy(input=outputs, target=label.long())\n            loss_domain = criterion(domain_output, domain_label.long())\n            loss = loss_ce + loss_domain\n            loss_all += loss.item()\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            preds.append(torch.argmax(outputs, dim=-1).cpu())\n            labels.append(label.cpu())\n        pred = torch.cat(preds, dim=0)\n        label = torch.cat(labels, dim=0)\n        train_accuracy = accuracy_score(label, pred)\n```\n\n## Citation\nIf you find our paper/code/dataset useful, please consider citing our work:\n```\n@inproceedings{10.1145/3581783.3613797,\nauthor = {Jiang, Wei-Bang and Liu, Xuan-Hao and Zheng, Wei-Long and Lu, Bao-Liang},\ntitle = {Multimodal Adaptive Emotion Transformer with Flexible Modality Inputs on A Novel Dataset with Continuous Labels},\nyear = {2023},\nisbn = {9798400701085},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3581783.3613797},\ndoi = {10.1145/3581783.3613797},\nabstract = {Emotion recognition from physiological signals is a topic of widespread interest, and researchers continue to develop novel techniques for perceiving emotions. However, the emergence of deep learning has highlighted the need for high-quality emotional datasets to accurately decode human emotions. In this study, we present a novel multimodal emotion dataset that incorporates electroencephalography (EEG) and eye movement signals to systematically explore human emotions. Seven basic emotions (happy, sad, fear, disgust, surprise, anger, and neutral) are elicited by a large number of 80 videos and fully investigated with continuous labels that indicate the intensity of the corresponding emotions. Additionally, we propose a novel Multimodal Adaptive Emotion Transformer (MAET), that can flexibly process both unimodal and multimodal inputs. Adversarial training is utilized in MAET to mitigate subject discrepancy, which enhances domain generalization. Our extensive experiments, encompassing both subject-dependent and cross-subject conditions, demonstrate MAET's superior performance in handling various inputs. The filtering of data for high emotional evocation using continuous labels proved to be effective in the experiments. Furthermore, the complementary properties between EEG and eye movements are observed. Our code is available at https://github.com/935963004/MAET.},\nbooktitle = {Proceedings of the 31st ACM International Conference on Multimedia},\npages = {5975–5984},\nnumpages = {10},\nkeywords = {eye movements, emotion recognition, eeg, dataset, continuous label},\nlocation = {Ottawa ON, Canada},\nseries = {MM '23}\n}\n```\nor\n```\n@ARTICLE{10731546,\n  author={Jiang, Wei-Bang and Liu, Xuan-Hao and Zheng, Wei-Long and Lu, Bao-Liang},\n  journal={IEEE Transactions on Affective Computing}, \n  title={SEED-VII: A Multimodal Dataset of Six Basic Emotions With Continuous Labels for Emotion Recognition}, \n  year={2025},\n  volume={16},\n  number={2},\n  pages={969-985},\n  keywords={Electroencephalography;Emotion recognition;Brain modeling;Physiology;Videos;Electrocardiography;Transformers;Recording;Computational modeling;Affective computing;Basic emotions;continuous label;EEG;emotion recognition;eye movements;multimodal dataset},\n  doi={10.1109/TAFFC.2024.3485057}}\n```\n", "metadata": {"source": "github_readmes\\935963004_MAET_README.md", "filename": "935963004_MAET_README.md", "type": "readme_full"}}
{"id": "AILab-CVC_M2PT_README.md", "paper_id": "AILab-CVC_M2PT_README", "text": "<h1 class=\"title\">Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities [CVPR 2024]</h1>\n\n<p align=\"center\" width=\"100%\">\n<img src=\"assets/banner.png\"  width=\"80%\" height=\"100%\">\n</p>\n\n<div align=\"center\">\n     <span class=\"author-block\">\n    <a href=\"https://invictus717.github.io/\" target=\"_blank\">Yiyuan Zhang</a><sup>1</sup>,</span>\n    <span class=\"author-block\">\n    <a href=\"https://dingxiaohan.xyz/\" target=\"_blank\">Xiaohan Ding</a><sup>2</sup>,\n    </span>\n    <span class=\"author-block\">\n    <a href=\"https://kxgong.github.io/\" target=\"_blank\">Kaixiong Gong</a><sup>1</sup>,</span>\n    <span class=\"author-block\">\n    </span>\n    <a href=\"https://geyixiao.com/\" target=\"_blank\">Yixiao Ge</a><sup>2</sup>,\n    </span>\n    <span class=\"author-block\">\n    <a href=\"https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en&oi=ao\" target=\"_blank\">Ying Shan</a><sup>2</sup>,\n    </span>\n    <span class=\"author-block\">\n    <a href=\"http://people.eecs.berkeley.edu/~xyyue/\" target=\"_blank\">Xiangyu Yue</a><sup>1</sup>\n    </span>\n</div>\n\n<div align=\"center\">\n    <sup>1</sup>\n    <a href='http://mmlab.ie.cuhk.edu.hk/' target='_blank'>The Chinese University of Hong Kong</a>&emsp;\n    <sup>2</sup>\n    <a href='https://ai.tencent.com/' target='_blank'>Tencent AI Lab</a>&emsp;\n</div>\n\n-----------------\n\n[![arXiv](https://img.shields.io/badge/arxiv-2401.14405-b31b1b?style=plastic&color=b31b1b&link=https%3A%2F%2Farxiv.org%2Fabs%2F2401.14405)](https://arxiv.org/abs/2401.14405)\n[![website](https://img.shields.io/badge/Project-Website-blue)](https://ailab-cvc.github.io/M2PT/)\n\n### Inspiration of Multimodal Pathway\nThis diagram's composition is inspired by a figure in Jeff Dean's blog post, where he envisions \"pathways\" as a high-level concept for general AI models. Our proposed Multimodal Pathway Transformer is a novel approach, and we are delighted to discover that some of its effects align with Jeff Dean's high-level vision, such as training a single model to *do many things, enabling multiple senses, and making models sparse and efficient*. Multimodal Pathway Transformer can be seen as an initial exploration of this \"pathways\" concept in the context of basic Transformer models and multimodal learning. Read more about Jeff Dean's concept in <a href=\"https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\" target=\"_blank\">his blog post</a>.\n\n### Abstract\nWe propose to improve transformers of a specific modality with irrelevant data from other modalities, *e.g*, improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (*e.g.* CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities.\n\n<p align=\"center\" width=\"100%\">\n<img src=\"assets/result.png\"  width=\"100%\" height=\"80%\">\n</p>\n\n## Model Zoo\n\n\n|      Model      |   Modality   | Pretraining | #Param |                                               Google Drive | Tencent Cloud                                               |\n| :------------: | :----------: | :----------------------: | :----: | :------: |:--------: | \n| ViT-B16  | Image |         MAE         |  86M  |   [ckpt](https://drive.google.com/file/d/1GG8W-n3rJwJKSfmFE7npARbeJTPYzDTo/view?usp=drive_link)    | [ckpt](https://share.weiyun.com/rFarvpGx)\n| Audio ViT-B16  | Audio |         Audio MAE         |  86M  |   [ckpt](https://drive.google.com/file/d/1xSelnHFBB27tZjOtP_m8ehRon3Ea8zlR/view?usp=drive_link)    | [ckpt](https://share.weiyun.com/QLoaFJyi)\n| Point ViT-B16  | Point |         Point MAE         |  86M  |   [ckpt](https://drive.google.com/file/d/1c3t4wcd34OU4E56tIjhOy91wiY3Pgt5_/view?usp=drive_link)    | [ckpt](https://share.weiyun.com/lnoMWqR8)\n| Video ViT-B16  | Video |         Video MAE         |  86M  |   [ckpt](https://drive.google.com/file/d/17FDpa7qUrPGv6NYyHsUrAH1x_ZyaxQ0g/view?usp=sharing)    | [ckpt](https://share.weiyun.com/QSYmzg0I)\n\n## Usage\n\n* Demo of use:\n\n    ```python\n    #  Create Model\n    import torch,timm\n    model = timm.create_model(\"vit_base_patch16_224\",pretrained = False)\n    aux_model = timm.create_model(\"vit_base_patch16_224\",pretrained = False)\n    #  Load Pretrained Models\n    pretrained_state_dict = torch.load(\"Image_ViT_B16.pth\")\n    aux_state_dict = torch.load(\"Audio_ViT_B16.pth\")\n    model.load_state_dict(pretrained_state_dict, strict=True)\n    aux_model.load_state_dict(aux_state_dict, strict=True)\n    #  Construct Multimodal Pathway\n    from multimodal_pathway import reparameterize_aux_into_target_model\n    reparameterize_aux_into_target_model(model, aux_model, layer_names=('attn.qkv', 'attn.proj', 'mlp.fc1','mlp.fc2'))\n    ```\n\n* For image recognition, please refer to [image doc](Image/README.md).\n\n* For video recognition, please refer to [video doc](Video/README.md).\n\n* For point cloud analysis, please refer to [pcd doc](Point/README.md).\n\n* For audio recognition, please refer to [audio doc](Audio/README.md).\n\n<section class=\"section\" id=\"BibTeX\">\n    <div class=\"container is-max-desktop content\">\n      <h2 class=\"title\">BibTeX</h2>\n      If you find our work useful, please kindly cite:\n      <pre><code>@article{zhang2024multimodal,\n      title={Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities},\n      author={Zhang, Yiyuan and Ding, Xiaohan and Gong, Kaixiong and Ge, Yixiao and Shan, Ying and Yue, Xiangyu},\n      journal={arXiv preprint arXiv:2401.14405},\n      year={2024}\n    }</code></pre>\n    </div>\n</section>\n", "metadata": {"source": "github_readmes\\AILab-CVC_M2PT_README.md", "filename": "AILab-CVC_M2PT_README.md", "type": "readme_full"}}
{"id": "aimagelab_ReT_README.md", "paper_id": "aimagelab_ReT_README", "text": "<div align=\"center\">\n\n  <h1>ReT: Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval (CVPR 2025) </h1>\n\n</div>\n\n\n\n<div align=\"center\">\n\n  \n\n  [![Paper](https://img.shields.io/badge/Paper-arxiv.2503.15621-B31B1B.svg)](https://www.arxiv.org/abs/2503.01980)\n\n  [![ReT](https://img.shields.io/badge/Checkpoints-🤗%20ReT-blue)](https://huggingface.co/collections/aimagelab/ret-67e15d4f9c60664d08ff8747)\n\n  [![Dataset](https://img.shields.io/badge/Dataset-🤗%20ReT--M2KR-blue)](https://huggingface.co/datasets/aimagelab/ReT-M2KR)\n\n\n\n\n\n</div>\n\n\n\n<p align=\"center\">\n\n  <img src=\"assets/model.png\" alt=\"ReT\" width=\"840\" />\n\n</p> \n\n\n\nPlease cite with the following BibTeX:\n\n```\n\n@inproceedings{caffagni2025recurrence,\n\n  title={{Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval}},\n\n  author={Caffagni, Davide and Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},\n\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n\n  year={2025}\n\n}\n\n```\n\n\n\n* ```12/09/2025``` 🔥🔥🔥 We release [ReT-2](https://github.com/aimagelab/ReT-2): Recurrence Meets Transformers for Universal Multimodal Retrieval\n\n\n\n## Installation\n\n1. Create the Python environment.\n\n```\n\nconda create -n ret -y --no-default-packages python==3.10.16\n\nconda activate ret\n\n```\n\n2. Install Pytorch.\n\n```\n\npip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118\n\n```\n\n3. Install faiss-gpu.\n\n```\n\nconda install -n ret -y -c conda-forge faiss-gpu==1.7.4\n\n```\n\n4. Clone the repo and install other dependencies.\n\n```\n\ngit clone https://github.com/aimagelab/ReT.git\n\ncd ReT\n\npip install -r requirements.txt\n\n```\n\n\n\n\n\n## Pre-trained models 🤗\n\nReT model checkpoints are available on [**Hugging Face**](https://huggingface.co/collections/aimagelab/ret-67e15d4f9c60664d08ff8747).\n\nYou can use these checkpoints directly for retrieval tasks or fine-tune them to suit your specific retrieval needs.\n\n\n\n### Available Checkpoints and Benchmark Results\n\n| Model         | WIT Recall@10 | IGLUE Recall@1 | KVQA Recall@5 | OVEN Recall@5 | LLaVA Recall@1 | InfoSeek Recall@5 | InfoSeek Pseudo Recall@5 | EVQA Recall@5 | EVQA Pseudo Recall@5 | OKVQA Recall@5 | OKVQA Pseudo Recall@5 |\n\n|---------------|---------------|----------------|------------------|---------------|----------------|---------------|----------------------|----------------|-----------------------|-------------------|--------------------------|\n\n| [ReT-CLIP-ViT-L-14🤗](https://huggingface.co/aimagelab/ReT-CLIP-ViT-L-14) | 0.734         | 0.818          | 0.635         | 0.820            | 0.799         | 0.470          | 0.605         | 0.445                | 0.579          | 0.202                 | 0.662             |\n\n| [ReT-OpenCLIP-ViT-H-14🤗](https://huggingface.co/aimagelab/ReT-OpenCLIP-ViT-H-14) | 0.714         | 0.800          | 0.593         | 0.830            | 0.798         | 0.473          | 0.607         | 0.448                | 0.578          | 0.182                 | 0.634             |\n\n| [ReT-OpenCLIP-ViT-G-14🤗](https://huggingface.co/aimagelab/ReT-OpenCLIP-ViT-G-14) | 0.751         | 0.822          | 0.606         | 0.840            | 0.792         | 0.520          | 0.625         | 0.486                | 0.602          | 0.190                 | 0.638             |\n\n\n\n\n\n## ReT-M2KR Dataset 🤗\n\n\n\nYou can download the ReT-M2KR benchmark by following the instructions provided [here](https://huggingface.co/datasets/aimagelab/ReT-M2KR). \n\nThis dataset is used for training and evaluating ReT in multimodal information retrieval and includes images (coming soon) and `JSONL` files.\n\n\n\nReT-M2KR benchmark is an extended version of the [M2KR dataset](https://huggingface.co/datasets/BByrneLab/multi_task_multi_modal_knowledge_retrieval_benchmark_M2KR), with the following modifications:\n\n\n\n- MSMARCO data is excluded, as it does not contain query images\n\n- Passage images have been added to the OVEN, InfoSeek, E-VQA, and OKVQA datasets\n\n\n\nFor further details, please refer to the associated research paper.\n\n\n\n\n\n\n\n## Use with Transformers\n\n```python\n\nfrom src.models import RetrieverModel, RetModel\n\nimport torch\n\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nretriever = RetrieverModel.from_pretrained('aimagelab/ReT-CLIP-ViT-L-14', device_map=device)\n\n\n\n# QUERY\n\nret: RetModel = retriever.get_query_model()\n\nret.init_tokenizer_and_image_processor()\n\nq_txt = \"Retrieve documents that provide an answer to the question alongside the image: What is the content of the image?\"\n\nq_img = 'assets/model.png'\n\n\n\nret_feats = ret.get_ret_features([[q_txt, q_img]])\n\nprint(ret_feats.shape)  # torch.Size([1, 32, 128])\n\n\n\n\n\n# PASSAGE\n\nret: RetModel = retriever.get_passage_model()\n\nret.init_tokenizer_and_image_processor()\n\n\n\np_txt = \"\"\"The image shows a diagram of what appears to be a neural network architecture using a fine-grained loss approach for multimodal learning.\n\nThe architecture has two parallel processing streams labeled \"ReTQ\" (left side, in purple) and \"ReTD\" (right side, in blue).\n\nEach side has: ...\"\"\"\n\np_img = ''\n\n\n\nret_feats = ret.get_ret_features([[p_txt, p_img]])\n\nprint(ret_feats.shape)  # torch.Size([1, 32, 128])\n\n```\n\n\n\n## Indexing and Searching\n\nTo evaluate ReT on the on M2KR benchmark, we provide SLURM script examples [here](./scripts). These scripts handle both indexing and searching processes.\n\n\n\nMake sure to set [`JSONL_ROOT_PATH`](https://github.com/aimagelab/ReT/blob/88abe2461106b07a047d57ccba32b7d2af52e3e1/scripts/inference_m2kr_large.sh#L37) and [`IMAGE_ROOT_PATH`](https://github.com/aimagelab/ReT/blob/88abe2461106b07a047d57ccba32b7d2af52e3e1/scripts/inference_m2kr_large.sh#L60) accordingly to the directories where the JSONL files and images have been downloaded.\n\n\n\n#### Known issue\n\nIf the inference script got stuck while indexing, try to clear the Pytorch cache and re-run\n\n```\n\nrm -rf ~/.cache/torch_extensions  \n\n```\n\n\n\n\n\n\n\n\n\n\n\n## Acknowledgments\n\nWe thank the teams behind [ColBERT](https://github.com/stanford-futuredata/ColBERT), [PreFLMR](https://github.com/LinWeizheDragon/FLMR), and [UniIR](https://github.com/TIGER-AI-Lab/UniIR) for open-sourcing their models, datasets, and code.\n\n\n\n", "metadata": {"source": "github_readmes\\aimagelab_ReT_README.md", "filename": "aimagelab_ReT_README.md", "type": "readme_full"}}
{"id": "AMAAI-Lab_Video2Music_README.md", "paper_id": "AMAAI-Lab_Video2Music_README", "text": "<div align=\"center\">\n\n# Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model\n\n\n\n[Demo](https://huggingface.co/spaces/amaai-lab/video2music) | [Website and Examples](https://amaai-lab.github.io/Video2Music/) | [Paper](https://doi.org/10.1016/j.eswa.2024.123640) | [Dataset (MuVi-Sync)](https://zenodo.org/records/10057093)\n\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/amaai-lab/video2music)  [![arXiv](https://img.shields.io/badge/arXiv-2311.00968-brightgreen.svg?style=flat-square)](https://arxiv.org/abs/2311.00968)\n\n</div>\n\nThis repository contains the code and dataset accompanying the paper \"Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model\" by Dr. Jaeyong Kang, Prof. Soujanya Poria, and Prof. Dorien Herremans.\n\n🔥 Live demo available on [HuggingFace](https://huggingface.co/spaces/amaai-lab/video2music) and [Replicate](https://replicate.com/amaai-lab/video2music).\n\n<div align=\"center\">\n  <img src=\"v2m.png\" width=\"400\"/>\n</div>\n\n## Introduction\nWe propose a novel AI-powered multimodal music generation framework called Video2Music. This framework uniquely uses video features as conditioning input to generate matching music using a Transformer architecture. By employing cutting-edge technology, our system aims to provide video creators with a seamless and efficient solution for generating tailor-made background music.\n\n![](framework.png)\n\n\n## Change Log\n- 2023-11-28: add new input method (YouTube URL) on HuggingFace\n\n## Quickstart Guide\n\nGenerate music from video:\n\n```python\nimport IPython\nfrom video2music import Video2music\n\ninput_video = \"input.mp4\"\n\ninput_primer = \"C Am F G\"\ninput_key = \"C major\"\n\nvideo2music = Video2music()\noutput_filename = video2music.generate(input_video, input_primer, input_key)\n\nIPython.display.Video(output_filename)\n```\n\n## Installation\n\nThis repo is developed using python version 3.8\n\n```bash\napt-get update\napt-get install ffmpeg\napt-get install fluidsynth\ngit clone https://github.com/AMAAI-Lab/Video2Music\ncd Video2Music\npip install -r requirements.txt\n```\n\n* Download the processed training data `AMT.zip` from [HERE](https://drive.google.com/file/d/1qpcBXF04pgdy9hqRexr0mTx7L9_CAFpt/view?usp=drive_link) and extract the zip file and put the extracted two files directly under this folder (`saved_models/AMT/`)\n\n* Download the soundfont file `default_sound_font.sf2` from [HERE](https://drive.google.com/file/d/1B9qjgimW9h6Gg5k8PZNt_ArWwSMJ4WuJ/view?usp=drive_link) and put the file directly under this folder (`soundfonts/`)\n\n* Our code is built on pytorch version 1.12.1 (torch==1.12.1 in the requirements.txt). But you might need to choose the correct version of `torch` based on your CUDA version\n\n## Dataset\n\n* Obtain the dataset:\n  * MuVi-Sync [(Link)](https://zenodo.org/records/10057093)\n \n* Put all directories started with `vevo` in the dataset under this folder (`dataset/`) \n\n## Directory Structure\n\n* `saved_models/`: saved model files\n* `utilities/`\n  * `run_model_vevo.py`: code for running model (AMT)\n  * `run_model_regression.py`: code for running model (bi-GRU)\n* `model/`\n  * `video_music_transformer.py`: Affective Multimodal Transformer (AMT) model \n  * `video_regression.py`: Bi-GRU regression model used for predicting note density/loudness\n  * `positional_encoding.py`: code for Positional encoding\n  * `rpr.py`: code for RPR (Relative Positional Representation)\n* `dataset/`\n  * `vevo_dataset.py`: Dataset loader\n* `script/` : code for extracting video/music features (sementic, motion, emotion, scene offset, loudness, and note density)\n* `train.py`: training script (AMT)\n* `train_regression.py`: training script (bi-GRU)\n* `evaluate.py`: evaluation script\n* `generate.py`: inference script\n* `video2music.py`: Video2Music module that outputs video with generated background music from input video\n* `demo.ipynb`: Jupyter notebook for Quickstart Guide\n\n## Training\n\n  ```shell\n  python train.py\n  ```\n\n## Inference\n\n  ```shell\n  python generate.py\n  ```\n\n\n## Subjective Evaluation by Listeners\n\n| **Model** | **Overall Music Quality** ↑ | **Music-Video Correspondence** ↑ | **Harmonic Matching** ↑ | **Rhythmic Matching** ↑ | **Loudness Matching** ↑ |\n|--------------------|:-----------:|:----------:|:----------:|:----------:|:----------:|\n| Music Transformer  | 3.4905      | 2.7476     | 2.6333     | 2.8476     | 3.1286     |\n| Video2Music        | 4.2095      | 3.6667     | 3.4143     | 3.8714     | 3.8143     |\n\n\n## TODO\n\n- [ ] Add other instruments (e.g., drum) for live demo\n\n## Citation\nIf you find this resource useful, [please cite the original work](https://doi.org/10.1016/j.eswa.2024.123640):\n\n```bibtex\n@article{KANG2024123640,\n  title = {Video2Music: Suitable music generation from videos using an Affective Multimodal Transformer model},\n  author = {Jaeyong Kang and Soujanya Poria and Dorien Herremans},\n  journal = {Expert Systems with Applications},\n  pages = {123640},\n  year = {2024},\n  issn = {0957-4174},\n  doi = {https://doi.org/10.1016/j.eswa.2024.123640},\n}\n```\n\nKang, J., Poria, S. & Herremans, D. (2024). Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model, Expert Systems with Applications (in press).\n\n\n## Acknowledgements\n\nOur code is based on [Music Transformer](https://github.com/gwinndr/MusicTransformer-Pytorch).\n\n\n", "metadata": {"source": "github_readmes\\AMAAI-Lab_Video2Music_README.md", "filename": "AMAAI-Lab_Video2Music_README.md", "type": "readme_full"}}
{"id": "Angknpng_SACNet_README.md", "paper_id": "Angknpng_SACNet_README", "text": "# SACNet\nCode repository for our paper entilted \"Alignment-Free RGBT Salient Object Detection: Semantics-guided Asymmetric Correlation Network and A Unified Benchmark\" accepted at TMM 2024.\n\narXiv version: https://arxiv.org/abs/2406.00917.\n\n## :tada: **News** :tada:  (December, 2024)\n\nWe are excited to announce that our new work **\"Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and Progressive Correlation Network\"** has been accepted to **AAAI 2025**! :link: [GitHub Repository: PCNet](https://github.com/Angknpng/PCNet)\n\nThis is also part of our ongoing **Alignment-Free RGB-T Salient Object Detection** series. Stay tuned for updates regarding code and resources related to this new work. 🚀\n\n## Citing our work\n\nIf you think our work is helpful, please cite\n\n```\n@article{Wang2024alignment,\n  title={Alignment-Free RGBT Salient Object Detection: Semantics-guided Asymmetric Correlation Network and A Unified Benchmark},\n  author={Wang, Kunpeng and Lin, Danying and Li, Chenglong and Tu, Zhengzheng and Luo, Bin},\n  journal={IEEE Transactions on Multimedia},\n  year={2024}\n}\n```\n## The Proposed Unaligned RGBT Salient Object Detection Dataset\n\n### UVT2000\n\nWe construct a novel benchmark dataset, containing 2000 unaligned visible-thermal image pairs directly captured from various real-word scenes, to facilitate research on alignment-free RGBT SOD.\n\n[![avatar](https://github.com/Angknpng/SACNet/raw/main/figures/dataset_sample.png)](https://github.com/Angknpng/SACNet/blob/main/figures/dataset_sample.png)\n\nThe proposed dataset link can be found here. [[baidu pan](https://pan.baidu.com/s/1tLYnRAMXANvEB4qUM1jZgw?pwd=nitk) fetch code: nitk] or [[google drive](https://drive.google.com/drive/folders/1Rm-zZRIAJmBhyS71WGKVL4IsrziR70bo?usp=drive_link)]\n\n### Dataset Statistics and Comparisons\n\nWe analyze the proposed UVT2000 datset from several statistical aspects and also conduct a comparison between UVT2000 and other existing multi-modal SOD datasets.\n\n[![avatar](https://github.com/Angknpng/SACNet/raw/main/figures/dataset_compare.png)](https://github.com/Angknpng/SACNet/blob/main/figures/dataset_compare.png)\n\n## Overview\n### Framework\n[![avatar](https://github.com/Angknpng/SACNet/raw/main/figures/framework.png)](https://github.com/Angknpng/SACNet/blob/main/figures/framework.png)\n### RGB-T SOD Performance\n[![avatar](https://github.com/Angknpng/SACNet/raw/main/figures/performance_RGBT.png)](https://github.com/Angknpng/SACNet/blob/main/figures/performance_RGBT.png)\n### RGB-D SOD Performance\n[![avatar](https://github.com/Angknpng/SACNet/raw/main/figures/performance_RGBD.png)](https://github.com/Angknpng/SACNet/blob/main/figures/performance_RGBD.png)\n### RGB SOD Performance\n[![avatar](https://github.com/Angknpng/SACNet/raw/main/figures/performance_RGB.png)](https://github.com/Angknpng/SACNet/blob/main/figures/performance_RGB.png)\n\n## Predictions\n\nRGB-T saliency maps can be found here. [[baidu pan](https://pan.baidu.com/s/1uuDHAh9TTQ4N9cJIl-p9OQ?pwd=kgej) fetch code: kgej] or [[google drive](https://drive.google.com/drive/folders/1fBS4GMBS5qja8pzg7ZCQd1AxmEC8rMun?usp=drive_link\n)]\n\nRGB-D saliency maps can be found here. [[baidu pan](https://pan.baidu.com/s/15V1RYOUDFnx2w-GXDjzv8Q?pwd=43bk) fetch code: 43bk] or [[google drive](https://drive.google.com/drive/folders/1fBS4GMBS5qja8pzg7ZCQd1AxmEC8rMun?usp=drive_link\n)]\n\nRGB saliency maps can be found here. [[baidu pan](https://pan.baidu.com/s/1MMmOOrTG7D_o3iDCihpdCQ?pwd=8ug9) fetch code: 8ug9] or [[google drive](https://drive.google.com/drive/folders/1fBS4GMBS5qja8pzg7ZCQd1AxmEC8rMun?usp=drive_link\n)]\n\nResNet50-based saliency maps can be found here. [[baidu pan](https://pan.baidu.com/s/14HFp58DmvjQHrLNSqz7uwA?pwd=i8xg) fetch code: i8xg]\n\nResNet50-based checkpoints can be found here. [[baidu pan](https://pan.baidu.com/s/1Gkkp3R7gq4-Slxqfr0BeVw?pwd=istd) fetch code: istd]\n\n## Pretrained Models\nThe pretrained parameters of our models can be found here. [[baidu pan](https://pan.baidu.com/s/177h2BnBwJ2C81qwVeC4Z0g?pwd=f2x5) fetch code: f2x5] or [[google drive](https://drive.google.com/drive/folders/1fBS4GMBS5qja8pzg7ZCQd1AxmEC8rMun?usp=drive_link\n)]\n\n## Usage\n\n### Requirement\n\n0. Download the datasets for training and testing from here. [[baidu pan](https://pan.baidu.com/s/1RE48go1wzGWymMblawG2wQ?pwd=vvgq) fetch code: vvgq]\n1. Download the pretrained parameters of the backbone from here. [[baidu pan](https://pan.baidu.com/s/1sBuu7Qw9n8aWRydQsDieBA?pwd=3ifw) fetch code: 3ifw]\n2. Create directories for the experiment and parameter files.\n3. Please use `conda` to install `torch` (1.12.0) and `torchvision` (0.13.0).\n4. Install other packages: `pip install -r requirements.txt`.\n5. Set your path of all datasets in `./Code/utils/options.py`.\n\n### Train\n\n```\npython -m torch.distributed.launch --nproc_per_node=2 --master_port=2212 train_parallel.py\n```\n\n### Test\n\n```\npython test_produce_maps.py\n```\n\n## Acknowledgement\n\nThe implement of this project is based on the following link.\n\n- [SOD Literature Tracking](https://github.com/jiwei0921/SOD-CNNs-based-code-summary-)\n- [PR Curve](https://github.com/lartpang/PySODEvalToolkit)\n- [Computational complexity test](https://github.com/yuhuan-wu/MobileSal)\n\n## Contact\n\nIf you have any questions, please contact us (kp.wang@foxmail.com).\n", "metadata": {"source": "github_readmes\\Angknpng_SACNet_README.md", "filename": "Angknpng_SACNet_README.md", "type": "readme_full"}}
{"id": "breezedeus_Coin-CLIP_README.md", "paper_id": "breezedeus_Coin-CLIP_README", "text": "<div align=\"center\">\n  <img src=\"./docs/images/coin-clip-logo.png\" width=\"160px\"/>\n  <div>&nbsp;</div>\n\n[![Downloads](https://static.pepy.tech/personalized-badge/coin_clip?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads)](https://pepy.tech/project/coin_clip)\n[![license](https://img.shields.io/github/license/breezedeus/coin-clip)](./LICENSE)\n[![PyPI version](https://badge.fury.io/py/coin_clip.svg)](https://badge.fury.io/py/coin_clip)\n[![forks](https://img.shields.io/github/forks/breezedeus/coin-clip)](https://github.com/breezedeus/coin-clip)\n[![stars](https://img.shields.io/github/stars/breezedeus/coin-clip)](https://github.com/breezedeus/coin-clip)\n![last-releast](https://img.shields.io/github/release-date/breezedeus/coin-clip)\n![last-commit](https://img.shields.io/github/last-commit/breezedeus/coin-clip)\n[![Twitter](https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2Fbreezedeus)](https://twitter.com/breezedeus)\n\n[🧳 Model](https://huggingface.co/breezedeus/coin-clip-vit-base-patch32) |\n[🛀🏻 Online Demo](https://huggingface.co/spaces/breezedeus/USA-Coin-Retrieval) |\n[💬 Contact](https://www.breezedeus.com/join-group)\n\n</div>\n\n <div align=\"center\">\n <strong>\n\n[[中文]](./README_cn.md) | [English]\n\n </strong>\n</div>\n\n# Coin-CLIP 🪙 : Enhancing Coin Image Retrieval with CLIP\n\n**[Coin-CLIP](https://huggingface.co/breezedeus/coin-clip-vit-base-patch32)** `breezedeus/coin-clip-vit-base-patch32` is built upon \nOpenAI's **[CLIP](https://huggingface.co/openai/clip-vit-base-patch32) (ViT-B/32)** model and fine-tuned on \na dataset of more than `340,000` coin images using contrastive learning techniques. This specialized model is designed to significantly improve feature extraction for coin images, leading to more accurate image-based search capabilities. Coin-CLIP combines the power of Visual Transformer (ViT) with CLIP's multimodal learning capabilities, specifically tailored for the numismatic domain.\n\n**Key Features:**\n- State-of-the-art coin image retrieval;\n- Enhanced feature extraction for numismatic images;\n- Seamless integration with CLIP's multimodal learning.\n\n\nTo further simplify the use of the **Coin-CLIP** model, this project provides tools for quickly building a coin image retrieval engine.\n\n# Comparison: Coin-CLIP vs. CLIP\n\n#### Example 1 (Left: Coin-CLIP; Right: CLIP)\n\n![1. Coin-CLIP vs. CLIP](./docs/images/3-c.gif)\n\n#### Example 2 (Left: Coin-CLIP; Right: CLIP)\n\n![2. Coin-CLIP vs. CLIP](./docs/images/5-c.gif)\n\n#### More Examples\n\n<details>\n\n<summary>more</summary>\n\nExample 3 (Left: Coin-CLIP; Right: CLIP)\n![3. Coin-CLIP vs. CLIP](./docs/images/1-c.gif)\n\nExample 4 (Left: Coin-CLIP; Right: CLIP)\n![4. Coin-CLIP vs. CLIP](./docs/images/4-c.gif)\n\nExample 5 (Left: Coin-CLIP; Right: CLIP)\n![5. Coin-CLIP vs. CLIP](./docs/images/2-c.gif)\n\nExample 6 (Left: Coin-CLIP; Right: CLIP)\n![6. Coin-CLIP vs. CLIP](./docs/images/6-c.gif)\n\n</details>\n\n# Install\n\n```bash\npip install coin_clip\n```\n\n# Usage\n## Code Examples\n\n### Extract Feature Vectors from Coin Images\n\n```python\nfrom coin_clip import CoinClip\n\n# Automatically download the model from Huggingface\nmodel = CoinClip(model_name='breezedeus/coin-clip-vit-base-patch32')\nimages = ['examples/10_back.jpg', 'examples/16_back.jpg']\nimg_feats, success_ids = model.get_image_features(images)\nprint(img_feats.shape)  # --> (2, 512)\n```\n\n> ⚠️ **Note**:\n> \n> The above code automatically downloads the [`breezedeus/coin-clip-vit-base-patch32`](https://huggingface.co/breezedeus/coin-clip-vit-base-patch32) model from Huggingface.\nIf you cannot download automatically, please manually download the model locally, and then initialize `CoinClip` by specifying the local directory of the model through the `model_name` parameter, like `model_name='path/to/coin-clip-vit-base-patch32'`.\n\n## Command line tools\n\n### Building a Vector Retrieval Engine\n\n`coin-clip build-db` can be used to build a vector search engine. It extracts features from all coin images 🪙 in a specified directory and builds a ChromaDB vector search engine.\n\n```bash\n$ coin-clip build-db -h\nUsage: coin-clip build-db [OPTIONS]\n\n  Extract vectors from a candidate image set and build a search engine based\n  on it.\n\nOptions:\n  -m, --model-name TEXT       Model Name; either local path or huggingface\n                              model name  [default: breezedeus/coin-clip-vit-\n                              base-patch32]\n  -d, --device TEXT           ['cpu', 'cuda']; Either 'cpu' or 'gpu', or\n                              specify a specific GPU like 'cuda:0'. Default is\n                              'cpu'.  [default: cpu]\n  -i, --input-image-dir TEXT  Folder with Coin Images to be indexed. [required]\n  -o, --output-db-dir TEXT    Folder where the built search engine is stored.\n                              [default: ./coin_clip_chroma.db]\n  -h, --help                  Show this message and exit.\n```\n\nFor instance, \n\n```bash\n$ coin-clip build-db -i examples -o coin_clip_chroma.db\n```\n\n### Querying\nAfter building the vector search engine with the above command, you can use the `coin-clip retrieve` command to retrieve the coin images 🪙 most similar to a specified coin image.\n\n```bash\n$ coin-clip retrieve -h\nUsage: coin-clip retrieve [OPTIONS]\n\n  Retrieve images from the search engine, based on the query image.\n\nOptions:\n  -m, --model-name TEXT  Model Name; either local path or huggingface model\n                         name  [default: breezedeus/coin-clip-vit-base-\n                         patch32]\n  -d, --device TEXT      ['cpu', 'cuda']; Either 'cpu' or 'gpu', or specify a\n                         specific GPU like 'cuda:0'. Default is 'cpu'.\n                         [default: cpu]\n  --db-dir TEXT          Folder where the built search engine is stored.\n                         [default: ./coin_clip_chroma.db]\n  -i, --image-fp TEXT    Image Path to retrieve  [required]\n  -h, --help             Show this message and exit.\n```\n\nFor instance, \n\n```bash\n$ coin-clip retrieve --db-dir coin_clip_chroma.db -i examples/10_back.jpg\n```\n\n\n## A cup of coffee for the author\n\nIt is not easy to maintain and evolve the project, so if it is helpful to you, please consider [offering the author a cup of coffee 🥤](https://www.breezedeus.com/buy-me-coffee).\n\n---\n\nOfficial code base: [https://github.com/breezedeus/coin-clip](https://github.com/breezedeus/coin-clip). Please cite it properly.", "metadata": {"source": "github_readmes\\breezedeus_Coin-CLIP_README.md", "filename": "breezedeus_Coin-CLIP_README.md", "type": "readme_full"}}
{"id": "bubaimaji_cmt-mser_README.md", "paper_id": "bubaimaji_cmt-mser_README", "text": "# cmt-mser\n\nThis repo contains the code for implementation of the paper \"MULTIMODAL EMOTION RECOGNITION BASED ON DEEP TEMPORAL FEATURES USING CROSS-MODAL TRANSFORMER AND SELF-ATTENTION\" Accepted in ICASSP 2023. \n\nThe architecture of our proposed model\n\n![Model](https://user-images.githubusercontent.com/42870654/198873440-3776f195-bc26-4df1-8f8c-3b01acc341e5.jpg)\n\n\n\n__Dataset__\n\nIn this experiment, we used [IEMOCAP](https://sail.usc.edu/iemocap/) dataset. \n", "metadata": {"source": "github_readmes\\bubaimaji_cmt-mser_README.md", "filename": "bubaimaji_cmt-mser_README.md", "type": "readme_full"}}
{"id": "Cassie07_PathOmics_README.md", "paper_id": "Cassie07_PathOmics_README", "text": "# PathOmics: Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction\n\nThe official code of \"Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction\" (Accepted to MICCAI2023, top 9%).\n\n### <b> Our Paper </b> [[Link](https://rdcu.be/dnwKf)]\n\n### <b> [2025.03 New Update!!!] We updated the [paper list](https://github.com/Cassie07/PathOmics#literature-reviews-of-pathology-and-genomics-multimodal-analysis-approaches-in-healthcare) of pathology-and-genomics multimodal analysis approaches in healthcare at the end of this repo. </b>\n\n## Workflow overview of the PathOmics\n<p align=\"center\">\n  <img src=\"https://github.com/Cassie07/PathOmics/blob/main/Figures/Figure1.png\" width=\"674.1\" height=\"368.3\" title=\"Figure1\">\n</p>\n\n<b> Workflow overview of the pathology-and-genomics multimodal transformer (PathOmics) for survival prediction. </b> In (a), we show the pipeline of extracting image and genomics feature embedding via an unsupervised pretraining towards multimodal data fusion. In (b) and (c), our supervised finetuning scheme could flexibly handle multiple types of data for prognostic prediction. With the multimodal pretrained model backbones, both multi- or single-modal data can be applicable for our model fine-tuning.\n\n## Citation\n```\n@inproceedings{ding2023pathology,\n  title={Pathology-and-genomics multimodal transformer for survival outcome prediction},\n  author={Ding, Kexin and Zhou, Mu and Metaxas, Dimitris N and Zhang, Shaoting},\n  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\n  pages={622--631},\n  year={2023},\n  organization={Springer}\n}\n```\n\n## Prerequisites\n```\npython 3.8.18\nPytorch 2.0.1\npytorch-cuda 11.8\nTorchvision 0.15.2\nPillow 9.4.0\nnumpy 1.24.3\npandas 2.0.3\nscikit-survival 0.21.0 \nscikit-learn 1.2.0\nh5py 2.8.0\n```\n## Usage\n### Data prerpocessing\n```\n1. Download WSIs from TCGA-COAD and TCGA-READ.\n2. Download genomics data from CbioPortal and move the downloaded folder into \"PathOmics\" folder.\n* \"coadread_tcga_pan_can_atlas_2018\" in `bash_main.py` and `bash_main_read.py` is the downloaded folder, please download it before you run the code.\n3. Split WSIs into patches and only keep the foreground patches.\n4. Extract patch features via pretrained models (e.g., ImageNet-pretrained ResNet50, ResNet101, etc).\n5. Save patch features as .npz files. (For each slide, we generate one .npz file to save patch features).\n```\nFor more details about extracting feature, please check [Issue 1](https://github.com/Cassie07/PathOmics/issues/1) and the code in split_tiles_utils/helper.py\n\n### Run code on TCGA-COAD only\nModel will be pretrained and finetuned on theTCGA-COAD training set (4-fold cross-validation).\nThe finetuned model will be evaluated on the TCGA-COAD hold-out set.\n\n```\npython bash_main.py --pretrain_loss 'MSE' --save_model_folder_name 'reproduce_experiments' --experiment_folder_name 'COAD_reproduce' --omic_modal 'miRNA' --kfold_split_seed 42 --pretrain_epochs 25 --finetune_epochs 25 --model_type 'PathOmics' --model_fusion_type 'concat' --model_pretrain_fusion_type 'concat' --cuda_device '2' --experiment_id '1' --use_GAP_in_pretrain_flag --seperate_test\n```\n### Run code on TCGA-COAD and TCGA-READ\nModel will be pretrained on TCGA-COAD (5-fold cross-validation).\nModel will be finetuned, validated, and evaluated on the TCGA-READ dataset.\n\n```\npython bash_main_read.py --k_fold 5 --fusion_mode 'concat' --prev_fusion_mode 'concat' --pretrain_loss 'MSE' --save_model_folder_name 'reproduce_experiments' --experiment_folder_name 'READ_reproduce' --omic_modal 'miRNA' --kfold_split_seed 42 --pretrain_epochs 25 --finetune_epochs 25 --model_type 'PathOmics' --cuda_device '2' --experiment_id '1' --use_GAP_in_pretrain_flag\n```\n\nIf you want to use TCGA-COAD pretrain weights and skip the pretraining stage, please add `--load_model_finetune` into your script.\nPlease modify the code to ensure your pretrain weights saving directory is correct.\n\n### Use data-efficient mode in finetuning stage\nPlease add `--less_data` into your script and set `--finetune_test_ratio` as your preferred ratio for indicating the ratio of data used for model finetuning.\n\n## Literature reviews of pathology-and-genomics multimodal analysis approaches in healthcare.\n|Publish Date|Title|Paper Link|Code|\n|---|---|---|---|\n|2025.01|Histo-Genomic Knowledge Association For Cancer Prognosis From Histopathology Whole Slide Images|[TMI](https://ieeexplore.ieee.org/abstract/document/10830530)|[Code](https://github.com/ZacharyWang-007/G-HANet)|\n|2024.10|MoME: Mixture of Multimodal Experts for Cancer Survival Prediction|[MICCAI 2024](https://papers.miccai.org/miccai-2024/paper/2168_paper.pdf)|[Code](https://github.com/BearCleverProud/MoME)|\n|2024.10|PG-MLIF: Multimodal Low-rank Interaction Fusion Framework Integrating Pathological Images and Genomic Data for Cancer Prognosis Prediction|[MICCAI 2024](https://papers.miccai.org/miccai-2024/paper/1221_paper.pdf)|[Code](https://github.com/panxipeng/PG-MLIF)|\n|2024.10|Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide Pathological Images|[MICCAI 2024](https://papers.miccai.org/miccai-2024/paper/1280_paper.pdf)|[Code](https://github.com/jsh0792/MCTI)|\n|2024.10|HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction|[MICCAI 2024](https://papers.miccai.org/miccai-2024/paper/0796_paper.pdf)|NA|\n|2024.07|A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model|[ArXiv](https://arxiv.org/abs/2407.15362)|NA|\n|2024.06|Transcriptomics-guided Slide Representation Learning in Computational Pathology|[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/html/Jaume_Transcriptomics-guided_Slide_Representation_Learning_in_Computational_Pathology_CVPR_2024_paper.html)|[Code](https://github.com/mahmoodlab/TANGLE)|\n|2024.06|Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction|[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Jaume_Modeling_Dense_Multimodal_Interactions_Between_Biological_Pathways_and_Histology_for_CVPR_2024_paper.pdf)|[Code](https://github.com/mahmoodlab/SurvPath)|\n|2023.10|Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency for Survival Prediction|[ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Multimodal_Optimal_Transport-based_Co-Attention_Transformer_with_Global_Structure_Consistency_for_ICCV_2023_paper.pdf)|[Code](https://github.com/Innse/MOTCat)|\n|2023.10|Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction|[MICCAI 2023](https://rdcu.be/dnwKf)|[Code](https://github.com/Cassie07/PathOmics)|\n|2023.10|Gene-induced Multimodal Pre-training for Image-omic Classification|[MICCAI 2023](https://link.springer.com/epdf/10.1007/978-3-031-43987-2_49?sharing_token=o-LUpRggl7nGR4-4H-Carve4RwlQNchNByi7wbcMAY7jxNo0bliUewITgRTD3ZK5Fj6WT7MCkvR1cQgUKw8y56vn3M_rNLfRgMpLN1Ln7rytbyfCglj7k-ImPaGGbBOVSA9qHdi0XnhwS27mLQ9ueSU11llzx5ZGz7eglf8kIjc%3D)|[Code](https://github.com/huangwudiduan/GIMP)|\n|2023.10|Cross-Modal Translation and Alignment for Survival Analysis|[ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Cross-Modal_Translation_and_Alignment_for_Survival_Analysis_ICCV_2023_paper.pdf)|[Code](https://github.com/FT-ZHOU-ZZZ/CMTA)|\n|2023.07|Assessment of emerging pretraining strategies in interpretable multimodal deep learning for cancer prognostication|[BioData Mining](https://link.springer.com/article/10.1186/s13040-023-00338-w)|NA|\n|2023.04|Multimodal data fusion for cancer biomarker discovery with deep learning|[Nature Machine Intelligence](https://link.springer.com/article/10.1186/s13040-023-00338-w)|NA|\n|2023.03|Hierarchical multimodal fusion framework based on noisy label learning and attention mechanism for cancer classification with pathology and genomic features|[Computerized Medical Imaging and Graphics](https://www.sciencedirect.com/science/article/pii/S089561112200146X)|NA|\n|2023.03|Hybrid Graph Convolutional Network With Online Masked Autoencoder for Robust Multimodal Cancer Survival Prediction|[TMI](https://ieeexplore.ieee.org/abstract/document/10061470)|[Code](https://github.com/lin-lcx/HGCN)|\n|2023.01|Multimodal deep learning to predict prognosis in adult and pediatric brain tumors|[Communications Medicine](https://www.nature.com/articles/s43856-023-00276-y)|[Code](https://github.com/gevaertlab/MultiModalBrainSurvival)|\n|2023.01|Survival Prediction for Gastric Cancer via Multimodal Learning of Whole Slide Images and Gene Expression|[BIBM 2022](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9995480)|[Code](https://github.com/constantjxyz/GC-SPLeM)|\n|2023.01|Deep Biological Pathway Informed Pathology-Genomic Multimodal Survival Prediction|[arXiv](https://arxiv.org/pdf/2301.02383.pdf)|NA|\n|2022.09|Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomic, and Demographic Data|[MICCAI 2022](https://rdcu.be/cVRze)|[Code](https://github.com/cuicathy/MMD_SurvivalPrediction)|\n|2022.09|Discrepancy and gradient guided multi-modal knowledge distillation for pathological glioma grading|[MICCAI 2022](https://rdcu.be/cVRzf)|[Code](https://github.com/CityU-AIM-Group/MultiModal-learning)|\n|2022.08|Multimodal integration of radiology, pathology and genomics for prediction of response to PD-(L)1 blockade in patients with non-small cell lung cancer|[Nature Cancer](https://www.nature.com/articles/s43018-022-00416-8)|[Code](https://github.com/msk-mind/luna/)|\n|2022.08|Pan-cancer integrative histology-genomic analysis via multimodal deep learning|[Cancer Cell](https://www.sciencedirect.com/science/article/pii/S1535610822003178?via%3Dihub)|[Code](https://github.com/mahmoodlab/PORPOISE)|\n|2022.03|HFBSurv: hierarchical multimodal fusion with factorized bilinear models for cancer survival prediction|[Bioinformatics](https://academic.oup.com/bioinformatics/article/38/9/2587/6533437#401769436)|[Code](https://github.com/Liruiqing-ustc/HFBSurv)|\n|2021.10|Multimodal co-attention transformer for survival prediction in gigapixel whole slide images|[ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Multimodal_Co-Attention_Transformer_for_Survival_Prediction_in_Gigapixel_Whole_Slide_ICCV_2021_paper.html?ref=https://githubhelp.com)|[Code](https://github.com/mahmoodlab/MCAT)|\n|2021.09|Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data|[MICCAI 2021](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_64)|NA|\n|2020.09|Pathomic fusion: an integrated framework for fusing histopathology and genomic features for cancer diagnosis and prognosis|[TMI](https://ieeexplore.ieee.org/abstract/document/9186053)|[Code](https://github.com/mahmoodlab/PathomicFusion)|\n|2020.08|Applying Machine Learning for Integration of Multi-Modal Genomics Data and Imaging Data to Quantify Heterogeneity in Tumour Tissues|[Artificial Neural Networks](https://link.springer.com/protocol/10.1007/978-1-0716-0826-5_10)|NA|\n|2019.09|Multimodal Multitask Representation Learning for Pathology Biobank Metadata Prediction|[arXiv](https://arxiv.org/pdf/1909.07846.pdf)|NA|\n|2019.07|Deep learning with multimodal representation for pancancer prognosis prediction|[Bioinformatics](https://academic.oup.com/bioinformatics/article/35/14/i446/5529139)|[Code](https://github.com/gevaertlab/MultimodalPrognosis)|\n|2019.06|Integrative Analysis of Pathological Images and Multi-Dimensional Genomic Data for Early-Stage Cancer Prognosis|[TMI](https://ieeexplore.ieee.org/abstract/document/8727966)|[Code](https://github.com/gevaertlab/MultimodalPrognosis)|\n", "metadata": {"source": "github_readmes\\Cassie07_PathOmics_README.md", "filename": "Cassie07_PathOmics_README.md", "type": "readme_full"}}
{"id": "chengjunyan1_SP-Transformer_README.md", "paper_id": "chengjunyan1_SP-Transformer_README", "text": "# Sparse Phased Transformer (SPT)\n\n\n\nThis is the official repo for the EMNLP 2021 paper \"[Multimodal Phased Transformer for Sentiment Analysis](https://aclanthology.org/2021.emnlp-main.189.pdf)\"\n\n\n\nPreprocessed MOSI and MOSEI dataset by MulT download in https://github.com/yaohungt/Multimodal-Transformer \n\nUR-FUNNY dataset download in https://github.com/ROC-HCI/UR-FUNNY \n\n\n\nUse run.py to run the model, use Optuna to search hyper-params.\n\n\n\n\n\n### Citation\n\nIf you use this code in your research, please cite the following paper:\n\n\n\n\n\n``` bibtex\n\n@inproceedings{cheng-etal-2021-multimodal,\n\n    title = \"Multimodal Phased Transformer for Sentiment Analysis\",\n\n    author = \"Cheng, Junyan  and\n\n      Fostiropoulos, Iordanis  and\n\n      Boehm, Barry  and\n\n      Soleymani, Mohammad\",\n\n    editor = \"Moens, Marie-Francine  and\n\n      Huang, Xuanjing  and\n\n      Specia, Lucia  and\n\n      Yih, Scott Wen-tau\",\n\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n\n    month = nov,\n\n    year = \"2021\",\n\n    address = \"Online and Punta Cana, Dominican Republic\",\n\n    publisher = \"Association for Computational Linguistics\",\n\n    url = \"https://aclanthology.org/2021.emnlp-main.189/\",\n\n    doi = \"10.18653/v1/2021.emnlp-main.189\",\n\n    pages = \"2447--2458\",\n\n    abstract = \"Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our model with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90{\\%} reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency.\"\n\n}\n\n```\n\n\n\n\n\n", "metadata": {"source": "github_readmes\\chengjunyan1_SP-Transformer_README.md", "filename": "chengjunyan1_SP-Transformer_README.md", "type": "readme_full"}}
{"id": "Cliu2_MTrans_README.md", "paper_id": "Cliu2_MTrans_README", "text": "# MTrans\nThe PyTorch implementation of [Multimodal Transformer for Automatic 3D Annotation and Object Detection](https://arxiv.org/abs/2207.09805), which has been accepted by ECCV2022.\n\n## Installation\nThe code has been tested on PyTorch v1.9.1. \n\nIoU loss is required for training. Before running, please install the IoU loss package following this [doc](https://github.com/Cliu2/MTrans/tree/main/loss#readme).\n\n## Data Preparation\nThe KITTI 3D detection dataset can be downloaded from the official webstie: [link](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d).\n\n## Train\nTo train a MTrans with the KITTI dataset. Simply run:\n> python train.py --cfg_file configs/MTrans_kitti.yaml\n\n## Trained Model\nTrained checkpoint can be downloaded from [here](https://connecthkuhk-my.sharepoint.com/:u:/g/personal/lcon7_connect_hku_hk/EYyRIedDNolHr3GSq0b3CZoBgsyI3XVjjtz4STD97WtKUQ?e=IhhNk4).\nAlthough we try to fix the random seeds, due to the randomness in some asynchronuous CUDA opearations and data preprocessing (e.g., point sampling), the result might not be exactly the same from run to run.\n\n## References\nThe IoU loss module is borrowed from \"https://github.com/lilanxiao/Rotated_IoU\". We thank the author for providing a neat implementation of the IoU loss.\n", "metadata": {"source": "github_readmes\\Cliu2_MTrans_README.md", "filename": "Cliu2_MTrans_README.md", "type": "readme_full"}}
{"id": "cosmaadrian_time-enriched-multimodal-depression-detection_README.md", "paper_id": "cosmaadrian_time-enriched-multimodal-depression-detection_README", "text": "## It’s Just a Matter of Time: Detecting Depression with Time-Enriched Multimodal Transformers\n\n### Ana-Maria Bucur, Adrian Cosma, Paolo Rosso and Liviu P. Dinu\n\n\n\nThis repository contains the official source code for the paper **\"[It’s Just a Matter of Time: Detecting Depression with Time-Enriched Multimodal Transformers](https://link.springer.com/chapter/10.1007/978-3-031-28244-7_13)\"**, accepted at the 2023 edition of European Conference on Information Retrieval (ECIR). \n\n\n\n[Presentation Slides](https://docs.google.com/presentation/d/1TmFzbmz93RinKHM246qA4OWO3fuuxYEoZBzBpaISdJM/edit?usp=sharing)\n\n\n\n### Abstract\n\n*Depression detection from user-generated content on the internet has been a long-lasting topic of interest in the research community, providing valuable screening tools for psychologists. The ubiquitous use of social media platforms lays out the perfect avenue for exploring mental health manifestations in posts and interactions with other users. Current methods for depression detection from social media mainly focus on text processing, and only a few also utilize images posted by users. In this work, we propose a flexible time-enriched multimodal transformer architecture for detecting depression from social media posts, using pretrained models for extracting image and text embeddings. Our model operates directly at the user-level, and we enrich it with the relative time between posts by using time2vec positional embeddings. Moreover, we propose another model variant, which can operate on randomly sampled and unordered sets of posts to be more robust to dataset noise. We show that our method, using EmoBERTa and CLIP embeddings, surpasses other methods on two multimodal datasets, obtaining state-of-the-art results of 0.931 F1 score on a popular multimodal Twitter dataset, and 0.902 F1 score on the only multimodal Reddit dataset.*\n\n\n\n![](images/MM-Depression-1.png)\n\n\n\n### Data\n\n\n\nThe Reddit and Twitter multimodal data used in our experiments are from the work of:\n\n\n\nUban, Ana-Sabina, Berta Chulvi, and Paolo Rosso. [Explainability of Depression Detection on Social Media: From Deep Learning Models to Psychological Interpretations and Multimodality](https://link.springer.com/chapter/10.1007/978-3-031-04431-1_13). In Early Detection of Mental Health Disorders by Social Media Monitoring, pp. 289-320. Springer, Cham, 2022.\n\n\n\nGui, Tao, Liang Zhu, Qi Zhang, Minlong Peng, Xu Zhou, Keyu Ding, and Zhigang Chen. [Cooperative Multimodal Approach to Depression Detection in Twitter](https://ojs.aaai.org/index.php/AAAI/article/view/3775). In Proceedings of the AAAI conference on Artificial Intelligence, vol. 33, no. 01, pp. 110-117. 2019.\n\n\n\nPlease contact the respective authors for accessing the data.\n\n\n\nAfter getting the data, two json files with the dates of the posts need to be created. For generating the files, the scripts  should be used: `scripts/format-reddit-2.py` and `scripts/format-twitter.py`.\n\n\n\n### Running experiments\n\n\n\nOur model definition can be found in the `models/` folder. The multimodal transformer model is based on [LXMERT](https://github.com/airsplay/lxmert) for the cross-encoder definition. The T-LSTM model is adapted from this [repo](https://github.com/duskybomb/tlstm). Time2Vec positional embeddings are adapted from this [repo](https://github.com/ojus1/Time2Vec-PyTorch).\n\n\n\nExperiments used to produce the results from the paper are defined in the bash script in `experiments/run_experiments.sh`.\n\n\n\n\n\nThis repo is based on [acumen-template](https://github.com/cosmaadrian/acumen-template) to organise the project, and uses [wandb.ai](https://wandb.ai/) for experiment tracking.\n\n\n\n### Results\n\n\n\nResults on Twitter data.\n\n\n\n![](images/table2.png)\n\n\n\nResults on Reddit data.\n\n\n\n![](images/table3.png)\n\n\n\n\n\nComparision between different types of text and images encoders.\n\n\n\n![](images/table4.png)\n\n\n\n\n\n### Citation\n\nIf you find our work useful, please cite us:\n\n\n\n```\n\n@InProceedings{10.1007/978-3-031-28244-7_13,\n\n  author=\"Bucur, Ana-Maria and Cosma, Adrian and Rosso, Paolo and Dinu, Liviu P.\",\n\n  title=\"It's Just a Matter of Time: Detecting Depression with Time-Enriched Multimodal Transformers\",\n\n  booktitle=\"Advances in Information Retrieval\",\n\n  year=\"2023\",\n\n  publisher=\"Springer Nature Switzerland\",\n\n  address=\"Cham\",\n\n  pages=\"200--215\",\n\n  isbn=\"978-3-031-28244-7\"\n\n}\n\n```\n\n", "metadata": {"source": "github_readmes\\cosmaadrian_time-enriched-multimodal-depression-detection_README.md", "filename": "cosmaadrian_time-enriched-multimodal-depression-detection_README.md", "type": "readme_full"}}
{"id": "CrawlScript_MIG-GT_README.md", "paper_id": "CrawlScript_MIG-GT_README", "text": "# MIG-GT\n\nSource code and dataset of the paper \"[Modality-Independent Graph Neural Networks with Global Transformers for Multimodal Recommendation](https://arxiv.org/abs/2412.13994)\", which is accepted by AAAI 2025.\n\n\n\n## Homepage and Paper\n\n+ Homepage (MIG-GT): https://github.com/CrawlScript/MIG-GT\n+ Paper Access:\n    - **ArXiv**: [https://arxiv.org/abs/2412.13994](https://arxiv.org/abs/2412.13994) \n\n\n\n## Dataset\n\nThe dataset is the same as the one used in the paper 'A Tale of Two Graphs: Freezing and Denoising Graph Structures for Multimodal Recommendation.' Please refer to their official repository to download the pre-processed dataset, which should be placed in the data directory.\n\n\n## Requirements\n\n+ Linux\n+ Python 3.7\n+ torch==1.12.1+cu113\n+ torchmetrics==0.11.4\n+ dgl==1.0.2+cu113\n+ ogb==1.3.5\n+ shortuuid==1.0.11\n+ pandas==1.3.5\n+ numpy==1.21.6\n+ tqdm==4.64.1\n\n\n\n## RUN\n\n```bash\n# Run the following command:\npython main.py --gpu 0 --seed 1 --dataset $DATASET --result_dir results --method mig_gt\n# Note: $DATASET can be 'baby', 'sports', or 'clothing'.\n```\n\n\n\n\n__License:__ [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html)\n\nCopyright (c) 2023-2024 Xtra Computing Group, NUS, Singapore.\n\n", "metadata": {"source": "github_readmes\\CrawlScript_MIG-GT_README.md", "filename": "CrawlScript_MIG-GT_README.md", "type": "readme_full"}}
{"id": "cshizhe_VLN-HAMT_README.md", "paper_id": "cshizhe_VLN-HAMT_README", "text": "# History Aware Multimodal Transformer for Vision-and-Language Navigation\n\nThis repository is the official implementation of [History Aware Multimodal Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2110.13309). \nProject webpage: https://cshizhe.github.io/projects/vln_hamt.html\n\nVision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. In this work, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single-step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with *fine-grained instructions* (R2R, RxR) *high-level instructions* (R2R-Last, REVERIE), *dialogs* (CVDN) as well as *long-horizon VLN* (R4R, R2R-Back).\n\n![framework](files/model_architecture.png)\n\n## Installation\n1. Install Matterport3D simulators: follow instructions [here](https://github.com/peteanderson80/Matterport3DSimulator). We use the latest version (all inputs and outputs are batched).\n```\nexport PYTHONPATH=Matterport3DSimulator/build:$PYTHONPATH\n```\n\n2. Install requirements:\n```setup\nconda create --name vlnhamt python=3.8.5\nconda activate vlnhamt\npip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\npip install -r requirements.txt\n\n# install timm\ngit clone https://github.com/rwightman/pytorch-image-models.git\ncd pytorch-image-models\ngit checkout 9cc7dda6e5fcbbc7ac5ba5d2d44050d2a8e3e38d\n```\n3. Download data from [Dropbox](https://www.dropbox.com/sh/3a5j03u286px604/AABNp887W7_Fhgv13gUt4wzda?dl=0), including processed annotations, features and pretrained models. Put the data in `datasets' directory.\n\n\n4. (Optional) If you want to train HAMT end-to-end, you should download [original Matterport3D data](https://github.com/niessner/Matterport).\nUse this [script](https://github.com/cshizhe/VLN-HAMT/blob/main/preprocess/build_image_lmdb.py) to extract panoramic images as the training data.\n\n## Extracting features (optional)\nScripts to extract visual features are in `preprocess` directory:\n```\nCUDA_VISIBLE_DEVICES=0 python preprocess/precompute_img_features_vit.py \\\n    --model_name vit_base_patch16_224 --out_image_logits \\\n    --connectivity_dir datasets/R2R/connectivity \\\n    --scan_dir datasets/Matterport3D/v1_unzip_scans \\\n    --num_workers 4 \\\n    --output_file datasets/R2R/features/pth_vit_base_patch16_224_imagenet.hdf5\n```\n\n## Training with proxy tasks\nStage 1: Pretrain with fixed ViT features\n```\nNODE_RANK=0\nNUM_GPUS=4\nCUDA_VISIBLE_DEVICES='0,1,2,3' python -m torch.distributed.launch \\\n    --nproc_per_node=${NUM_GPUS} --node_rank $NODE_RANK \\\n    pretrain_src/main_r2r.py --world_size ${NUM_GPUS} \\\n    --model_config pretrain_src/config/r2r_model_config.json \\\n    --config pretrain_src/config/pretrain_r2r.json \\\n    --output_dir datasets/R2R/exprs/pretrain/cmt-vitbase-6tasks\n```\n\nStage 2: Train ViT in an end-to-end manner \n\nRun `pretrain_src/main_r2r_image.py' and use the config file `pretrain_r2r_e2e.json'.\n\n\n## Fine-tuning for sequential action prediction\n```fine-tune\ncd finetune_src\nbash scripts/run_r2r.bash\nbash scripts/run_r2r_back.bash\nbash scripts/run_r2r_last.bash\nbash scripts/run_r4r.bash\nbash scripts/run_reverie.bash\nbash scripts/run_cvdn.bash\n```\n\n\n## Citation\nIf you find this work useful, please consider citing:\n```\n@InProceedings{chen2021hamt,\nauthor       = {Chen, Shizhe and Guhur, Pierre-Louis and Schmid, Cordelia and Laptev, Ivan},\ntitle        = {History Aware multimodal Transformer for Vision-and-Language Navigation},\nbooktitle    = {NeurIPS},\nyear         = {2021},\n}\n```\n\n## Acknowledgement\nSome of the codes are built upon [pytorch-image-models](https://github.com/rwightman/pytorch-image-models), [UNITER](https://github.com/ChenRocks/UNITER) and [Recurrent-VLN-BERT](https://github.com/YicongHong/Recurrent-VLN-BERT).\nThanks them for their great works!\n", "metadata": {"source": "github_readmes\\cshizhe_VLN-HAMT_README.md", "filename": "cshizhe_VLN-HAMT_README.md", "type": "readme_full"}}
{"id": "CSIPlab_MMSFormer_README.md", "paper_id": "CSIPlab_MMSFormer_README", "text": "<div align=\"center\"> \n\n## MMSFormer: Multimodal Transformer for Material and Semantic Segmentation\n\n</div>\n\n<div align=\"center\"> \n    \n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/multimodal-transformer-for-material/semantic-segmentation-on-mcubes)](https://paperswithcode.com/sota/semantic-segmentation-on-mcubes?p=multimodal-transformer-for-material)\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/multimodal-transformer-for-material/semantic-segmentation-on-fmb-dataset)](https://paperswithcode.com/sota/semantic-segmentation-on-fmb-dataset?p=multimodal-transformer-for-material)\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/multimodal-transformer-for-material/thermal-image-segmentation-on-pst900)](https://paperswithcode.com/sota/thermal-image-segmentation-on-pst900?p=multimodal-transformer-for-material)\n\n<a href=\"https://ieeexplore.ieee.org/document/10502124/\">\n    <img src=\"https://img.shields.io/badge/Paper-IEEE OJSP-green\" />\n</a>\n<a href=\"https://arxiv.org/pdf/2309.04001\">\n    <img src=\"https://img.shields.io/badge/arXiv-2309.04001-red\" />\n</a>\n<a href=\"https://csiplab.github.io/MMSFormer/\">\n    <img src=\"https://img.shields.io/badge/Project-Webpage-blue\" />\n</a>\n<a href=\"https://pytorch.org/\">\n    <img src=\"https://img.shields.io/badge/Framework-PyTorch-orange.svg\" />\n</a>\n</div>\n\n## Introduction\n\nLeveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different modality combinations. We also propose a new model named **M**ulti-**M**odal **S**egmentation Trans**Former** (**MMSFormer**) that incorporates the proposed fusion strategy to perform multimodal material and semantic segmentation tasks. MMSFormer outperforms current state-of-the-art models on three different datasets. As we begin with only one input modality, performance improves progressively as additional modalities are incorporated, showcasing the effectiveness of the fusion block in combining useful information from diverse input modalities. Ablation studies show that different modules in the fusion block are crucial for overall model performance. Furthermore, our ablation studies also highlight the capacity of different input modalities to improve performance in the identification of different types of materials.\n\nFor more details, please check our [arXiv](https://arxiv.org/abs/2309.04001) paper.\n\n## Updates\n- [x] 09/2023: Init repository.\n- [x] 09/2023: Release the code for MMSFormer.\n- [x] 09/2023: Release MMSFormer model weights. Download from [**GoogleDrive**](https://drive.google.com/drive/folders/1OPr7PUrL7hkBXogmHFzHuTJweHuJmlP-?usp=sharing).\n- [x] 01/2024: Update code, description and pretrained weights.\n- [x] 04/2024: Accepted by [IEEE Open Journal of Signal Processing](https://ieeexplore.ieee.org/document/10502124).\n\n## MMSFormer model\n\n<div align=\"center\"> \n\n![MMSFormer](figs/MMSFormer-V2.png)\n**Figure:** Overall architecture of MMSFormer model and proposed fusion block.\n\n</div>\n\n## Environment\n\nFirst, create and activate the environment using the following commands: \n```bash\nconda env create -f environment.yaml\nconda activate mmsformer\n```\n\n## Data preparation\nDownload the dataset:\n- [MCubeS](https://github.com/kyotovision-public/multimodal-material-segmentation), for multimodal material segmentation with RGB-A-D-N modalities.\n- [FMB](https://github.com/JinyuanLiu-CV/SegMiF), for FMB dataset with RGB-Infrared modalities.\n- [PST](https://github.com/ShreyasSkandanS/pst900_thermal_rgb), for PST900 dataset with RGB-Thermal modalities.\n\nThen, put the dataset under `data` directory as follows:\n\n```\ndata/\n├── MCubeS\n│   ├── polL_color\n│   ├── polL_aolp_sin\n│   ├── polL_aolp_cos\n│   ├── polL_dolp\n│   ├── NIR_warped\n│   ├── NIR_warped_mask\n│   ├── GT\n│   ├── SSGT4MS\n│   ├── list_folder\n│   └── SS\n├── FMB\n│   ├── test\n│   │   ├── color\n│   │   ├── Infrared\n│   │   ├── Label\n│   │   └── Visible\n│   ├── train\n│   │   ├── color\n│   │   ├── Infrared\n│   │   ├── Label\n│   │   └── Visible\n├── PST\n│   ├── test\n│   │   ├── rgb\n│   │   ├── thermal\n│   │   └── labels\n│   ├── train\n│   │   ├── rgb\n│   │   ├── thermal\n│   │   └── labels\n```\n\n## Model Zoo\n\n### MCubeS\n| Model-Modal      | mIoU   | weight |\n| :--------------- | :----- | :----- |\n| MCubeS-RGB       | 50.44 | [GoogleDrive](https://drive.google.com/drive/folders/1TiC4spUgMGo8zO2iChpuuRo8cmZC2yeh?usp=sharing) |\n| MCubeS-RGB-A     | 51.30 | [GoogleDrive](https://drive.google.com/drive/folders/1TiC4spUgMGo8zO2iChpuuRo8cmZC2yeh?usp=sharing) |\n| MCubeS-RGB-A-D   | 52.03 | [GoogleDrive](https://drive.google.com/drive/folders/1TiC4spUgMGo8zO2iChpuuRo8cmZC2yeh?usp=sharing) |\n| MCubeS-RGB-A-D-N | 53.11 | [GoogleDrive](https://drive.google.com/drive/folders/1TiC4spUgMGo8zO2iChpuuRo8cmZC2yeh?usp=sharing) |\n\n### FMB\n| Model-Modal      | mIoU   | weight |\n| :--------------- | :----- | :----- |\n| FMB-RGB          | 57.17 | [GoogleDrive](https://drive.google.com/drive/folders/15kuBWiEHOxxOLMxvASYzPhdSgG8ZWfgm?usp=sharing) |\n| FMB-RGB-Infrared | 61.68 | [GoogleDrive](https://drive.google.com/drive/folders/15kuBWiEHOxxOLMxvASYzPhdSgG8ZWfgm?usp=sharing) |\n\n### PST900\n| Model-Modal      | mIoU   | weight |\n| :--------------- | :----- | :----- |\n| PST-RGB-T        | 87.45 | [GoogleDrive](https://drive.google.com/drive/folders/1yv7wfGVLrxBYQ3teDg3eL-zYJsie56Ll?usp=sharing) |\n\n\n## Training\n\nBefore training, please download [pre-trained SegFormer](https://drive.google.com/drive/folders/10XgSW8f7ghRs9fJ0dE-EV8G2E_guVsT5), and put it in the correct directory following this structure:\n\n```text\ncheckpoints/pretrained/segformer\n├── mit_b0.pth\n├── mit_b1.pth\n├── mit_b2.pth\n├── mit_b3.pth\n└── mit_b4.pth\n```\n\nTo train MMSFormer model, please update the appropriate configuration file in `configs/` with appropriate paths and hyper-parameters. Then run as follows:\n\n```bash\ncd path/to/MMSFormer\nconda activate mmsformer\n\npython -m tools.train_mm --cfg configs/mcubes_rgbadn.yaml\n\npython -m tools.train_mm --cfg configs/fmb_rgbt.yaml\n\npython -m tools.train_mm --cfg configs/pst_rgbt.yaml\n```\n\n\n## Evaluation\nTo evaluate MMSFormer models, please download respective model weights ([**GoogleDrive**](https://drive.google.com/drive/folders/1OPr7PUrL7hkBXogmHFzHuTJweHuJmlP-?usp=sharing)) and save them under any folder you like.\n\n<!-- \n```text\noutput/\n├── MCubeS\n│   ├── MMSFormer_MiT_B2_MCubeS_RGB.pth\n│   ├── MMSFormer_MiT_B2_MCubeS_RGBA.pth\n│   ├── MMSFormer_MiT_B2_MCubeS_RGBAD.pth\n│   ├── MMSFormer_MiT_B2_MCubeS_RGBNAD.pth\n``` -->\n\nThen, update the `EVAL` section of the appropriate configuration file in `configs/` and run:\n\n```bash\ncd path/to/MMSFormer\nconda activate mmsformer\n\npython -m tools.val_mm --cfg configs/mcubes_rgbadn.yaml\n\npython -m tools.val_mm --cfg configs/fmb_rgbt.yaml\n\npython -m tools.val_mm --cfg configs/pst_rgbt.yaml\n```\n\n## License\n\nThis repository is under the Apache-2.0 license. For commercial use, please contact with the authors.\n\n\n## Citations\n\nIf you use MMSFormer model, please cite the following work:\n\n- **MMSFormer** [[**arXiv**](https://arxiv.org/abs/2309.04001)]\n```\n@ARTICLE{Reza2024MMSFormer,\n    author={Reza, Md Kaykobad and Prater-Bennette, Ashley and Asif, M. Salman},\n    journal={IEEE Open Journal of Signal Processing}, \n    title={MMSFormer: Multimodal Transformer for Material and Semantic Segmentation}, \n    year={2024},\n    volume={},\n    number={},\n    pages={1-12},\n    keywords={Image segmentation;Feature extraction;Transformers;Task analysis;Fuses;Semantic segmentation;Decoding;multimodal image segmentation;material segmentation;semantic segmentation;multimodal fusion;transformer},\n    doi={10.1109/OJSP.2024.3389812}\n}\n```\n\n## Acknowledgements\nOur codebase is based on the following Github repositories. Thanks to the following public repositories:\n- [DELIVER](https://github.com/jamycheung/DELIVER)\n- [RGBX-semantic-segmentation](https://github.com/huaaaliu/RGBX_Semantic_Segmentation)\n- [Semantic-segmentation](https://github.com/sithu31296/semantic-segmentation)\n\n**Note:** This is a research level repository and might contain issues/bugs. Please contact the authors for any query.\n", "metadata": {"source": "github_readmes\\CSIPlab_MMSFormer_README.md", "filename": "CSIPlab_MMSFormer_README.md", "type": "readme_full"}}
{"id": "dailenson_SDT_README.md", "paper_id": "dailenson_SDT_README", "text": "![MIT LICENSE](https://shields.io/badge/license-MIT-green)\n![python 3.8](https://img.shields.io/badge/python-3.8-brightgreen)\n# 🔥 Disentangling Writer and Character Styles for Handwriting Generation\n\n <p align='center'>\n  <b>\n    <a href=\"https://arxiv.org/abs/2303.14736\">ArXiv</a>\n    |\n    <a href=\"https://github.com/dailenson/SDT/blob/master/static/Poster_SDT.pdf\">Poster</a>\n    | \n    <a href=\"https://youtu.be/mKbYLEwa4dI\">Video</a>\n    | \n    <a href=\"https://cvpr2023.thecvf.com/virtual/2023/poster/20954\">Project</a>\n  </b>\n</p> \n\n## 📢 Introduction\n- The proposed style-disentangled Transformer (SDT) generates online handwritings with conditional content and style. \n- Existing RNN-based methods mainly focus on capturing a person’s overall writing style, neglecting subtle style inconsistencies between characters written by the same person. In light of this, SDT disentangles the writer-wise and character-wise style representations from individual handwriting samples for enhancing imitation performance. \n- We extend SDT and introduce an offline-to-offline framework for improving the generation quality of offline Chinese handwritings.\n\n<div style=\"display: flex; flex-direction: column; align-items: center; \">\n<img src=\"static/overview_sdt.jpg\" style=\"width: 100%;\">\n</div>\n<p align=\"center\" style=\"margin-bottom: 10px;\">\nOverview of our SDT\n</p>\n\n<div style=\"display: flex; justify-content: center;\">\n<img src=\"static/duo_loop.gif\" style=\"width: 33.33%;\"><img src=\"static/mo_loop.gif\" style=\"width: 33.33%;\"><img src=\"static/tai_loop.gif\" style=\"width: 33.33%;\">\n</div>\n<p align=\"center\">\nThree samples of online characters with writing orders\n</p>\n\n## 📅 News\n- [2025/06/26] 🎉🎉🎉 [DiffBrush](https://github.com/dailenson/DiffBrush), a novel state-of-the-art approach for full-line text generation, is accepted to ICCV 2025.\n- [2024/11/26] 🎉🎉🎉 Release of the implementations of Content Score and Style Score. \n- [2024/07/01] 🎉🎉🎉 A new state-of-the-art method for handwritten text generation, named [One-DM](https://github.com/dailenson/One-DM), is accepted by ECCV 2024. \n- [2024/01/07] Add a tutorial and code for synthesizing handwriting with user-customized styles, more information can be found [here](https://github.com/dailenson/SDT/issues/43).\n- [2023/12/15] 🎉🎉🎉 This work is reported by a top [bilibili](https://www.bilibili.com/video/BV19w411t7vD/?buvid=XX73A437799B0DCC93D6D21690FA9CAE696EC&from_spmid=default-value&is_story_h5=false&mid=Xr0IfLrZqLFnTCriRB2HcQ%3D%3D&p=1&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=2f9e186f-d693-4b61-80c6-372942bec32b&share_source=WEIXIN&share_source=weixin&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1720580374&unique_k=OqWsKIV&up_id=19319172) video blogger with 2.7 million followers and received nearly one million views.\n- [2023/10/10] The [author](https://scholar.google.com.hk/citations?user=a2SwkisAAAAJ&hl=zh-CN) is invited to give a [talk](https://www.bilibili.com/video/BV1kQ4y1W7a7/?spm_id_from=333.999.0.0&vd_source=cbc77ced94dbf77f5ecef4e0afa94a33) (in Chinese) by CSIG (China Society of Image and Graphics).\n- [2023/06/14] This work is reported by [Synced](https://mp.weixin.qq.com/s/EX_Loj4PvIztQH5zrl2FNw) (机器之心).\n- [2023/04/12] Initial release of the datasets, pre-trained models, training and testing codes.\n- [2023/02/28] 🎉🎉🎉 Our SDT is accepted by CVPR 2023.\n\n## 📺 Handwriting generation results\n- **Online Chinese handwriting generation**\n![online Chinese](static/online_Chinese.jpg)\n\n- **Applications to various scripts**\n![other scripts](static/various_scripts.jpg)\n- **Extension on offline Chinese handwriting generation**\n![offline Chinese](static/offline_Chinese.jpg)\n\n\n## 🔨 Requirements\n```\nconda create -n sdt python=3.8 -y\nconda activate sdt\n# install all dependencies\nconda env create -f environment.yml\n```\n\n## 📂 Folder Structure\n  ```\n  SDT/\n  │\n  ├── train.py - main script to start training\n  ├── test.py - generate characters via trained model\n  ├── evaluate.py - evaluation of generated samples\n  │\n  ├── configs/*.yml - holds configuration for training\n  ├── parse_config.py - class to handle config file\n  │\n  ├── data_loader/ - anything about data loading goes here\n  │   └── loader.py\n  │\n  ├── model_zoo/ - pre-trained content encoder model\n  │\n  ├── data/ - default directory for storing experimental datasets\n  │\n  ├── model/ - networks, models and losses\n  │   ├── encoder.py\n  │   ├── gmm.py\n  │   ├── loss.py\n  │   ├── model.py\n  │   └── transformer.py\n  │\n  ├── saved/\n  │   ├── models/ - trained models are saved here\n  │   ├── tborad/ - tensorboard visualization\n  │   └── samples/ - visualization samples in the training process\n  │\n  ├── trainer/ - trainers\n  │   └── trainer.py\n  │  \n  └── utils/ - small utility functions\n      ├── util.py\n      └── logger.py - set log dir for tensorboard and logging output\n  ```\n\n## 💿 Datasets\n\nWe provide Chinese, Japanese and English datasets in [Google Drive](https://drive.google.com/drive/folders/17Ju2chVwlNvoX7HCKrhJOqySK-Y-hU8K?usp=share_link) | [Baidu Netdisk](https://pan.baidu.com/s/1RNQSRhBAEFPe2kFXsHZfLA) PW:xu9u. Please download these datasets, uzip them and move the extracted files to /data.\n\n## 🍔 Pre-trained model\n\n| Model|Google Drive|Baidu Netdisk|\n|---------------|---------|-----------------------------------------|\n|Well-trained SDT|[Google Drive](https://drive.google.com/drive/folders/1LendizOwcNXlyY946ThS8HQ4wJX--YL7?usp=sharing) | [Baidu Netdisk](https://pan.baidu.com/s/1RNQSRhBAEFPe2kFXsHZfLA?pwd=xu9u)\n|Content encoder|[Google Drive](https://drive.google.com/drive/folders/1N-MGRnXEZmxAW-98Hz2f-o80oHrNaN_a?usp=share_link) | [Baidu Netdisk](https://pan.baidu.com/s/1RNQSRhBAEFPe2kFXsHZfLA?pwd=xu9u)\n|Content Score|[Google Drive](https://drive.google.com/drive/folders/1-2ciY6yfI4l1bVUD661EzEW5PInZb_62?usp=sharing)|[Baidu Netdisk]( https://pan.baidu.com/s/1cs8qWOhwISZz7w1dAYMQ3g?pwd=s8e8)\n|Style Score|[Google Drive](https://drive.google.com/drive/folders/1-2ciY6yfI4l1bVUD661EzEW5PInZb_62?usp=sharing) | [Baidu Netdisk]( https://pan.baidu.com/s/1cs8qWOhwISZz7w1dAYMQ3g?pwd=s8e8)\n\n**Note**:\nPlease download these weights, and move them to /model_zoo.\n\n## 🚀 Training & Test\n**Training**\n- To train the SDT on the Chinese dataset, run this command:\n```\npython train.py --cfg configs/CHINESE_CASIA.yml --log Chinese_log\n```\n\n- To train the SDT on the Japanese dataset, run this command:\n```\npython train.py --cfg configs/Japanese_TUATHANDS.yml --log Japanese_log\n```\n\n- To train the SDT on the English dataset, run this command:\n```\npython train.py --cfg configs/English_CASIA.yml --log English_log\n```\n\n**Qualitative Test**\n- To generate **online Chinese handwritings** with our SDT, run this command:\n```\npython test.py --pretrained_model checkpoint_path --store_type online --sample_size 500 --dir Generated/Chinese\n```\n- To generate **offline Chinese handwriting images** with our SDT, run this command:\n```\npython test.py --pretrained_model checkpoint_path --store_type offline --sample_size 500 --dir Generated_img/Chinese\n```\n\n- To generate **online Japanese handwritings** with our SDT, run this command:\n```\npython test.py --pretrained_model checkpoint_path --store_type online --sample_size 500 --dir Generated/Japanese\n```\n- To generate **offline Japanese handwriting images** with our SDT, run this command:\n```\npython test.py --pretrained_model checkpoint_path --store_type offline --sample_size 500 --dir Generated_img/Japanese\n```\n- To generate **online English handwritings** with our SDT, run this command:\n```\npython test.py --pretrained_model checkpoint_path --store_type online --sample_size 500 --dir Generated/English\n```\n- To generate **offline English handwriting images** with our SDT, run this command:\n```\npython test.py --pretrained_model checkpoint_path --store_type offline --sample_size 500 --dir Generated_img/English\n```\n\n**Quantitative Evaluation**\n- To evaluate the generated handwritings, you need to set `data_path` to the path of the generated handwritings (e.g., Generated/Chinese), and run this command:\n```\npython evaluate.py --data_path Generated/Chinese --metric DTW\n```\n- To calculate the Content Score of generated handwritings, you need to set `data_path` to the path of the generated handwritings (e.g., Generated/Chinese), and run this command:\n```\npython evaluate.py --data_path Generated/Chinese --metric Content_score --pretrained_model model_zoo/chinese_content_iter30k_acc95.pth\n```\n- To calculate the Style Score of generated handwritings, you need to set `data_path` to the path of the generated handwriting images (e.g., Generated_img/Chinese), and run this command:\n```\npython evaluate.py --data_path Generated_img/Chinese --metric Style_score --pretrained_model models_zoo/chinese_style_iter60k_acc999.pth\n```\n## 🏰 Practical Application\nWe are delighted to discover that **[P0etry-rain](https://github.com/P0etry-rain)** has proposed a pipeline that involves initially converting the generated results by our SDT to TTF format, followed by the development of software to enable flexible adjustments in spacing between paragraphs, lines, and characters. Below, we present TTF files, software interface and the printed results. More details can be seen in [#78](https://github.com/dailenson/SDT/issues/78#issue-2247810028).\n- **TTF File**\n![SVG](static/svg.png)\n\n- **Software Interface**\n![Interface](static/software.png)\n\n- **Printed Results**\n![Result](static/print.png)\n\n\n\n## ❤️ Citation\nIf you find our work inspiring or use our codebase in your research, please cite our work:\n```\n@inproceedings{dai2023disentangling,\n  title={Disentangling Writer and Character Styles for Handwriting Generation},\n  author={Dai, Gang and Zhang, Yifan and Wang, Qingfeng and Du, Qing and Yu, Zhuliang and Liu, Zhuoman and Huang, Shuangping},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n  pages={5977--5986},\n  year={2023}\n}\n```\n\n## ⭐ StarGraph\n[![Star History Chart](https://api.star-history.com/svg?repos=dailenson/SDT&type=Timeline)](https://star-history.com/#dailenson/SDT&Timeline)\n\n\n", "metadata": {"source": "github_readmes\\dailenson_SDT_README.md", "filename": "dailenson_SDT_README.md", "type": "readme_full"}}
{"id": "DAMO-NLP-SG_Multipurpose-Chatbot_README.md", "paper_id": "DAMO-NLP-SG_Multipurpose-Chatbot_README", "text": "# Multi-purpose Chatbot (Local, Remote and HF spaces)\n\nA Chatbot UI that support Chatbot, RAG, Text completion, Multi-modal across [HF Transformers](https://github.com/huggingface/transformers), [llama.cppp](https://github.com/ggerganov/llama.cpp), [Apple MLX](https://github.com/ml-explore/mlx) and [vLLM](https://github.com/vllm-project/vllm).\n\nDesigned support both locally, remote and huggingface spaces.\n\n![image](assets/image_doc_rag.gif)\n\n---\n\n**Checkout cool demos using Multi-purpose chatbot.**\n- [MultiModal SeaLLMs/SeaLLM-7B](https://huggingface.co/spaces/SeaLLMs/SeaLLM-7B)\n- [MultiPurpose-Chatbot-DEMO test](https://huggingface.co/spaces/nxphi47/MultiPurpose-Chatbot-DEMO) - This DEMO test the UI without LLM.\n\n**Supported features**\n- Vanilla chat interface - [ChatInterfaceDemo](multipurpose_chatbot/demos/chat_interface.py)\n- Chat with short document (full context) - [DocChatInterfaceDemo](multipurpose_chatbot/demos/multimodal_chat_interface.py)\n- Chat with visual image - [VisionChatInterfaceDemo](multipurpose_chatbot/demos/multimodal_chat_interface.py)\n- Chat with visual image and short document - [VisionDocChatInterfaceDemo](multipurpose_chatbot/demos/multimodal_chat_interface.py)\n- Chat with long document via RAG - [RagChatInterfaceDemo](multipurpose_chatbot/demos/rag_chat_interface.py)\n- Text completion (free form prompting) - [TextCompletionDemo](multipurpose_chatbot/demos/text_completion.py)\n- Batch inference (via file upload with vLLM) - [BatchInferenceDemo](multipurpose_chatbot/demos/batch_inference.py)\n\n**Support backend**\n- [GPU Transformers](https://github.com/huggingface/transformers) with full support MultiModal, document QA, RAG, completion.\n- [llama.cppp](https://github.com/ggerganov/llama.cpp) like Transformers, except pending MultiModal. PR welcome.\n- [Apple MLX](https://github.com/ml-explore/mlx) like Transformers, except pending MultiModal. PR welcome.\n- [vLLM](https://github.com/vllm-project/vllm) like Transformers + **Batch inference via file upload**, pending MultiModal. PR welcome.\n\nMulti-purpose Chatbot use `ENVIRONMENT VARIABLE` instead of `argparse` to set hyperparmeters to support seamless integration with HF space, which requires us to set params via environment vars. The app is launch only with `python app.py`\n\n## Installation\n\n```bash\npip install -r requirements.txt\n```\n\n#### Transformers\n```bash\npip install -r transformers_requirements.txt\n```\n\n\n#### VLLM\n```bash\npip install -r vllm_requirements.txt\n```\n\n\n#### llama.cpp\nFollow [Llama-cpp-python](https://llama-cpp-python.readthedocs.io/en/latest/#installation) to install `llama.cpp`\n\ne.g: On Macos\n```bash\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n```\n\n\n#### MLX\n\nOnly on MacOS, remember to install [**NATIVE** python environment](https://ml-explore.github.io/mlx/build/html/install.html).\n\n```bash\npython -c \"import platform; print(platform.processor())\"\n# should output \"arm\", if not reinstall python with native\n```\n\nInstall requirements\n```bash\npip install -r mlx_requirements.txt\n```\n\n\n## Usage\n\nWe use bash environment to define model variables\n\n#### Transformers\n\n`MODEL_PATH` must be a model with chat_template with system prompt (e.g Mistral-7B-Instruct-v0.2 does not have system prompt)\n\n```bash\nexport BACKEND=transformers\nexport MODEL_PATH=teknium/OpenHermes-2.5-Mistral-7B\nexport RAG_EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2\nexport DEMOS=DocChatInterfaceDemo,ChatInterfaceDemo,RagChatInterfaceDemo,TextCompletionDemo\npython app.py\n```\n\n#### Llava-1.5 Transformers\n\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0\nexport TEMPERATURE=0.7\nexport MAX_TOKENS=512\nexport MODEL_PATH=llava-hf/llava-1.5-7b-hf\nexport IMAGE_TOKEN=\"<image>\"\nexport BACKEND=llava15_transformers\nexport DEMOS=VisionChatInterfaceDemo,VisionDocChatInterfaceDemo,TextCompletionDemo\npython app.py\n\n```\n\n#### VLLM\n\n```bash\nexport BACKEND=vllm\nexport MODEL_PATH=teknium/OpenHermes-2.5-Mistral-7B\nexport RAG_EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2\nexport DEMOS=DocChatInterfaceDemo,ChatInterfaceDemo,RagChatInterfaceDemo,TextCompletionDemo\npython app.py\n```\n\n\n#### llama.cpp\n\n```bash\nexport BACKEND=llama_cpp\nexport MODEL_PATH=/path/to/model.gguf\nexport RAG_EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2\nexport DEMOS=DocChatInterfaceDemo,ChatInterfaceDemo,RagChatInterfaceDemo,TextCompletionDemo\npython app.py\n```\n\n\n#### MLX\n\n```bash\nexport BACKEND=mlx\nexport MODEL_PATH=mlx-community/Nous-Hermes-2-Mistral-7B-DPO-4bit-MLX\nexport RAG_EMBED_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2\nexport DEMOS=DocChatInterfaceDemo,ChatInterfaceDemo,RagChatInterfaceDemo,TextCompletionDemo\npython app.py\n```\n\n\n## Customization\n\n#### Configs:\n\n* [configs.py](multipurpose_chatbot/configs.py) where you can find customize UI markdowns and settings global variables\n\n#### Backend and engines\n* [base_engine](multipurpose_chatbot/engines/base_engine.py),  [transformers_engine](multipurpose_chatbot/engines/transformers_engine.py) and [llama_cpp_engine](multipurpose_chatbot/engines/llama_cpp_engine.py) to find how different model backend works. Feel free to extend and implement new features.\n* [llava15_transformers_engine](multipurpose_chatbot/engines/llava15_transformers_engine.py) describe how to implement Llava-1.5\n\n\n#### Gradio Demo tabs\n* Checkout [chat_interface](multipurpose_chatbot/demos/chat_interface.py), [multimodal_chat_interface](multipurpose_chatbot/demos/multimodal_chat_interface.py) and other interface demo under [multipurpose_chatbot/demos](multipurpose_chatbot/demos) to find out how the demo works.\n\n\n#### Enableing demos\n\nSetting comma-separated demo class names (e.g `ChatInterfaceDemo` to enable demo).\n\n```bash\nexport DEMOS=VisionDocChatInterfaceDemo,VisionChatInterfaceDemo,DocChatInterfaceDemo,ChatInterfaceDemo,RagChatInterfaceDemo,TextCompletionDemo\n```\n\n\n## Contributing\n\nWe welcome and value any contributions and collaborations. Feel free to open a PR\n\n\n## Citation\n\nIf you find our project useful, hope you can star our repo and cite our repo as follows:\n```\n@article{multipurpose_chatbot_2024,\n  author = {Xuan-Phi Nguyen, },\n  title = {Multipurpose Chatbot},\n  year = 2024,\n}\n", "metadata": {"source": "github_readmes\\DAMO-NLP-SG_Multipurpose-Chatbot_README.md", "filename": "DAMO-NLP-SG_Multipurpose-Chatbot_README.md", "type": "readme_full"}}
{"id": "decisionforce_mmTransformer_README.md", "paper_id": "decisionforce_mmTransformer_README", "text": "# mmTransformer\n\n## Introduction\n\n- This repo is official implementation for [mmTransformer](https://github.com/decisionforce/mmTransformer) in pytorch. Currently, the core code of mmTransformer is implemented in the commercial project, we provide **inference code** of model with six trajectory propopals for your reference. \n\n- For other information, please refer to our paper **Multimodal Motion Prediction with Stacked Transformers**. (CVPR 2021) [[Paper](https://arxiv.org/pdf/2103.11624.pdf)] [[Webpage](https://decisionforce.github.io/mmTransformer/)]\n\n![img](./figs/model.png)\n\n## Set up your virtual environment\n\n- Initialize virtual environment:\n\n      conda create -n mmTrans python=3.7\n\n- Install agoverse api. Please refer to [this page](https://github.com/argoai/argoverse-api).\n\n- Install the [pytorch](https://pytorch.org/). The latest codes are tested on Ubuntu 16.04, CUDA11.1, PyTorch 1.8 and Python 3.7:\n  (Note that we require the version of torch >= 1.5.0 for testing with pretrained model)\n\n      pip install torch==1.8.0+cu111\\\n            torchvision==0.9.0+cu111\\\n            torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n\n- For other requirement, please install with following command:\n\n      pip install -r requirement.txt\n    \n\n## Preparation\n\n### Download the code, model and data\n\n1. Clone this repo from the GitHub.\n\n        git clone https://github.com/decisionforce/mmTransformer.git\n\n2. Download the pretrained model and data [[here](https://drive.google.com/file/d/10koDID95zoOnU3pb6AkHAqJInupMScJd/view?usp=sharing)] (map.pkl for Python 3.7 is available [[here](https://drive.google.com/file/d/1HbsgutM1PKjPj-3IIA5kG3mJEMhHGhS0/view?usp=sharing)]) and save it to `./models` and `./interm_data`.\n   \n        cd mmTransformer\n        mkdir models\n        mkdir interm_data\n\n3. Finally, your directory structure should look something like this:\n\n        mmTransformer\n        └── models\n            └── demo.pt\n        └── interm_data\n            └── argoverse_info_val.pkl\n            └── map.pkl\n\n### Preprocess the dataset\n\nAlternatively, you can process the data from scratch using following commands.\n\n1. Download Argoverse dataset and create a symbolic link to `./data` folder or use following commands.\n\n        cd path/to/mmtransformer/root\n        mkdir data\n        cd data\n        wget https://s3.amazonaws.com/argoai-argoverse/forecasting_val_v1.1.tar.gz \n        tar -zxvf  forecasting_val_v1.1.tar.gz\n\n2. Then extract the agent and map information from raw data via Argoverse API:\n\n        python -m lib.dataset.argoverse_convertor ./config/demo.py\n\n3. Finally, your directory structure should look something like above illustrated.\n\n\nFormat of processed data in ‘argoverse_info_val.pkl’:\n\n![img](./figs/format1.png)\n\nFormat of map information in ‘map.pkl’:\n\n![img](./figs/format2.png)\n\n\n## Run the mmTransformer\n\nFor testing:\n\n    python Evaluation.py ./config/demo.py --model-name demo\n\n## Results\n\nHere we showcase the expected results on validation set:\n\n| Model | Expected results | Results in paper\n|--|--|--|\n| minADE | 0.709 | 0.713 |\n| minFDE | 1.081 | 1.153 |\n| MR (K=6) | 10.2 | 10.6 |\n\n## TODO\n\n- We are going to open source our visualization tools and a demo result. (TBD)\n\n## Contact us\nIf you have any issues with the code, please contact to this email: <moooooore66@gmail.com>\n\n## Citation\nIf you find our work useful for your research, please consider citing the paper\n```\n@article{liu2021multimodal,\n  title={Multimodal Motion Prediction with Stacked Transformers},\n  author={Liu, Yicheng and Zhang, Jinghuai and Fang, Liangji and Jiang, Qinhong and Zhou, Bolei},\n  journal={Computer Vision and Pattern Recognition},\n  year={2021}\n}\n```\n", "metadata": {"source": "github_readmes\\decisionforce_mmTransformer_README.md", "filename": "decisionforce_mmTransformer_README.md", "type": "readme_full"}}
{"id": "double125_MADTP_README.md", "paper_id": "double125_MADTP_README", "text": "# MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer\n\n<p align=\"center\"> <a href=\"https://arxiv.org/pdf/2403.02991.pdf\" target=\"_blank\">[Paper]</a> \n<a href=\"https://arxiv.org/abs/2403.02991\" target=\"_blank\">[ArXiv]</a> \n<a href=\"https://github.com/double125/MADTP\" target=\"_blank\">[Code]</a>\n\n<img src=\"MADTP.png\" width=\"800\">\n\nOfficial implementation of [MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer](https://arxiv.org/abs/2403.02991). \n\n### What's New 🥳\n\n* (SEP 6, 2024), we released the ```implementation``` and ```scripts``` of MADTP. (Note that ```checkpoints``` and ```logs``` will come soon.)[[Code]](https://github.com/double125/MADTP\") 🚩\n\n* (Feb 27, 2024), MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer was accepted by CVPR 2024. [[Paper]](https://arxiv.org/pdf/2403.02991.pdf) [[ArXiv]](https://arxiv.org/abs/2403.02991). 🎉\n\n\n### Installation\nThe code is tested on `Pytorch==1.11.0`, `cuda==11.3.1`, and `python==3.8.13`. The dependencies can be installed by:\n```\nconda env create -f environment.yml\n```\n\n### Supported Tasks, Models, and Datasets\nType |  Supported Tasks | Supported Models  | Supported Datasets |\n--- | --- | :---: | :---: \nMulti-modal | [Visual Reasoning](https://github.com/double125/MADTP#visual-reasoning-on-the-nlvr2-dataset) | [BLIP](https://github.com/salesforce/BLIP) ([instructions](https://github.com/double125/MADTP#visual-reasoning-on-the-nlvr2-dataset)) | [NLVR2](https://lil.nlp.cornell.edu/nlvr/)\nMulti-modal |[Image Caption](https://github.com/double125/MADTP#image-caption-on-the-coco-caption-dataset) | [BLIP](https://github.com/salesforce/BLIP) ([instructions](https://github.com/double125/MADTP#image-caption-on-the-coco-caption-dataset)) | [COCO Caption](https://cocodataset.org/#home)\nMulti-modal |[Visual Question Answer](https://github.com/double125/MADTP#visual-question-answer-on-the-vqav2-dataset) | [BLIP](https://github.com/salesforce/BLIP) ([instructions](https://github.com/double125/MADTP#visual-question-answer-on-the-vqav2-dataset)) | [VQAv2](https://visualqa.org/)\nMulti-modal |[Image-Text Retrieval](https://github.com/double125/MADTP#image-text-and-text-image-retrieval-on-the-coco-dataset) | [CLIP](https://github.com/openai/CLIP) ([instructions](https://github.com/double125/MADTP#image-text-and-text-image-retrieval-on-the-coco-dataset-with-clip)), [BLIP](https://github.com/salesforce/BLIP) ([instructions](https://github.com/double125/MADTP#image-text-and-text-image-retrieval-on-the-coco-dataset)) | [COCO](https://cocodataset.org/#home), [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/)\nMulti-modal |[Text-Image Retrieval](https://github.com/double125/MADTP#image-text-and-text-image-retrieval-on-the-coco-dataset) | [CLIP](https://github.com/openai/CLIP) ([instructions](https://github.com/double125/MADTP#image-text-and-text-image-retrieval-on-the-flickr30k-dataset-with-clip)), [BLIP](https://github.com/salesforce/BLIP) ([instructions](https://github.com/double125/MADTP#image-text-and-text-image-retrieval-on-the-flickr30k-dataset)) | [COCO](https://cocodataset.org/#home), [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/)\n\n### Visual Reasoning on the NLVR2 Dataset\n\n* Dataset & Annotation\n\n    Download the [NLVR2](https://lil.nlp.cornell.edu/nlvr/) dataset, unzip it under the `datasets` folder, and accordingly modify the `image_root` in [config](./configs/nlvr.yaml). Download all-in-one annotations (including annotations for Visual Reasoning, Image Caption, VQA, Image-Text Retrieval, and Text-Image Retrieval tasks) from [this link](https://drive.google.com/uc?export=download&id=19Vk07K3DbQYa68DipJ4dFNcF0_Br7cmD), unzip it under the `annotation` folder, and accordingly modify the `annotation` in [config](./configs/nlvr.yaml). See [here](https://github.com/double125/MADTP#expected-folder-structures) for expected folder structres.\n\n* Evaluation\n  \n    Download compressed checkpoints from the table below, put them under the `output` folder, and accordingly modify the `--pretrained` of the scripts. For example, to evaluate a compressed model with 0.5 reduce ratio:\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_nlvr.py --evaluate \\\n    --pretrained output/nlvr_nlvr2_compression_p0.5/model_base_nlvr_nlvr2_p0.5_compressed.pth \\\n    --config ./configs/nlvr.yaml \\\n    --output_dir output/nlvr_nlvr2_compression_p0.5\n    ```\n\n* Compression\n  \n    Download the uncompressed model from the table below, put it under the `pretrained` folder, and accordingly modify the `pretrained` in [config](./configs/nlvr.yaml). For example, to conduct a compression at 0.5 reduce ratio on 8 A100 GPUs (80G): \n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_nlvr_dtp.py --p 0.5 --epoch 15 \\\n    --pretrained pretrained/model_base_nlvr.pth \\\n    --config ./configs/nlvr.yaml \\\n    --output_dir output/nlvr_nlvr2_compression_p0.5\n    ```\n\n* Resources\n\n    Reduction | Uncompressed Model | Compression Script | Training Log | Compressed Checkpoint | Evaluation Script\n    --- | :---: | :---: | :---: | :---: | :---: \n    0.3 | <a href=\"https://drive.google.com/uc?export=download&id=1pcsvlNRzzoq_q6Kaku_Kkg1MFELGoIxE\">Download</a> | [Link](./scripts/compress_nlvr_nlvr2_p0.3.sh) | <a href=\"https://drive.google.com/file/d/1aqiY86op26ceuWp6SFu1kaScqDnAIl1G/view?usp=drive_link\">Download</a> | <a href=\"https://drive.google.com/file/d/1foe-c6qU97QGEz7kNC9OsGJ8OXk7OmQT/view?usp=drive_link\">Download</a> | [Link](./scripts/evaluate_nlvr_nlvr2_p0.3_compressed.sh)\n    0.5 | <a href=\"https://drive.google.com/uc?export=download&id=1pcsvlNRzzoq_q6Kaku_Kkg1MFELGoIxE\">Download</a> | [Link](./scripts/compress_nlvr_nlvr2_p0.5.sh) | <a href=\"https://drive.google.com/file/d/1JyYypUDbZVD00ep5SSnQEc6LnOEL-ODT/view?usp=drive_link\">Download</a> | <a href=\"https://drive.google.com/file/d/1R_TgQKlHv6Y6Fh5_ny4fRKNLAva75Frs/view?usp=drive_link\">Download</a> | [Link](./scripts/evaluate_nlvr_nlvr2_p0.5_compressed.sh)\n    0.6 | <a href=\"https://drive.google.com/uc?export=download&id=1pcsvlNRzzoq_q6Kaku_Kkg1MFELGoIxE\">Download</a> | [Link](./scripts/compress_nlvr_nlvr2_p0.6.sh)| <a href=\"https://drive.google.com/file/d/1YB8xJee2R7B5PSjzLEJBjmQkBs5XAfIe/view?usp=drive_link\">Download</a> | <a href=\"https://drive.google.com/file/d/1Sg_agxwV04o13d6XnJLblGby5cedtngT/view?usp=drive_link\">Download</a> | [Link](./scripts/evaluate_nlvr_nlvr2_p0.6_compressed.sh)\n    0.7 | <a href=\"https://drive.google.com/uc?export=download&id=1pcsvlNRzzoq_q6Kaku_Kkg1MFELGoIxE\">Download</a> | [Link](./scripts/compress_nlvr_nlvr2_p0.7.sh)| <a href=\"https://drive.google.com/file/d/11DbcbzsCjA7mH5gbJQrtrHapobIz12n-/view?usp=drive_link\">Download</a> | <a href=\"https://drive.google.com/file/d/1qcZf5YOl1aDW8S5OEDsIH6lZN4z2UgI8/view?usp=drive_link\">Download</a> | [Link](./scripts/evaluate_nlvr_nlvr2_p0.7_compressed.sh)\n    0.8 | <a href=\"https://drive.google.com/uc?export=download&id=1pcsvlNRzzoq_q6Kaku_Kkg1MFELGoIxE\">Download</a> | [Link](./scripts/compress_nlvr_nlvr2_p0.8.sh) | <a href=\"https://drive.google.com/file/d/16K2WIslVVoAzqmMcwvoBWI4gTfxNc8Rv/view?usp=drive_link\">Download</a> | <a href=\"https://drive.google.com/file/d/1l_isAhyRTr7n8qpzXaa8y6hz2BSyR95Y/view?usp=drive_link\">Download</a> | [Link](./scripts/evaluate_nlvr_nlvr2_p0.8_compressed.sh)\n\n\n\n### Image Caption on the COCO Caption Dataset\n\n* Dataset & Annotation\n\n    Download the [COCO Caption](https://cocodataset.org/#home) dataset, unzip it under the `datasets` folder, and accordingly modify the `image_root` in [config](./configs/caption_coco.yaml). Download all-in-one annotations  from [this link](https://drive.google.com/uc?export=download&id=19Vk07K3DbQYa68DipJ4dFNcF0_Br7cmD), unzip it under the `annotation` folder, and accordingly modify the `annotation` in [config](./configs/caption_coco.yaml). See [here](https://github.com/double125/MADTP#expected-folder-structures) for expected folder structres.\n\n* Evaluation\n  \n    Download compressed checkpoints from the table below, put them under the `output` folder, and accordingly modify the `--pretrained` of the scripts. For example, to evaluate a compressed model with 0.5 reduce ratio:\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_caption_dtp.py --evaluate \\\n    --pretrained output/caption_coco_compression_p0.5/model_base_caption_capfilt_large_coco_p0.5_compressed.pth \\\n    --config ./configs/caption_coco.yaml \\\n    --output_dir output/caption_coco_compression_p0.5\n    ```\n\n* Compression\n  \n    Download the uncompressed model from the table below, put it under the `pretrained` folder, and accordingly modify the `pretrained` in [config](./configs/caption_coco.yaml). For example, to conduct a compression at 0.5 reduce ratio on 8 A100 GPUs (80G): \n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_caption_dtp.py --p 0.5 --epoch 5 \\\n    --pretrained pretrained/model_base_caption_capfilt_large.pth \\\n    --config ./configs/caption_coco.yaml \\\n    --output_dir output/caption_coco_compression_p0.5\n    ```\n\n<!-- * Resources\n\n    Reduction | Uncompressed Model | Compression Script | Training Log | Compressed Checkpoint | Evaluation Script\n    --- | :---: | :---: | :---: | :---: | :---: \n    0.5 | <a href=\"https://drive.google.com/uc?export=download&id=1qW_0DpQsDc6u9g3fSfTI4g_VXYsMA5s8\">Download</a> | [Link](./scripts/compress_caption_coco_p0.5.sh) | <a href=\"*****r\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_caption_coco_p0.5_compressed.sh)\n    0.75 | <a href=\"https://drive.google.com/uc?export=download&id=1qW_0DpQsDc6u9g3fSfTI4g_VXYsMA5s8\">Download</a> | [Link](./scripts/compress_caption_coco_p0.75.sh)| <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_caption_coco_p0.75_compressed.sh) -->\n    \n\n\n### Visual Question Answer on the VQAv2 Dataset\n\n* Dataset & Annotation\n\n    Download the [VQAv2](https://visualqa.org/) dataset and [Visual Genome](https://visualgenome.org/) dataset, unzip them under the `datasets` folder, and accordingly modify the `image_root` in [config](./configs/vqa.yaml). Download all-in-one annotations  from [this link](https://drive.google.com/uc?export=download&id=19Vk07K3DbQYa68DipJ4dFNcF0_Br7cmD), unzip it under the `annotation` folder, and accordingly modify the `annotation` in [config](./configs/vqa.yaml). See [here](https://github.com/double125/MADTP#expected-folder-structures) for expected folder structres.\n\n* Evaluation\n  \n    Download compressed checkpoints from the table below, put them under the `output` folder, and accordingly modify the `--pretrained` of the scripts. For example, to evaluate a compressed model with 0.5 reduce ratio: (Note that the scripts will generate answers `vqa_result.json`, which should be submitted to the [official server](https://eval.ai/web/challenges/challenge-page/830/overview) to obtain evaluation results.) \n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_vqa_dtp.py --evaluate \\\n    --pretrained output/vqa_vqa2_compression_p0.5/model_base_vqa_capfilt_large_vqa2_p0.5_compressed.pth \\\n    --config ./configs/vqa.yaml \\\n    --output_dir output/vqa_vqa2_compression_p0.5\n    ```\n\n* Compression\n  \n    Download the uncompressed model from the table below, put it under the `pretrained` folder, and accordingly modify the `pretrained` in [config](./configs/vqa.yaml). For example, to conduct a compression at 0.5 reduce ratio on 8 A100 GPUs (80G): \n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_vqa_dtp.py --p 0.5 --epoch 3 \\\n    --pretrained pretrained/model_base_vqa_capfilt_large.pth \\\n    --config ./configs/vqa.yaml \\\n    --output_dir output/vqa_vqa2_compression_p0.5\n    ```\n\n<!-- * Resources\n\n    Reduction | Uncompressed Model | Compression Script | Training Log | Compressed Checkpoint | Evaluation Script\n    --- | :---: | :---: | :---: | :---: | :---: \n    0.5 | <a href=\"https://drive.google.com/uc?export=download&id=18Ihg2NA_puj3_92uVszqonSusLFgmID-\">Download</a> | [Link](./scripts/compress_vqa_vqa2_p0.5.sh) | <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_vqa_vqa2_p0.5_compressed.sh)\n    0.75 | <a href=\"https://drive.google.com/uc?export=download&id=18Ihg2NA_puj3_92uVszqonSusLFgmID-\">Download</a> | [Link](./scripts/compress_vqa_vqa2_p0.75.sh)| <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_vqa_vqa2_p0.75_compressed.sh) -->\n    \n\n### Image-Text and Text-Image Retrieval on the COCO Dataset\n\n* Dataset & Annotation\n\n    Download the [COCO](https://cocodataset.org/#home) dataset, unzip it under the `datasets` folder, and accordingly modify the `image_root` in [config](./configs/retrieval_coco.yaml). Download all-in-one annotations  from [this link](https://drive.google.com/uc?export=download&id=19Vk07K3DbQYa68DipJ4dFNcF0_Br7cmD), unzip it under the `annotation` folder, and accordingly modify the `annotation` in [config](./configs/retrieval_coco.yaml). See [here](https://github.com/double125/MADTP#expected-folder-structures) for expected folder structres.\n\n* Evaluation\n  \n    Download compressed checkpoints from the table below, put them under the `output` folder, and accordingly modify the `--pretrained` of the scripts. For example, to evaluate a compressed model with 0.5 reduce ratio:\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_dtp.py --evaluate \\\n    --pretrained output/retrieval_coco_compression_p0.5/model_base_retrieval_coco_p0.5_compressed.pth --config ./configs/retrieval_coco.yaml \\\n    --output_dir output/retrieval_coco_compression_p0.5\n    ```\n\n* Compression\n  \n    Download the uncompressed model from the table below, put it under the `pretrained` folder, and accordingly modify the `pretrained` in [config](./configs/retrieval_coco.yaml). For example, to conduct a compression at 0.5 reduce ratio on 8 A100 GPUs (80G):\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_dtp.py --p 0.5 --epoch 5 \\\n    --pretrained pretrained/model_base_retrieval_coco.pth \\\n    --config ./configs/retrieval_coco.yaml \\\n    --output_dir output/retrieval_coco_compression_p0.5\n    ```\n\n<!-- * Resources\n\n    Reduction | Uncompressed Model | Compression Script | Training Log | Compressed Checkpoint | Evaluation Script\n    --- | :---: | :---: | :---: | :---: | :---: \n    0.5 | <a href=\"https://drive.google.com/uc?export=download&id=19nxvphpnIH2kbV4unL0MDAM_2zlBnruq\">Download</a> | [Link](./scripts/compress_retrieval_coco_p0.5.sh) | <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_coco_p0.5_compressed.sh)\n    0.75 | <a href=\"https://drive.google.com/uc?export=download&id=19nxvphpnIH2kbV4unL0MDAM_2zlBnruq\">Download</a> | [Link](./scripts/compress_retrieval_coco_p0.75.sh)| <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_coco_p0.75_compressed.sh) -->\n    \n\n### Image-Text and Text-Image Retrieval on the Flickr30K Dataset\n\n* Dataset & Annotation\n\n    Download the [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/) dataset, unzip it under the `datasets` folder, and accordingly modify the `image_root` in [config](./configs/retrieval_flickr.yaml). Download all-in-one annotations  from [this link](https://drive.google.com/uc?export=download&id=19Vk07K3DbQYa68DipJ4dFNcF0_Br7cmD), unzip it under the `annotation` folder, and accordingly modify the `annotation` in [config](./configs/retrieval_flickr.yaml). See [here](https://github.com/double125/MADTP#expected-folder-structures) for expected folder structres.\n\n* Evaluation\n  \n    Download compressed checkpoints from the table below, put them under the `output` folder, and accordingly modify the `--pretrained` of the scripts. For example, to evaluate a compressed model with 0.5 reduce ratio:\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_flickr.py --evaluate \\\n    --pretrained output/retrieval_flickr_compression_2x/model_base_retrieval_flickr_2x_compressed.pth \\\n    --config ./configs/retrieval_flickr.yaml \\\n    --output_dir output/retrieval_flickr_compression_2x\n    ```\n\n* Compression\n  \n    Download the uncompressed model from the table below, put it under the `pretrained` folder, and accordingly modify the `pretrained` in [config](./configs/retrieval_flickr.yaml). For example, to conduct a compression at 0.5 reduce ratio on 8 A100 GPUs (80G):\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_flickr_dtp.py --p 0.5 --epoch 10 \\\n    --pretrained pretrained/model_base_retrieval_flickr.pth \\\n    --config ./configs/retrieval_flickr.yaml \\\n    --output_dir output/retrieval_flickr_compression_p0.75\n    ```\n\n<!-- * Resources\n\n    Reduction | Uncompressed Model | Compression Script | Training Log | Compressed Checkpoint | Evaluation Script\n    --- | :---: | :---: | :---: | :---: | :---: \n    0.5 | <a href=\"https://drive.google.com/uc?export=download&id=1mrd7unZMFMC77Qb_3DAx7MhpZJv4Ptbw\">Download</a> | [Link](./scripts/compress_retrieval_flickr_p0.5.sh) | <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_flickr_p0.5_compressed.sh)\n    0.75 | <a href=\"https://drive.google.com/uc?export=download&id=1mrd7unZMFMC77Qb_3DAx7MhpZJv4Ptbw\">Download</a> | [Link](./scripts/compress_retrieval_flickr_p0.75.sh)| <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_flickr_p0.75_compressed.sh) -->\n\n\n### Image-Text and Text-Image Retrieval on the COCO Dataset with CLIP\n\n* Dataset & Annotation\n\n    Download the [COCO](https://cocodataset.org/#home) dataset, unzip it under the `datasets` folder, and accordingly modify the `image_root` in [config](./configs/retrieval_coco_clip.yaml). Download all-in-one annotations  from [this link](https://drive.google.com/uc?export=download&id=19Vk07K3DbQYa68DipJ4dFNcF0_Br7cmD), unzip it under the `annotation` folder, and accordingly modify the `annotation` in [config](./configs/retrieval_coco_clip.yaml). See [here](https://github.com/double125/MADTP#expected-folder-structures) for expected folder structres.\n\n* Evaluation\n  \n    Download compressed checkpoints from the table below, put them under the `output` folder, and accordingly modify the `--pretrained` of the scripts. For example, to evaluate a compressed model with 0.5 reduce ratio:\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_clip_dtp.py --evaluate \\\n    --pretrained output/retrieval_coco_clip_compression_p0.5/clip_large_retrieval_coco_p0.5_compressed.pth \\\n    --config ./configs/retrieval_coco_clip.yaml \\\n    --output_dir output/retrieval_coco_clip_compression_p0.5\n    ```\n\n* Compression\n  \n    Download the uncompressed model from the table below, put it under the `pretrained` folder, and accordingly modify the `pretrained` in [config](./configs/retrieval_coco_clip.yaml). For example, to conduct a compression at 0.5 reduce ratio on 8 A100 GPUs (80G): \n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_clip_dtp.py --p 0.5 --epoch 5 \\\n    --pretrained pretrained/clip_large_retrieval_coco.pth \\\n    --config ./configs/retrieval_coco_clip.yaml \\\n    --output_dir output/retrieval_coco_clip_compression_p0.5\n    ```\n\n<!-- * Resources\n\n    Reduction | Uncompressed Model | Compression Script | Training Log | Compressed Checkpoint | Evaluation Script\n    --- | :---: | :---: | :---: | :---: | :---: \n    0.5 | <a href=\"https://drive.google.com/uc?export=download&id=10p1oPdiMUqo0MfPul5hCb_h9mCaNCh6q\">Download</a> | [Link](./scripts/compress_retrieval_coco_clip_p0.5.sh) | <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_coco_clip_p0.5_compressed.sh)\n    0.75 | <a href=\"https://drive.google.com/uc?export=download&id=10p1oPdiMUqo0MfPul5hCb_h9mCaNCh6q\">Download</a> | [Link](./scripts/compress_retrieval_coco_clip_p0.75.sh)| <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_coco_clip_p0.75_compressed.sh) -->\n\n\n### Image-Text and Text-Image Retrieval on the Flickr30K Dataset with CLIP\n\n* Dataset & Annotation\n\n    Download the [Flickr30k](https://shannon.cs.illinois.edu/DenotationGraph/) dataset, unzip it under the `datasets` folder, and accordingly modify the `image_root` in [config](./configs/retrieval_flickr_clip.yaml). Download all-in-one annotations  from [this link](https://drive.google.com/uc?export=download&id=19Vk07K3DbQYa68DipJ4dFNcF0_Br7cmD), unzip it under the `annotation` folder, and accordingly modify the `annotation` in [config](./configs/retrieval_flickr_clip.yaml). See [here](https://github.com/double125/MADTP#expected-folder-structures) for expected folder structres.\n\n* Evaluation\n  \n    Download compressed checkpoints from the table below, put them under the `output` folder, and accordingly modify the `--pretrained` of the scripts. For example, to evaluate a compressed model with 0.5 reduce ratio:\n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_clip_dtp.py --evaluate \\\n    --pretrained output/retrieval_flickr_clip_compression_p0.5/checkpoint_best.pth \\\n    --config ./configs/retrieval_flickr_clip.yaml \\\n    --output_dir output/retrieval_flickr_clip_compression_p0.5\n    ```\n\n* Compression\n  \n    Download the uncompressed model from the table below, put it under the `pretrained` folder, and accordingly modify the `pretrained` in [config](./configs/retrieval_flickr_clip.yaml). For example, to conduct a compression at 0.5 reduce ratio on 8 A100 GPUs (80G): \n    ```bash\n    python -m torch.distributed.run --nproc_per_node=8 compress_retrieval_clip_dtp.py --p 0.5 --epoch 10 \\\n    --pretrained pretrained/clip_large_retrieval_flickr.pth \\\n    --config ./configs/retrieval_flickr_clip.yaml \\\n    --output_dir output/retrieval_flickr_clip_compression_p0.5\n    ```\n\n<!-- * Resources\n\n    Reduce Ratio | Uncompressed Model | Compression Script | Training Log | Compressed Checkpoint | Evaluation Script\n    --- | :---: | :---: | :---: | :---: | :---: \n    0.5 | <a href=\"https://drive.google.com/uc?export=download&id=1-MZP6xQRnmLZr1_pqUK4TvOA8Ic7XCoI\">Download</a> | [Link](./scripts/compress_retrieval_flickr_clip_p0.5.sh) | <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_flickr_clip_p0.5_compressed.sh)\n    0.75 | <a href=\"https://drive.google.com/uc?export=download&id=1-MZP6xQRnmLZr1_pqUK4TvOA8Ic7XCoI\">Download</a> | [Link](./scripts/compress_retrieval_flickr_clip_p0.75.sh)| <a href=\"*****\">Download</a> | <a href=\"*****\">Download</a> | [Link](./scripts/evaluate_retrieval_flickr_clip_p0.75_compressed.sh) -->\n\n### Common Issues\n\n#### 1. Evaluation with single GPU\n   \n* For BLIP and CLIP models, evaluate the 2x compressed BLIP model on the NLVR2 dataset as an example:\n\n    ```bash\n    python compress_nlvr_dtp.py --evaluate \\\n    --pretrained output/nlvr_nlvr2_compression_p0.5/checkpoint_best.pth \\\n    --config ./configs/nlvr.yaml \\\n    --output_dir output/nlvr_nlvr2_compression_p0.5\n    ```\n\n#### 2. Compress with single GPU\n   \n* For BLIP and CLIP models, compress the BLIP model to half on the NLVR2 dataset as an example:\n\n    ```bash\n    python compress_nlvr_dtp.py --p 0.5 --epoch 15 \\\n    --pretrained pretrained/model_base_nlvr.pth \\\n    --config ./configs/nlvr.yaml \\\n    --output_dir output/nlvr_nlvr2_compression_p0.5\n    ```\n\n#### 3. Other issues\n\nYou can post them on the [Issues](https://github.com/double125/MADTP/issues) page.\n\n\n### Expected Folder Structures\n\n```\n├── annotation\n│   ├── answer_list.json\n│   ├── coco_gt\n│   │   ├── coco_karpathy_test_gt.json\n│   │   └── coco_karpathy_val_gt.json\n│   ├── ...\n├── clip                                               \n├── compress_caption_dtp.py             \n├── compress_nlvr_dtp.py                  \n├── compress ...    \n├── configs                                             \n├── data                                        \n├── datasets\n│   └── vision\n│       ├── coco\n│       ├── flickr\n│       ├── NLVR2     \n│       ├── ...                                                                               \n├── log                                     \n├── models            \n├── output                                    \n├── pretrained\n│   ├── bert-base-uncased\n│   ├── clip_large_retrieval_coco.pth\n│   ├── clip_large_retrieval_flickr.pth\n│   ├── ...       \n├──                                                                                \n├── transform                                                                           \n└── utils.py                                \n```\n\n### Acknowledgments\nThis code is built upon <a href=\"https://github.com/salesforce/BLIP\">BLIP</a>, <a href=\"https://github.com/openai/CLIP\">CLIP</a>, <a href=\"https://github.com/sdc17/UPop\">UPop</a>, and <a href=https://github.com/huggingface/pytorch-image-models/tree/main/timm>timm</a>. We thank the original authors for their open-source work.\n\n\n### Citation\nIf you find this work useful, please consider citing the corresponding paper:\n```bibtex\n@article{cao2024madtp,\n  title={MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer},\n  author={Jianjian, Cao and Peng, Ye and Shengze, Li and Chong, Yu and Yansong, Tang and Jiwen, Lu and Tao, Chen},\n  journal={IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2024}\n}\n```\n\n", "metadata": {"source": "github_readmes\\double125_MADTP_README.md", "filename": "double125_MADTP_README.md", "type": "readme_full"}}
{"id": "ericyinyzy_MTN_trajectory_README.md", "paper_id": "ericyinyzy_MTN_trajectory_README", "text": "**MTN_trajectory**: Multimodal Transformer Networks for Pedestrian Trajectory Prediction\n=======\n\n![logo](https://github.com/ericyinyzy/MTN_trajectory/blob/main/logo/MTN.png)\n\n😎PyTorch(1.6.0) training, evaluating models for MTN_trajectory.\nFor details see [Multimodal Transformer Networks for Pedestrian Trajectory Prediction](https://doi.org/10.24963/ijcai.2021/174) by Ziyi Yin, Ruijin Liu, Zhiliang Xiong, Zejian Yuan.\n\n## Data Preparation\n* PIE Dataset\n\nEnter the PIE directory.\n\n```\ncd path/to/MTN_trajectory/PIE/\n```\n\nDownload and extract PIE dataset:  \n```\ngit clone https://github.com/aras62/PIE.git\nmv PIE PIE_dataset\nunzip -d PIE_dataset/ PIE_dataset/annotations.zip\nunzip -d PIE_dataset/ PIE_dataset/annotations_attributes.zip\nunzip -d PIE_dataset/ PIE_dataset/annotations_vehicle.zip\nmv PIE_dataset/pie_data.py ./\n```\nDownload and extract [optical flow representations of PIE](https://drive.google.com/file/d/1RhsaPAAm90L8pZLJIrzd1VRN4_z09_na/view?usp=sharing) from google drive. \nWe expect the directory structure to be follwing: \n```\npath/to/MTN_trajectory/\n    PIE/\n        PIE_dataset/\n        PIE_model/\n        flow/\n        transformer/\n        pie_data.py\n        individual_TF.py\n        baselineUtils.py\n        train_pie.py\n        test_pie.py\n    JAAD/\n```\n\n* JAAD Dataset\n\nEnter the JAAD directory.\n\n```\ncd path/to/MTN_trajectory/JAAD/\n```\n\nDownload and extract JAAD dataset:  \n```\ngit clone https://github.com/ykotseruba/JAAD.git\nmv JAAD JAAD_dataset\nmv JAAD_dataset/jaad_data.py ./\n```\nDownload and extract [optical flow representations of JAAD](https://drive.google.com/file/d/1Zmf7H_mKlmnCmB-wn4X8EfFqMe3Z4w33/view?usp=sharing) from google drive. \nWe expect the directory structure to be follwing: \n```\npath/to/MTN_trajectory/\n    PIE/\n    JAAD/\n        JAAD_dataset/\n        JAAD_model/\n        flow/\n        transformer/\n        jaad_data.py\n        individual_TF.py\n        baselineUtils.py\n        train_jaad.py\n        test_jaad.py\n```\n\n## Set Envirionment\n\n* Linux ubuntu 16.04\n\n\n```\nconda create -n MTN python=3.7.9\n```\n\nAfter you create the environment, activate it\n\n```\nconda activate MTN \n```\n\nThen\n\n```\npip install torch==1.6.0+cu92 torchvision==0.7.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\npip install numpy==1.19.4\npip install scikit-learn==0.23.2\npip install opencv-python==4.4.0.46\npip install tqdm\n```\n\n## Training and Evaluation\n\n* on PIE Dataset\n\nTo train a model on PIE dataset, run following codes:\n```\ncd path/to/MTN_trajectory/PIE\npython train_pie.py\n```\nSaved model files (every 10 epoches) are in ./PIE_model during training.\n\nTo evaluate, run:\n```\npython test_pie.py\n```\n* on JAAD Dataset\n\nTo train a model on JAAD dataset, run following codes:\n```\ncd path/to/MTN_trajectory/JAAD\npython train_jaad.py\n```\nSaved model files (every 10 epoches) are in ./JAAD_model during training.\n\nTo evaluate, run:\n```\npython test_jaad.py\n```\nYou can test your own model by setting `model_path` in `test_jaad.py` or `test_pie.py`. According to our experiments, the test result from each training model may have slight differences as distinct initializations and GPU settings. \nOn PIE dataset, the MSE results are taken from 440 to 460. \nOn JAAD dataset, the MSE results are taken from 995 to 1030.\n\n## Citation\n```\n@InProceedings{MTN_trajectory,\nauthor = {Ziyi Yin and Ruijin Liu and Zhiliang Xiong and Zejian Yuan},\ntitle = {Multimodal Transformer Networks for Pedestrian Trajectory Prediction},\nbooktitle = {IJCAI},\nyear = {2021}\n}\n```\n## License\nMTN_trajectory is released under BSD 3-Clause License. Please see [LICENSE](LICENSE) file for more information.\n\n\n## Acknowledgements\n\n[PIE dataset](https://github.com/aras62/PIE)\n\n[JAAD dataset](https://github.com/ykotseruba/JAAD)\n\n[Trajectory-Transformer](https://github.com/FGiuliari/Trajectory-Transformer)\n\n[RAFT](https://github.com/princeton-vl/RAFT)\n", "metadata": {"source": "github_readmes\\ericyinyzy_MTN_trajectory_README.md", "filename": "ericyinyzy_MTN_trajectory_README.md", "type": "readme_full"}}
{"id": "Eurus-Holmes_MulT_README.md", "paper_id": "Eurus-Holmes_MulT_README", "text": "# MulT\n\n![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg)  \n\n> Pytorch implementation for the paper \"[Multimodal Transformer for Unaligned Multimodal Language Sequences](https://arxiv.org/pdf/1906.00295.pdf)\". \n\n> Original author's implementation is [here](https://github.com/yaohungt/Multimodal-Transformer).\n \n  \n## Datasets\n\n - Data files (containing processed MOSI, MOSEI and IEMOCAP datasets) can be downloaded from [here](https://www.dropbox.com/sh/hyzpgx1hp9nj37s/AAB7FhBqJOFDw2hEyvv2ZXHxa?dl=0).\n\n - To retrieve the meta information and the raw data, please refer to the [SDK for these datasets](https://github.com/A2Zadeh/CMU-MultimodalSDK).\n\n\n## Prerequisites\n- Python 3.6\n- [Pytorch (>=1.0.0) and torchvision](https://pytorch.org/)\n- CUDA 10.0 or above\n\n\n## Run the Code\n\n1. Create (empty) folders for data and pre-trained models:\n~~~~\nmkdir data pre_trained_models\n~~~~\n\nand put the downloaded data in 'data/'.\n\n2. Command as follows\n~~~~\npython main.py [--FLAGS]\n~~~~\n\nNote that the defualt arguments are for unaligned version of MOSEI. For other datasets, please refer to Supplmentary.\n\n### Results\n\n```\nnohup python main.py &\n```\n\n - unaligned version of MOSEI\n\nOutput: [nohup.out](https://github.com/Eurus-Holmes/MulT/blob/master/nohup.out)\n\n```\nMAE:  0.6139981\nCorrelation Coefficient:  0.6773945850196033\nmult_acc_7:  0.48873148744365746\nmult_acc_5:  0.5028976175144881\nF1 score:  0.8201431177436439\nAccuracy:  0.8200330214639515\n```\n\n### If Using CTC\n\nTransformer requires no CTC module. However, as we describe in the paper, CTC module offers an alternative to applying other kinds of sequence models (e.g., recurrent architectures) to unaligned multimodal streams.\n\nIf you want to use the CTC module, plesase install warp-ctc from [here](https://github.com/baidu-research/warp-ctc).\n\nThe quick version:\n~~~~\ngit clone https://github.com/SeanNaren/warp-ctc.git\ncd warp-ctc\nmkdir build; cd build\ncmake ..\nmake\ncd ../pytorch_binding\npython setup.py install\nexport WARP_CTC_PATH=/home/xxx/warp-ctc/build\n~~~~\n\n## Acknowledgement\nSome portion of the code were adapted from the [fairseq](https://github.com/pytorch/fairseq) repo.\n\n\n## Citation\n\n```tex\n@inproceedings{tsai2019multimodal,\n  title={Multimodal Transformer for Unaligned Multimodal Language Sequences},\n  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},\n  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},\n  year={2019}\n}\n```\n", "metadata": {"source": "github_readmes\\Eurus-Holmes_MulT_README.md", "filename": "Eurus-Holmes_MulT_README.md", "type": "readme_full"}}
{"id": "FirasGit_MeTra_README.md", "paper_id": "FirasGit_MeTra_README", "text": "MeTra (Medical Transformer)\n=========================\n\nThis repository contains the code to our corresponding publication \"Medical Transformer for Multimodal Survival Prediction in Intensive Care through Chest Radiographs and Clinical Data\".\n\n![alt text](assets/model.png)\n\n\n## Setup\n\nIn order to run this model, please download MIMIC-CXR-JPG and MIMIC-IV (version 1.0) datasets and follow the steps detailed in utils/mimic4extract to create the dataset splits.\n\nAdditionally, create a virtual environment (e.g. with conda):\n````\nconda create -n metra python=3.8\n````\n\nand run \n```\nconda activate metra\n```\nfollowed by\n```\npip install -r requirements.txt\n```\nto download and install the required dependencies.\n\nNote that we log our training results on Weights and Biases. (evtl. noch anpassen?)\n\n\n## Training\n\nOnce everything is set up, run the follow commands to train the model.\n\nTo train the EHR model, run:\n```\npython classification/training/trainer.py dataset=mimic_lab meta.transforms=True optimizer.lr=5e-6 model.output_logits=1 model=multi_modal_pretrained_vit_lab meta.prefix_name=EHR scheduler=cosine_annealing epochs=200 meta.batch_size=50 meta.cross_validation=False meta.num_workers=20 model.transforms.img_size=384 meta.gpus=[0] meta.imbalance_handler=None optimizer.name=AdamW optimizer.lr_scheduler=None model.meta.p_visual_dropout=1.0 model.meta.p_feature_dropout=0.0\n\n```\n\nTo train the CXR model, run:\n```\npython classification/training/trainer.py dataset=mimic_lab meta.transforms=True optimizer.lr=5e-6 model.output_logits=1 model=multi_modal_pretrained_vit_lab meta.prefix_name=CXR scheduler=cosine_annealing epochs=200 meta.batch_size=50 meta.cross_validation=False meta.num_workers=20 model.transforms.img_size=384 meta.gpus=[2] meta.imbalance_handler=None optimizer.name=AdamW optimizer.lr_scheduler=None model.meta.p_visual_dropout=.0 model.meta.p_feature_dropout=1.0\n```\n\nTo train the EHR+CXR model, run:\n```\nclassification/training/trainer.py dataset=mimic_lab meta.transforms=True optimizer.lr=5e-6 model.output_logits=1 model=multi_modal_pretrained_vit_lab meta.prefix_name=EHR+CXR scheduler=cosine_annealing epochs=200 meta.batch_size=50 meta.cross_validation=False meta.num_workers=20 model.transforms.img_size=384 meta.gpus=[3] meta.imbalance_handler=None optimizer.name=AdamW optimizer.lr_scheduler=None model.meta.p_visual_dropout=.3 meta.checkpoint_path=[ABSOLUTE PATH TO BEST EHR CHECKPOINT]\n```\n\n## Evaluation\n\nIn order to evaluate the models, open the jupyter notebook located at classification/eval/evaluate.ipynb and follow the stops. Note that you will need to provide the paths to the trained models.\n\n\n", "metadata": {"source": "github_readmes\\FirasGit_MeTra_README.md", "filename": "FirasGit_MeTra_README.md", "type": "readme_full"}}
{"id": "FusionBrainLab_OmniFusion_README.md", "paper_id": "FusionBrainLab_OmniFusion_README", "text": "# OmniFusion\n\n[![Hugging Face](https://img.shields.io/badge/Model-Hugging%20Face-yellow)](https://huggingface.co/AIRI-Institute/OmniFusion)\n\n[ArXiv](https://arxiv.org/abs/2404.06212) [Project page](https://airi-institute.github.io/OmniFusion/)\n\n\n\n**OmniFusion** is an advanced multimodal AI model designed to extend the capabilities of traditional language processing systems by integrating additional data modalities such as images, and potentially audio, 3D and video content.\n\n### ChangeLog\n[10/04/2024] OmniFusion-1.1 weights are uploaded to [Huggingface](https://huggingface.co/AIRI-Institute/OmniFusion/tree/main/OmniMistral-v1_1). Now the model can speak Russian :)\n\n[01/04/2024] Model training [source code](https://github.com/AIRI-Institute/OmniFusion/tree/main/OmniFusion/train_src) for OmniFusion-1.1 released\n\n[22/11/2023] OmniFusion weights are available on [Huggingface](https://huggingface.co/AIRI-Institute/OmniFusion)\n\n### Architecture\n\n<p align=\"left\">\n<img src=\"./content/architecture2.png\" width=\"100%\">\n</p>\n\n\nThe open source OmniFusion core is Mistral-7B. There are two versions of the model: the first uses one visual encoder CLIP-ViT-L, the second uses two encoders (CLIP-ViT-L and Dino V2). Initially focusing on images, we chose CLIP-ViT-L as a visual encoder due to for its efficient information transfer capabilities.\n\nThe most important component of OmniFusion is its adapter, a mechanism that allows the language model to interpret and incorporate information from different modalities. For the single encoder version, the adapter is a single-layer four-headed transformer layer that has shown superior performance compared to simpler linear layers or MLP structures. The model with two encoders uses an adapter that collects features from all layers of visual encoders, this adapter does not have an attention layer.\n\nThe adapter takes embeddings from the visual encoder (excluding the CLS token) and maps them to textual embeddings that are compatible with the language model.\n\nTo further enhance the multimodal capabilities of the model, we use learnable custom tokens to mark the beginning and end of visual data in a text sequence.\n\n\n### Training Process consists of two stages\n\n1. Pre-training the adapter on Image Captioning tasks (LAION, CC-4M, etc.).\n2. Once the adapter has learned to map visual embeddings to the language model's textual space, we proceed to unfreeze Mistral for improved understanding of dialog formats and complex queries.\n3. The dataset consists of data in English and Russian and has the following structure:\n\n| Task          | Dataset Source                     | #Samples   |\n| --------------| ---------------------------------- |  --------- |\n| Caption       | ShareGPT4V                         | 100K       |\n| VQA           | COCO, SAM-9K                       | 20K, 9K    |\n| WebQA         | WebData                            | 1.5K       |\n| OCRQA         | TextVQA, OCRVQA                    | 120K       |\n| Conversation  | LLaVA-v1.5-665K, OCRVQA            | 665K       |\n| DocVQA        | Proprietary data (ru)              | 20K        |\n| Text-only SFT | Proprietary data (ru), Alpaca (en) | 10K        |\n\n### How to Use\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom urllib.request import urlopen\nimport torch.nn as nn\nfrom huggingface_hub import hf_hub_download\n\n# Loading some sources of the projection adapter and image encoder\nhf_hub_download(repo_id=\"AIRI-Institute/OmniFusion\", filename=\"models.py\", local_dir='./')\nfrom models import CLIPVisionTower\n\nDEVICE = \"cuda:0\"\nPROMPT = \"This is a dialog with AI assistant.\\n\"\n\ntokenizer = AutoTokenizer.from_pretrained(\"AIRI-Institute/OmniFusion\", subfolder=\"OmniMistral-v1_1/tokenizer\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"AIRI-Institute/OmniFusion\", subfolder=\"OmniMistral-v1_1/tuned-model\", torch_dtype=torch.bfloat16, device_map=DEVICE)\n\nhf_hub_download(repo_id=\"AIRI-Institute/OmniFusion\", filename=\"OmniMistral-v1_1/projection.pt\", local_dir='./')\nhf_hub_download(repo_id=\"AIRI-Institute/OmniFusion\", filename=\"OmniMistral-v1_1/special_embeddings.pt\", local_dir='./')\nprojection = torch.load(\"OmniMistral-v1_1/projection.pt\", map_location=DEVICE)\nspecial_embs = torch.load(\"OmniMistral-v1_1/special_embeddings.pt\", map_location=DEVICE)\n\nclip = CLIPVisionTower(\"openai/clip-vit-large-patch14-336\")\nclip.load_model()\nclip = clip.to(device=DEVICE, dtype=torch.bfloat16)\n\ndef gen_answer(model, tokenizer, clip, projection, query, special_embs, image=None):\n    bad_words_ids = tokenizer([\"\\n\", \"</s>\", \":\"], add_special_tokens=False).input_ids + [[13]]\n    gen_params = {\n        \"do_sample\": False,\n        \"max_new_tokens\": 50,\n        \"early_stopping\": True,\n        \"num_beams\": 3,\n        \"repetition_penalty\": 1.0,\n        \"remove_invalid_values\": True,\n        \"eos_token_id\": 2,\n        \"pad_token_id\": 2,\n        \"forced_eos_token_id\": 2,\n        \"use_cache\": True,\n        \"no_repeat_ngram_size\": 4,\n        \"bad_words_ids\": bad_words_ids,\n        \"num_return_sequences\": 1,\n    }\n    with torch.no_grad():\n        image_features = clip.image_processor(image, return_tensors='pt')\n        image_embedding = clip(image_features['pixel_values']).to(device=DEVICE, dtype=torch.bfloat16)\n\n        projected_vision_embeddings = projection(image_embedding).to(device=DEVICE, dtype=torch.bfloat16)\n        prompt_ids = tokenizer.encode(f\"{PROMPT}\", add_special_tokens=False, return_tensors=\"pt\").to(device=DEVICE)\n        question_ids = tokenizer.encode(query, add_special_tokens=False, return_tensors=\"pt\").to(device=DEVICE)\n\n        prompt_embeddings = model.model.embed_tokens(prompt_ids).to(torch.bfloat16)\n        question_embeddings = model.model.embed_tokens(question_ids).to(torch.bfloat16)\n\n        embeddings = torch.cat(\n            [\n                prompt_embeddings,\n                special_embs['SOI'][None, None, ...],\n                projected_vision_embeddings,\n                special_embs['EOI'][None, None, ...],\n                special_embs['USER'][None, None, ...],\n                question_embeddings,\n                special_embs['BOT'][None, None, ...]\n            ],\n            dim=1,\n        ).to(dtype=torch.bfloat16, device=DEVICE)\n        out = model.generate(inputs_embeds=embeddings, **gen_params)\n    out = out[:, 1:]\n    generated_texts = tokenizer.batch_decode(out)[0]\n    return generated_texts\n\nimg_url = \"https://i.pinimg.com/originals/32/c7/81/32c78115cb47fd4825e6907a83b7afff.jpg\"\nquestion = \"What is the sky color on this image?\"\nimg = Image.open(urlopen(img_url))\n\nanswer = gen_answer(\n    model,\n    tokenizer,\n    clip,\n    projection,\n    query=question,\n    special_embs=special_embs,\n    image=img\n)\n\nimg.show()\nprint(question)\nprint(answer)\n```\n\n### Results\n\nOmniFusion was benchmarked against the latest multimodal SOTA models. It excelled in generative metrics and classification benchmarks like TextVQA.\n\nOmniFusion-1.1 (GigaChat LLM) results on various benchmarks:\n<p align=\"left\">\n<img src=\"./content/radar_plot_gigachat.png\" width=\"50%\">\n</p>\n\nOmifusion-1.0 results:\n<p align=\"left\">\n<img src=\"./content/radar.png\" width=\"50%\">\n</p>\n\nOmifusion-1.1 (Mistral)\n| Model                                  | textvqa| scienceqa  | pope      | gqa      | ok_vqa  |\n| -------------------------------------- | ------ | ---------- | --------- | -------- | ------- |\n| OmniFusion-1.1 (one encoder, Mistral)  | **0.4893** | **0.6802**     | 0.7818    | 0.4600   | 0.5187  |\n| OmniFusion-1.1 (two encoders, Mistral) | 0.4755 | 0.6732     | **0.8153**    | **0.4761**   | **0.5317**  |\n\nOmifusion-1.0 (previous version) Performance on Visual Dialog Benchmark\n\n| Model        | NDCG | MRR  | Recall@1 | Recall@5 | Recall@10 |\n| ------------ | ---- | ---- | -------- | -------- | --------- |\n| OmniFusion   | 25.91| 10.78| 4.74     | 13.80    | 20.53     |\n| LLaVA-13B    | 24.74| 8.91 | 2.98     | 10.80    | 18.02     |\n\n\n\n### OmniFusion-1.1 examples\n<p align=\"left\">\n<img src=\"https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/content/ex1.png\" width=\"100%\">\n</p>\n<p align=\"left\">\n<img src=\"https://raw.githubusercontent.com/AIRI-Institute/OmniFusion/main/content/ex2.png\" width=\"100%\">\n</p>\n\n### OmniFusion-1.0 Examples\n<p align=\"left\">\n<img src=\"./content/examples.png\" width=\"100%\">\n</p>\n\n### Future Plans\n\nWork is underway on a version that understands Russian, uses ImageBind encoders, and accepts more modalities (sound, 3D, video). Stay tuned for updates on GitHub!\n\n### Authors\n\nThe FusionBrain scientific group from the AIRI Institute, in collaboration with scientists from Sber AI, led the model's development.\n\nMain contributors:\n+ Anton Razzhigaev: [Blog](https://t.me/abstractDL)\n+ Elizaveta Goncharova\n+ Matvey Mihkalchuk\n+ Maxim Kurkin\n+ Irina Abdullaeva\n+ Denis Dimitrov [Blog](https://t.me/dendi_math_ai)\n+ Andrey Kuznetsov [Blog](https://t.me/complete_ai)\n", "metadata": {"source": "github_readmes\\FusionBrainLab_OmniFusion_README.md", "filename": "FusionBrainLab_OmniFusion_README.md", "type": "readme_full"}}
{"id": "gabeur_mmt_README.md", "paper_id": "gabeur_mmt_README", "text": "# MMT: Multi-modal Transformer for Video Retrieval\n\n![architecture](figs/Cross_mod_architecture.png)\n\n## Intro\n\nThis repository provides the code for training our video retrieval cross-modal architecture.\nOur approach is described in the paper \"Multi-modal Transformer for Video Retrieval\" [[arXiv](https://arxiv.org/abs/2007.10639), [webpage](http://thoth.inrialpes.fr/research/MMT/)]\n\nOur proposed Multi-Modal Transformer (MMT) aggregates sequences of multi-modal features (e.g. appearance, motion, audio, OCR, etc.) from a video. It then embeds the aggregated multi-modal feature to a shared space with text for retrieval. It achieves state-of-the-art performance on MSRVTT, ActivityNet and LSMDC datasets.\n\n## Installing\n```bash\ngit clone https://github.com/gabeur/mmt.git\n```\n\n## Requirements\n* Python 3.7\n* Pytorch 1.4.0\n* Transformers 3.1.0\n* Numpy 1.18.1\n\n```bash\ncd mmt\n# Install the requirements\npip install -r requirements.txt\n```\n\n## ECCV paper\n\nIn order to reproduce the results of our ECCV 2020 Spotlight paper, please first download the video features from [this page](http://thoth.inrialpes.fr/research/video-features/) by running the following commands:\n\n```bash\n# Create and move to mmt/data directory\nmkdir data\ncd data\n# Download the video features\nwget http://pascal.inrialpes.fr/data2/vgabeur/video-features/MSRVTT.tar.gz\nwget http://pascal.inrialpes.fr/data2/vgabeur/video-features/activity-net.tar.gz\nwget http://pascal.inrialpes.fr/data2/vgabeur/video-features/LSMDC.tar.gz\n# Extract the video features\ntar -xvf MSRVTT.tar.gz\ntar -xvf activity-net.tar.gz\ntar -xvf LSMDC.tar.gz\n```\n\nDownload the checkpoints:\n```bash\n# Create and move to mmt/data/checkpoints directory\nmkdir checkpoints\ncd checkpoints\n# Download checkpoints\nwget http://pascal.inrialpes.fr/data2/vgabeur/mmt/data/checkpoints/HowTo100M_full_train.pth\nwget http://pascal.inrialpes.fr/data2/vgabeur/mmt/data/checkpoints/MSRVTT_jsfusion_trainval.pth\nwget http://pascal.inrialpes.fr/data2/vgabeur/mmt/data/checkpoints/prtrn_MSRVTT_jsfusion_trainval.pth\n```\n\nYou can then run the following scripts:\n\n### MSRVTT\n\n#### Training from scratch\n\nTraining + evaluation:\n```bash\npython -m train --config configs_pub/eccv20/MSRVTT_jsfusion_trainval.json\n```\n\nEvaluation from checkpoint:\n```bash\npython -m train --config configs_pub/eccv20/MSRVTT_jsfusion_trainval.json --only_eval --load_checkpoint data/checkpoints/MSRVTT_jsfusion_trainval.pth\n```\n\nExpected results:\n```\nMSRVTT_jsfusion_test:\nt2v_metrics/R1/final_eval: 24.1\nt2v_metrics/R5/final_eval: 56.4\nt2v_metrics/R10/final_eval: 69.6\nt2v_metrics/R50/final_eval: 90.4\nt2v_metrics/MedR/final_eval: 4.0\nt2v_metrics/MeanR/final_eval: 25.797\nt2v_metrics/geometric_mean_R1-R5-R10/final_eval: 45.56539387310681\nv2t_metrics/R1/final_eval: 25.9\nv2t_metrics/R5/final_eval: 58.1\nv2t_metrics/R10/final_eval: 69.3\nv2t_metrics/R50/final_eval: 90.8\nv2t_metrics/MedR/final_eval: 4.0\nv2t_metrics/MeanR/final_eval: 22.852\nv2t_metrics/geometric_mean_R1-R5-R10/final_eval: 47.06915231647284\n```\n\n#### Finetuning from a HowTo100M pretrained model:\n\nTraining + evaluation:\n```bash\npython -m train --config configs_pub/eccv20/prtrn_MSRVTT_jsfusion_trainval.json --load_checkpoint data/checkpoints/HowTo100M_full_train.pth\n```\n\nEvaluation from checkpoint:\n```bash\npython -m train --config configs_pub/eccv20/prtrn_MSRVTT_jsfusion_trainval.json --only_eval --load_checkpoint data/checkpoints/prtrn_MSRVTT_jsfusion_trainval.pth\n```\n\nExpected results:\n```\nMSRVTT_jsfusion_test:\nt2v_metrics/R1/final_eval: 25.8\nt2v_metrics/R5/final_eval: 57.2\nt2v_metrics/R10/final_eval: 69.3\nt2v_metrics/R50/final_eval: 90.7\nt2v_metrics/MedR/final_eval: 4.0\nt2v_metrics/MeanR/final_eval: 22.355\nt2v_metrics/geometric_mean_R1-R5-R10/final_eval: 46.76450299746546\nv2t_metrics/R1/final_eval: 26.1\nv2t_metrics/R5/final_eval: 57.8\nv2t_metrics/R10/final_eval: 68.5\nv2t_metrics/R50/final_eval: 90.6\nv2t_metrics/MedR/final_eval: 4.0\nv2t_metrics/MeanR/final_eval: 20.056\nv2t_metrics/geometric_mean_R1-R5-R10/final_eval: 46.92665942024404\n```\n\n### ActivityNet\n\nTraining from scratch\n```bash\npython -m train --config configs_pub/eccv20/ActivityNet_val1_trainval.json\n```\n\n### LSMDC\n\nTraining from scratch\n```bash\npython -m train --config configs_pub/eccv20/LSMDC_full_trainval.json\n```\n\n## References\nIf you find this code useful or use the \"s3d\"(motion) video features, please consider citing:\n```\n@inproceedings{gabeur2020mmt,\n    TITLE = {{Multi-modal Transformer for Video Retrieval}},\n    AUTHOR = {Gabeur, Valentin and Sun, Chen and Alahari, Karteek and Schmid, Cordelia},\n    BOOKTITLE = {{European Conference on Computer Vision (ECCV)}},\n    YEAR = {2020}\n}\n```\n\nThe features \"face\", \"ocr\", \"rgb\"(appearance), \"scene\" and \"speech\" were extracted by the authors of [Collaborative Experts](https://github.com/albanie/collaborative-experts). If you use those features, please consider citing:\n```\n@inproceedings{Liu2019a,\n    author = {Liu, Y. and Albanie, S. and Nagrani, A. and Zisserman, A.},\n    booktitle = {British Machine Vision Conference},\n    title = {Use What You Have: Video retrieval using representations from collaborative experts},\n    date = {2019}\n}\n```\n\n## Acknowledgements\n\nOur code is structured following the [template](https://github.com/victoresque/pytorch-template) proposed by @victoresque. Our code is based on the implementation of [Collaborative Experts](https://github.com/albanie/collaborative-experts), [Transformers](https://github.com/huggingface/transformers) and [Mixture of Embedding Experts](https://github.com/antoine77340/Mixture-of-Embedding-Experts). We thank Maksim Dzabraev for discovering bugs in our implementation and notifying us of the issues (See the issues section for more detail).\n", "metadata": {"source": "github_readmes\\gabeur_mmt_README.md", "filename": "gabeur_mmt_README.md", "type": "readme_full"}}
{"id": "GaochangWu_FMF-Benchmark_README.md", "paper_id": "GaochangWu_FMF-Benchmark_README", "text": "# Fused Magnesium Smelting Process Benchmark\n### [Project Page](https://gaochangwu.github.io/FmFormer/FmFormer.html) | [Paper](https://arxiv.org/abs/2406.09016)\n\nThis is a cross-modal benchmark for the fused magnesium smelting process. The benchmark contains a total of 3 hours of synchronously acquired videos and three-phase alternating current data from different production batches. \n\n![Teaser Image](https://gaochangwu.github.io/FmFormer/images/FMF.png)\nCross-modal information is exploited to perform anomaly detection in the context of a typical industrial process, fused magnesium smelting, as illustrated in (a). The picture at the bottom left shows an anomaly region on the furnace shell, whose visual feature is difficult to detect due to interference from heavy water mist. A novel FMF Transformer (FmFormer) is proposed using synchronous acquired video and current data, to explore the internal features of each modality by self-attention and the correlation feature across modalities by cross-attention, as shown in (b).\n\n## News!\n- 12/08/2025:  The Baidu Disk dataset link has been updated.\n- 03/04/2025:  Our dataset, featuring diverse sampling rates, is now available for download.\n- 02/13/2025:  Our code is now available.\n- 12/16/2024:  Our code is cooming soon.\n- 11/25/2024:  Our dataset with pixel-level annotations is now available for download. You can access it via: \n\n  **Google Drive**:\nhttps://drive.google.com/file/d/12vQ_CHqKQ5TOK6i9whKO39OZO4Nssz_e/view?usp=sharing\n\n  **Baidu Disk**: \nhttps://pan.baidu.com/s/1es0GzxQnFLiJigsIs5wB4A?pwd=5nz5   (Fetch Code: **5nz5**)\n\n  **Baidu Disk**: \n https://pan.baidu.com/s/1fh6xiVDn1kxJiM17eUZEIQ?pwd=ebu3   (Fetch Code: **ebu3**) **Note:** Synchronous acquisition with different sampling rates.\n- 11/01/2024:  We are in the process of preparing the datasets, which are currently not very convenient for research usage. If you would like to access the dataset in advance, please feel free to contact us: wugc\\at mail\\dot neu\\dot edu\\dot cn.\n\n## Dataset Description\n\nThis dataset includes three sets of data stored in `.mat` format, comprising $2.7 \\times 10^5$ samples with pixel-level annotations. For access to our large-scale class-level annotated dataset, which contains $2.2 \\times 10^6$ samples, please feel free to contact us. In our pixel-level annotated dataset, each file contains the following components:\n\n- **`video`**: A 4D tensor of shape `(height, width, RGB channel, N)` representing the 3D video modality. Here, `N` denotes the number of frames.\n  \n- **`current`**: A 2D array of shape `(phase channel, N)` representing the 1D three-phase alternating current modality.\n\n- **`label`**: A 3D tensor of shape `(height, width, N)` representing pixel-level normal (`0`) and abnormal (`1`) masks. You can convert these masks to class-level labels using the formula:\n  ```matlab\n  class_label = single(sum(label, [1, 2]) > 0.5);  # Matlab code\n  ```\n\n- **`train_test_index`**: A 1D array of shape `(1, N)` indicating the train-test split. A value of `0` represents a training example, while `1` indicates a test example.\n\nWhen using Python h5py to read videos and labels, the dimensions of these data will be reversed. Please pay attention to the transformation of dimensions when building your Dataset. This is a example for obtaining videos and labels using h5py:\n```python\nimport h5py\nimport numpy as np\n\nsample_path = \"yourPath/FMF-Benchmark/pixel-level/videos/SaveToAvi-4-19-21_40_52-6002.mat\"\nwith h5py.File(sample_path, 'r') as reader:\n    video = reader['data']   # In MATLAB, size of video is [H W C T], but in h5py the size is [T C W H]\n    label = reader['label']  # In MATLAB, size of label is [H W T], but in h5py the size is [T W H]\n    video_clip = np.array(video[0:10], dtype=np.uint8)  # Get a 10 frame video clip\n    clip_label = np.array(label[9], dtype=np.uint8)     # Get pixel-level label for video clip\nvideo_clip = np.transpose(video_clip, axes=(0, 1, 3, 2))  # [10 C W H] -> [10 C H W]\nclip_label = np.transpose(clip_label, axes=(1, 0))     # [W H] -> [H W]\nclip_label_cls = np.max(clip_label)  # Get class-level label for video clip\n```\n\n## Code Description\nThis is an official pytorch implementation of our IEEE TCSVT 2024 paper [Cross-Modal Learning for Anomaly Detection in Complex Industrial Process: Methodology and Benchmark](https://ieeexplore.ieee.org/document/10744600). \nWe propose a cross-modal Transformer (dubbed FmFormer), designed to facilitate anomaly detection by exploring the correlation between visual features (video) and process variables (current) in the context of the fused magnesium smelting process.\n\n### Installation\n\nThis project mainly requires the following python packages:\n- torch\n- numpy \n- fvcore \n- einops \n- h5py\n- pandas\n\nAfter downloading this project, run the following code to build the codebase:\n```\ncd YourProjectPath/FMF-Benchmark\npython setup.py build develop\n```\n### Testing FmFormer\n\nUse the following code to test the cross-modal classification network:\n```\npython tools/test_cls.py --cfg './configs/CfgForViCu_Cls.yaml' MODEL.PRETRAINED './state_dict_vicu_cls.pth' DATA.SAMPLING_RATE 18\n```\n\n### Acknowledgements\n\nThis project is based on [PySlowFast](https://github.com/facebookresearch/SlowFast) and [TimeSformer](https://github.com/facebookresearch/TimeSformer).\n\n\n## BibTex Citation\n\nIf you find this benchmark useful, please cite our paper☺️.\n```\n@article{wu2024crossmodal,\n  title={Cross-Modal Learning for Anomaly Detection in Complex Industrial Process: Methodology and Benchmark},\n  author={Gaochang Wu and Yapeng Zhang and Lan Deng and Jingxin Zhang and Tianyou Chai},\n  year={2025},\n  volume={35},\n  number={3},\n  page={2632 - 2645},\n  DOI={10.1109/TCSVT.2024.3491865},\n  journal={IEEE Transactions on Circuits and Systems for Video Technology}\n}\n```\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n", "metadata": {"source": "github_readmes\\GaochangWu_FMF-Benchmark_README.md", "filename": "GaochangWu_FMF-Benchmark_README.md", "type": "readme_full"}}
{"id": "georgesterpu_Taris_README.md", "paper_id": "georgesterpu_Taris_README", "text": "# Taris\nTransformer-based online speech recognition system with TensorFlow 2\n\n### About\n\nTaris is an approach to online speech recognition described in [1].\nThe system dynamically segments a spoken sentence by learning to count the number of spoken words therein.\nDecoding is conditioned on a dynamic window of segments, instead of an entire utterance as in the original sequence to sequence architecture.\n\nThis repository also maintains the audio-visual alignment and fusion strategy AV Align [2,3] currently implemented with the Transformer stacks instead of the original recurrent networks [4]\n\n\n### Overview\nTo decode online, Taris learns to count the number of words in a spoken sentence. As we show in [1], \nthis task facilitates the partitioning of the speech input into segments that can be decoded eagerly.\nHowever, a longer context is needed in order to match the accuracy of an offline system.\n\nThe figure below illustrates an example where the decoder uses two look-back and look-ahead segments\nto condition all the characters within a given word in the output modality.\n\n![diagram](./img/taris.png)\n\nOnce all the characters in `decision` are processed and the system predicts a blank space token,\nthe attention distribution advances by one more segment, and is used in computing audio context vectors\nfor every character in the next word `away`.\n\n\n![diagram](./img/taris2.png)\n\n\n### How to use\n\n##### Launch scripts\nThe script `run_audio.py` launches audio-only experiments.\\\nRelevant system flags are:\n\n+ `--architecture (default: transformer)`\n+ `--transformer_online_encoder (default: False)`\n+ `--transformer_encoder_lookahead (default: 11)`\n+ `--transformer_encoder_lookback (default: 11)`\n+ `--transformer_online_decoder (default: False)`\n+ `--transformer_decoder_lookahead (default: 5)`\n+ `--transformer_decoder_lookback (default: 5)`\n\nThe script `run_audiovisual.py` launches audio-visual experiments implementing the AV Align strategy with a Transformer,\nreproducing the work in [4, 5]. By default, the Action Unit regularisation loss, controlled by the `--au_loss` flag,\nis set to `False`. Please ensure that `--architecture=av_transformer`.\n\n##### Data preparation\n\nOur published articles used the legacy [avsr-tf1](https://github.com/georgesterpu/avsr-tf1) project for data preparation.\n\\\nThe most notable change in Taris is the audio spectrogram processing with `librosa` as opposed to the native TensorFlow signal processing API. \nTherefore, this repository provides **experimental** support only, and has not been validated thoroughly. \n\n\nPlease see the example script `write_records.py`\n\nFor AVSR experiments it is required to process your video clips in advance with the TadasBaltrusaitis/OpenFace tool.\n\\\nPlease refer to the example script `extract_faces.py`.\n\n### References\n\n[1] Learning to Count Words in Fluent Speech enables Online Speech Recognition\\\nGeorge Sterpu, Christian Saam, Naomi Harte\\\nIEEE Spoken Language Technology Workshop (SLT 2021).\\\n[[arXiv](https://arxiv.org/abs/2006.04928)]\n\n[2] How to Teach DNNs to Pay Attention to the Visual Modality in Speech Recognition\\\nGeorge Sterpu, Christian Saam, Naomi Harte\\\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020\\\n[ [pdf accepted version](https://raw.githubusercontent.com/georgesterpu/georgesterpu.github.io/master/papers/taslp2020.pdf) ] [[IEEE version](https://ieeexplore.ieee.org/document/9035650)]\n\n[3] Attention-based Audio-Visual Fusion for Robust Automatic Speech Recognition\\\nGeorge Sterpu, Christian Saam, Naomi Harte\\\nin ICMI 2018\\\n[[arXiv](https://arxiv.org/abs/1809.01728)]\n\n[4] Should we hard-code the recurrence concept or learn it instead ?\nExploring the Transformer architecture for Audio-Visual Speech Recognition \\\nGeorge Sterpu, Christian Saam, Naomi Harte\\\nInterspeech 2020\\\n[[arXiv](https://arxiv.org/abs/2005.09297)]\n\n[5] AV Taris: Online Audio-Visual Speech Recognition\\\nGeorge Sterpu and Naomi Harte\\\nUnder Review\n[[arXiv](https://arxiv.org/abs/2012.07467)]\n\n\n### Dependencies\n```\ntensorflow >= 2.4\ntensorflow_addons\nmatplotlib\n```\nFor data preparation:\n```\nlibrosa\nimageio (AVSR)\ncv2 (AVSR)\nTadasBaltrusaitis/OpenFace (AVSR)\n```\nPlease use the latest versions available on your platform, and let me know of any breakings and deprecations.\n\\\nThis project is actively maintained.\n", "metadata": {"source": "github_readmes\\georgesterpu_Taris_README.md", "filename": "georgesterpu_Taris_README.md", "type": "readme_full"}}
{"id": "georgian-io_Multimodal-Toolkit_README.md", "paper_id": "georgian-io_Multimodal-Toolkit_README", "text": "# Multimodal Transformers | Transformers with Tabular Data\n\n--------------------------------------------------------------------------------\n**[Documentation](https://multimodal-toolkit.readthedocs.io/en/latest/index.html)** | **[Colab Notebook](https://multimodal-toolkit.readthedocs.io/en/latest/notes/colab_example.html)** | **[Blog Post](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4)**\n\nA toolkit for incorporating multimodal data on top of text data for classification\nand regression tasks. It uses HuggingFace transformers as the base model for text features.\nThe toolkit adds a combining module that takes the outputs of the transformer in addition to categorical and numerical features\nto produce rich multimodal features for downstream classification/regression layers.\nGiven a pretrained transformer, the parameters of the combining module and transformer are trained based\non the supervised task. For a brief literature review, check out the accompanying [blog post](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4) on Georgian's Impact Blog. \n\n![](https://drive.google.com/uc?export=view&id=1kyExPDQNkg49NRYgcw2wk8xg4QtQ6Ppt)\n\n\n\n## Installation\nThe code was developed in Python 3.7 with PyTorch and Transformers 4.26.1.\nThe multimodal specific code is in `multimodal_transformers` folder.\n```\npip install multimodal-transformers\n```\n\n## Supported Transformers\nThe following Hugging Face Transformers are supported to handle tabular data. See the documentation [here](https://multimodal-toolkit.readthedocs.io/en/latest/modules/model.html#module-multimodal_transformers.model.tabular_transformers).\n* [BERT](https://huggingface.co/transformers/v3.1.0/model_doc/bert.html) from Devlin et al.:\n[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (ACL 2019)\n* [ALBERT](https://huggingface.co/transformers/v3.1.0/model_doc/albert.html) from Lan et al.: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n](https://arxiv.org/abs/1909.11942) (ICLR 2020)\n* [DistilBERT](https://huggingface.co/transformers/v3.1.0/model_doc/distilbert.html) from Sanh et al.: \n[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) (NeurIPS 2019)\n* [RoBERTa](https://huggingface.co/transformers/v3.1.0/model_doc/roberta.html) \nfrom Liu et al.: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\n* [XLM](https://huggingface.co/transformers/v3.1.0/model_doc/xlm.html) from Lample et al.: [Cross-lingual Language Model Pretraining\n](https://arxiv.org/abs/1901.07291) (NeurIPS 2019)\n* [XLNET](https://huggingface.co/transformers/v3.1.0/model_doc/xlnet.html) from Yang et al.:\n[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) (NeurIPS 2019)\n* [XLM-RoBERTa](https://huggingface.co/transformers/v3.1.0/model_doc/xlmroberta.html) from Conneau et al.:\n[Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) (ACL 2020)\n\n## Included Datasets\nThis repository also includes two kaggle datasets which contain text data and \nrich tabular features\n* [Women's Clothing E-Commerce Reviews](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) for Recommendation Prediction (Classification)\n* [Melbourne Airbnb Open Data](https://www.kaggle.com/tylerx/melbourne-airbnb-open-data) for Price Prediction (Regression)\n* [PetFindermy Adoption Prediction](https://www.kaggle.com/c/petfinder-adoption-prediction) for Pet Adoption Speed Prediction (Multiclass Classification)\n \n\n## Working Examples\nTo quickly see these models in action on say one of the above datasets with preset configurations \n```\n$ python main.py ./datasets/Melbourne_Airbnb_Open_Data/train_config.json\n```\n\nOr if you prefer command line arguments run \n```\n$ python main.py \\\n    --output_dir=./logs/test \\\n    --task=classification \\\n    --combine_feat_method=individual_mlps_on_cat_and_numerical_feats_then_concat \\\n    --do_train \\\n    --model_name_or_path=distilbert-base-uncased \\\n    --data_path=./datasets/Womens_Clothing_E-Commerce_Reviews \\\n    --column_info_path=./datasets/Womens_Clothing_E-Commerce_Reviews/column_info.json\n```\n`main.py` expects a `json` file detailing which columns in a dataset contain text, \ncategorical, or numerical input features. It also expects a path to the folder where\nthe data is stored as `train.csv`, and `test.csv`(and if given `val.csv`).For more details on the arguments see \n`multimodal_exp_args.py`.\n### Notebook Introduction\nTo see the modules come together in a notebook: \\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/georgianpartners/Multimodal-Toolkit/blob/master/notebooks/text_w_tabular_classification.ipynb)\n\n## Included Methods\n| combine feat method |description | requires both cat and num features | \n|:--------------|:-------------------|:-------|\n| text_only | Uses just the text columns as processed by a HuggingFace transformer before final classifier layer(s). Essentially equivalent to HuggingFace's `ForSequenceClassification` models |  False | \n| concat | Concatenate transformer output, numerical feats, and categorical feats all at once before final classifier layer(s) | False |\n| mlp_on_categorical_then_concat | MLP on categorical feats then concat transformer output, numerical feats, and processed categorical feats before final classifier layer(s) | False (Requires cat feats)\n| individual_mlps_on_cat_and_numerical_feats_then_concat | Separate MLPs on categorical feats and numerical feats then concatenation of transformer output, with processed numerical feats, and processed categorical feats before final classifier layer(s). | False\n| mlp_on_concatenated_cat_and_numerical_feats_then_concat | MLP on concatenated categorical and numerical feat then concatenated with transformer output before final classifier layer(s) | True\n| attention_on_cat_and_numerical_feats | Attention based summation of transformer outputs, numerical feats, and categorical feats queried by transformer outputs before final classifier layer(s). | False\n| gating_on_cat_and_num_feats_then_sum | Gated summation of transformer outputs, numerical feats, and categorical feats before final classifier layer(s). Inspired by [Integrating Multimodal Information in Large Pretrained Transformers](https://www.aclweb.org/anthology/2020.acl-main.214.pdf) which performs the mechanism for each token. | False\n| weighted_feature_sum_on_transformer_cat_and_numerical_feats | Learnable weighted feature-wise sum of transformer outputs, numerical feats and categorical feats for each feature dimension before final classifier layer(s) | False\n### Simple baseline model\nIn practice, taking the categorical and numerical features as they are and just tokenizing them and just concatenating them to \nthe text columns as extra text sentences is a strong baseline. To do that here, just specify all the categorical and numerical\ncolumns as text columns and set `combine_feat_method` to `text_only`. For example for each of the included sample datasets in `./datasets`, \nin `train_config.json` change `combine_feat_method` to `text_only` and `column_info_path` to  `./datasets/{dataset}/column_info_all_text.json`.\n\nIn the experiments below this baseline corresponds to Combine Feat Method being `unimodal`.\n\n## Results\nThe following tables shows the results on the two included datasets's respective test sets, by running main.py \nNon specified parameters are the default. \n\n### Review Prediction\nSpecific training parameters can be seen in `datasets/Womens_Clothing_E-Commerce_Reviews/train_config.json`.\n\nThere are **2** text columns, **3** categorical columns, and **3** numerical columns.\n\nModel | Combine Feat Method |F1 | PR AUC\n--------|-------------|---------|------- \nBert Base Uncased | text_only | 0.957 | 0.992\nBert Base Uncased | unimodal | **0.968** | **0.995**\nBert Base Uncased | concat | 0.958 | 0.992\nBert Base Uncased | individual_mlps_on_cat_and_numerical_feats_then_concat | 0.959 | 0.992\nBert Base Uncased | attention_on_cat_and_numerical_feats | 0.959 | 0.992\nBert Base Uncased | gating_on_cat_and_num_feats_then_sum | 0.961 | 0.994\nBert Base Uncased | weighted_feature_sum_on_transformer_cat_and_numerical_feats | 0.962 | 0.994\n\n\n### Pricing Prediction\nSpecific training parameters can be seen in `datasets/Melbourne_Airbnb_Open_Data/train_config.json`.\n\nThere are **3** text columns, **74** categorical columns, and **15** numerical columns.\n\nModel | Combine Feat Method | MAE | RMSE | \n--------|-------------|---------|------- | \nBert Base Multilingual Uncased | text_only | 82.74 | 254.0 |\nBert Base Multilingual Uncased | unimodal | 79.34 | 245.2 |\nBert Base Uncased | concat | **65.68** | 239.3 \nBert Base Multilingual Uncased | individual_mlps_on_cat_and_numerical_feats_then_concat | 66.73 | **237.3**  \nBert Base Multilingual Uncased | attention_on_cat_and_numerical_feats | 74.72 |246.3\nBert Base Multilingual Uncased | gating_on_cat_and_num_feats_then_sum | 66.64 | 237.8 \nBert Base Multilingual Uncased | weighted_feature_sum_on_transformer_cat_and_numerical_feats | 71.19 | 245.2 \n\n\n### Pet Adoption Prediction\nSpecific training parameters can be seen in `datasets/PetFindermy_Adoption_Prediction`\nThere are **2** text columns, **14** categorical columns, and **5** numerical columns.\n\nModel | Combine Feat Method | F1_macro | F1_micro | \n--------|-------------|---------|------- | \nBert Base Multilingual Uncased | text_only | 0.088 | 0.281 |\nBert Base Multilingual Uncased | unimodal | 0.089 | 0.283 |\nBert Base Uncased | concat | 0.199 | 0.362 \nBert Base Multilingual Uncased | individual_mlps_on_cat_and_numerical_feats_then_concat | 0.244 | 0.352\nBert Base Multilingual Uncased | attention_on_cat_and_numerical_feats | 0.254 | 0.375\nBert Base Multilingual Uncased | gating_on_cat_and_num_feats_then_sum | **0.275** | 0.375 \nBert Base Multilingual Uncased | weighted_feature_sum_on_transformer_cat_and_numerical_feats | 0.266 | **0.380**\n\n## Citation\nWe now have a [paper](https://www.aclweb.org/anthology/2021.maiworkshop-1.10/) you can cite for the Multimodal-Toolkit.\n```bibtex\n@inproceedings{gu-budhkar-2021-package,\n    title = \"A Package for Learning on Tabular and Text Data with Transformers\",\n    author = \"Gu, Ken  and\n      Budhkar, Akshay\",\n    booktitle = \"Proceedings of the Third Workshop on Multimodal Artificial Intelligence\",\n    month = jun,\n    year = \"2021\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2021.maiworkshop-1.10\",\n    doi = \"10.18653/v1/2021.maiworkshop-1.10\",\n    pages = \"69--73\",\n}\n```\n", "metadata": {"source": "github_readmes\\georgian-io_Multimodal-Toolkit_README.md", "filename": "georgian-io_Multimodal-Toolkit_README.md", "type": "readme_full"}}
{"id": "guillefix_transflower-lightning_README.md", "paper_id": "guillefix_transflower-lightning_README", "text": "This repo holds the code to perform experiments with the multimodal autoregressive probabilistic model [Transflower](http://metagen.ai/transflower).\n\nPaper: [Transflower: probabilistic autoregressive dance generation with multimodal attention](https://arxiv.org/abs/2106.13871)\n\nVideo summary [here](https://www.youtube.com/watch?v=uBnCePehA-Y&ab_channel=GuillermoValle)\n\n_Abstract_\n\nDance requires skillful composition of complex movements that follow rhythmic, tonal and timbral features of music. Formally, generating dance conditioned on a piece of music can be expressed as a problem of modelling a high-dimensional continuous motion signal, conditioned on an audio signal. In this work we make two contributions to tackle this problem. First, we present a novel probabilistic autoregressive architecture that models the distribution over future poses with a normalizing flow conditioned on previous poses as well as music context, using a multimodal transformer encoder. Second, we introduce the currently largest 3D dance-motion dataset, obtained with a variety of motion-capture technologies, and including both professional and casual dancers. Using this dataset, we compare our new model against two baselines, via objective metrics and a user study, and show that both the ability to model a probability distribution, as well as being able to attend over a large motion and music context are necessary to produce interesting, diverse, and realistic dance that matches the music.\n\n----------\n\nThis repo is archived and continued development is found in [this repo](https://github.com/MetaGenAI/multimodal-transflower).\n\n# Overview of the repo\n\nIt is structured into folders which hold the code or assets for different parts of the workflow. I am using a general framework which was inspired by the CycleGAN project, and then modified to work with [Pytorch Lightning](https://www.pytorchlightning.ai/).\n\n* `Training` holds the training code. The entry point file is `train.py`\n  * `Datasets` holds the pytorch datasets and also convenience utilities to create the datasets and dataloaders. A dataset is defined as a class inheriting from BaseDataset, with a class name which should be \"DatasetNameDataset\" where \"DatasetName\" (but lowercased) will be the name used to refer to the dataset in the options.\n  * `experiments` holds the results of training runs, in folders given by the experiment id. Its contents are .gitignored\n  * `options` holds utilities to set up general argument options, as well as collect the options specific to the model, or dataset, specified for the experiment\n  * `hparams` holds the configuration files with hyperparameters that I'm using to run different experiments\n* `models` defines the models as Lightning modules (actually they inherit from BaseModel, for some convenient common functions). A model is defined as a class inheriting from BaseModel, with a class name which should be \"ModelNameModel\" where \"ModelName\" (but lowercased) will be the name used to refer to the model in the options.\n* `inference` holds the code and results for testing the model at inference time. In our case, this means generation, as we are working with generative models. Note that this is just a skeleton, and it relies on a `generate` function defined on each model.\n* `feature_extraction` holds code to extract features. At the moment functions to process features for audio and motion data are available\n* `data` holds the actual data, and a couple of scripts to download data\n* `analysis` holds code to analyze and explore the data, as well as visualizing results. Beware of files named \"sandbox\", as they are used by me using [Hydrogen](https://nteract.io/atom), so the outside observer would look like a jumbled mess of code (which it is). It is literally a sandbox of code, to play around.\n\n# How to run training and inference\n\nThe scripts in the root folder are for running the common high level tasks. `script_generate.sh` generates samples for the model of a particular experiment. `script_train.sh` trains a model on some data, as specified by a particular hyperparameters file, though any of the hparams can be overriden as argparse arguments. You can add `--help` to to get some explanation of what the different options/hparams do. Note that the options parser (in `training/options/base_options.py`) gathers the set of available options by aggregating the base options with the options defined in the particular model and dataset you have specified in the `model` and `dataset_name` options.\n\nYou can run `./script_train.sh transflower_expmap` to train using the hparams file in `training/hparams/dance_combined/transflower_expmap.yaml`, which is the configuration we used in the paper. This assumes the data is inside `data/dance_combined3` but can be changed with the `--data_dir` argument.\n\nYou can run `./script_generate.sh transflower_expmap the_basement --generate_bvh --data_dir data/dance_combined3 --generate_video` to generate using the model trained in the experiment `transflower_expmap`, on the sequence with sequence id `the_basement`. This sequence_id is just the basename of the original sound file.\n\n## Training on your own data\n\nFirst install requirements with `pip install -r requirements.txt`\n\nThen, I assume you have a folder, say inside `data/myData` which contains bvh files and wav files which are paired and time-syncced. In particular, they should have the same basename, for example, `sequence1.bvh` and `sequence1.wav` would be paired. With that, you can run `./feature_extraction/audio_feature_extraction.sh data/myData` and `./feature_extraction/motion_feature_extraction.sh data/myData` to extract the audio and motion features. You can pass the argument `--replace_existing` if you want to recompute features with the same names. This will create files with names like `sequence1.audio_feats_scaled_20.npy` and `sequence1.expmap_cr_scaled_20.npy`. The `expmap_cr_scaled_20` and `expmap_cr_scaled_20` parts are the \"feature names\" which should match the ones that are given in the `input_modalities` arguments when training (see the different hparams files). There are different audio and motion feature options available which can be seen in the different scripts called by `audio_feature_extraction.sh` and `motion_feature_extraction.sh`.\n\nOne this is done, you can now call `./script_train.sh transflower_expmap --data_dir=data/myData` to train, as mentioned above. The checkpoints and results will be found in `training/experiments/transflower_expmap`. You can use `tensorboard --logdir training/experiments` to track progress using tensorboard.\n\n*More options*: You can specify a different experiment name which will look for a different hparams file inside `training/hparams/dance_combined`. You can also directly pass arguments to `./script_train.sh`. You can see some of them by typing `python training/train.py -h`, but this is not a complete list, as some arguments are specific to datasets and models. You can see the model and dataset specific arguments by looking at their class definitions. The dataset definition is in `training/datasets/multimodal_dataset.py` and the model is in `models/transflower_model.py`. \n\n## Generating a dance\n\nTo give a more concrete example, and following the exact workflow in the provided [Google colab]() which I recommend using to try the model, assume we have a filename called `myCoolSong.wav`. After installing dependencies, we can create a folder to store it, and also one for the seeds and pretrained models, and download them. This assumes you have [gsutil installed](https://cloud.google.com/storage/docs/gsutil_install).\n\n```\npip install -r requirements.txt\napt-get install ffmpeg\nmkdir songs\nmkdir training/experiments\ngsutil -m cp -r gs://metagen/models/transflower_expmap_old training/experiments/\ngsutil -m cp -r gs://metagen/models/transflower_expmap_finetune2_old training/experiments/\ngsutil -m cp -r gs://metagen/scalers/* songs/\ngsutil -m cp -r gs://metagen/seeds/* songs/\n```\n\nWe then extract the music features as follows\n\n```\nchmod +x ./feature_extraction/audio_feature_extraction_test.sh\nchmod +x ./feature_extraction/script_to_list_filenames\n./feature_extraction/audio_feature_extraction_test.sh songs/\n```\n\nFinally, we can generate as follows\n\n```\n./script_generate.sh transflower_expmap_old myCoolSong --generate_bvh --generate_video --data_dir=songs\n```\n\nand the result will be inside `inference/generated/transflower_expmap_old/videos`.\n\n-----------------\n\nNext we explain in some more detail the structure and options of the general dataset format we use, as well as models available and their main options\n\n# Multimodal dataset\n\nThe multimodal dataset in `training/datasets` is designed to be a general purpose dataset which works for all the experiments within the scope of this repository.\nIt expects numpy arrays corresponding to time-synced and same-length sequneces of a number of \"input modalities\" and a number of \"output modalities\", to be found in the folder given in `--data_dir` argument. These should be numpy arrays of rank 2, where the first dimension is the sequence dimension, and the second one is the feature dimension.\nThe files in the folder should be formated as `[sequence_id].[feature_name].npy`. The dataset will read which sequence ids to use for training from a file named `base_filenames_train.txt` on the same folder as the data, and then it will pick up the desired input and output modalities, according to the modalities specified (comma-separated) in the arguments `--input_modalities` and `output_modalities`.\nIt will constract individual data points from these sequences, by specifying the length of window that you wish for each input modality (comma-separated) in `--input_lenghts`, and for each output modality (comma_separated in `--output_lengths`, you can provide offsets for each modality in `--output_time_offsets` and `--input_time_offsets`, both zero-indexed\n\nIn the image below you see an illustration of how a data point would look like with two input modalities (e.g. music, and movement), and one output modality (e.g. movement), with the specified lengths and offsets. This way of specifying data points is quite general and can conver a variety of tasks, including the multimodal autoregressive generation tasks we are studying here.\n\n![multimodal dataset example](docs/multimodal_dataset_example.png)\n\nNote that although not a technical requirement for the above dataset to work, for more tasks, we expect the sequences to be sampled at the same frequency, and have the same length (the later requirement could be relaxed, as we are gonna take subwindows of different sizes, but it at the moment it's what it expects)\n\n# Multimodal models\n\nMost of the models studied here are designed to work with the above multimodal dataset, and so they will build a model to fit the corresponding data point structure (number, and lengths, of the input and output modalities). The modality dimensions (number of features) are specified in the `--dins` and `--douts` arguments. \n\n## Transformer model\n\nThe basic (multimodal) transformer model is a generalization of the model proposed in [this paper](https://arxiv.org/abs/2101.08779), to work with any number of input or output modalities.\n\nHere's an example of how it would look with the input and output modalities described above: \n\n![multimodal transformer example](docs/multimodal_transformer_example.png)\n\nNote:\n\n* The \"output\" of the model is taken to be the first N outputs of the cross modal transformer, starting from the first, where N is the desired `output_length`. Note that this limits the output length to be at most the sum of input lengths. One could extend the model to relax this\n* If there were multiple output modalities, the input modality transformers would be shared, but a new \"cross-modal\" head would be added, per output modality.\n\n## Transflower model(s)\n\nThe idea of using autoregressive normalizing flows (NF), in particular for probabilistic modelling of motion, was inspired by [MoGlow](https://arxiv.org/abs/1905.06598). The idea of the Transflower model is to combine that idea with the multimodal transformer defined above. This can be done simply by feeding the output of the multimodal transformer (MT) as conditioning for the normalizing flow, which is done through the coupling layers of the normalizing flow, as illustrated below.\n\n![transflower example](docs/transflower_example.png)\n\n\nWithin the normalizing flow there are a series of different architectural choices possible, some of which correspond to the model variations in the `model` folder. \n* The `Transflower` model itself is using the [Flow++](https://arxiv.org/abs/1902.00275) architecture, but with hyperparameter choices that simplify the model as we are not working with images. Furthermore, the coupling neural network has been replaced with a Transformer network.\n* The `Transformerflow` model is an earlier iteration which is the same, except that the coupling neural network has been left to be the same as what it was for Flow++.\n* The `Transglower` model is an attempt at combining the MT with the original MoGlow model (which feeds an LSTM into the coupling layers of the Glow model, which is very similar to the Flow++ one we are using, given our simplified hyperparameters).\n* There's also an implementation of the original `MoGlow` model itselft to be used as a baseline\n* The `ResidualFlower` model is a new experiment I'm trying now where the mean of output is predicted by a (deterministic) MT, and the NF models only the deviation (residal) from the mean.\n\nYou can see some more details in [this post](https://openlab-flowers.inria.fr/t/transflower-high-dim-continuous-probabilistic-models-with-attention-and-their-applications/909)\n\n### Normalizing flows\n\nA very quick summary of normalizing flows. Normalizing flows are implicit generative models, which also allow computation of the exact probability (so they are explicit at the same time? I find the nomenclature confusing). This is done because the function mapping the stochastic latent to the output is reversible. We usually define the following\n* The forward flow, is a function mapping a vector/tensor in the \"output domain\" to a vector/tensor of the same size, in the \"latent domain\". This flow has to be reversible, which in particular implies that the Jacobian is non-singular\n* The reverse flow, is the inverse of the above function. This is the flow that allows to sample from the model, by sampling a latent vector from the latent distribution, and sending it through the flow.\n\nThe model is trained by exact likelihood maximization. We can compute the likelihood of the data, by using the transformation of variables formula, to compute P(X) as the determinant of the Jacobian of the forward flow, times Prior(f(X)), where Prior is the latent distribution (usually a standard Gaussian), and f(X) is the Z to which X is mapped by the forward flow. By the chain rule, the Jacobian determinant of the flow, can be decomposed into a product of the Jacobian determinant of each of the atomic operations in the flow. There are tricks to make these Jacobians more efficient to compute. `sldj` on the code I think stands for \"sum of log determinant of Jacobian\", as the product becomes sum after taking the log.\n\nMost of the ingenuity in desining normalizing flows has gone into desining expressive reversible functions. The [Real NVP paper](https://arxiv.org/abs/1605.08803) introduced reversible \"coupling layers\" which are a very flexible (and clever I think) class of reversible transformations, which also have simple Jacobians. I recommend the Real NVP paper as a good introduction to NFs. The [Glow paper](https://arxiv.org/abs/1807.03039) introduced reversible 1x1 convolutoins, which are also often used.\n\nMoGlow made use of the coupling layers to make the NF conditional on the recent history, thus making the model autoregressive.\n\nAs an interesting recent development in NFs, there's [FFJORD](https://arxiv.org/abs/1810.01367), which uses the interesting fact that as ODEs are reversible, one can use Neural ODEs to make NFs!\n\nThere's a [recent review](https://arxiv.org/abs/2103.04922) of different types of probabilistic deep generative models. However, it only focuses on images, so it is unclear how much their conclusions transfer to other domains. I think the question of which generative models work well for different tasks is a very interesting and valuable one, as these models are so fundamental and useful for real-world applications, where stochasticity and uncertainty means that we need to model probability distributions!\n\nAs a very quick summary, from that paper + my experience, the main advantages of NFs are:\n* They model the distribution very faithfully, avoiding problems of model collapse, for example. This typically means they are better at producing diverse solutions than models that suffer from mode collapse.\n* They do exact inference, meaning that if your model is flexible enough, and your optimizer is reasonable, you shouldn't have stability problems like often found in GANs.\n\nThe main disadvantages are:\n* They are slower at training and inference. I am not sure how much this is the case quantiatively, and I think also it may depend on different factors, like how good your optimizer is. But generally, a well-tempered GAN can train faster, and also will be faster at inference time. I think the slower inference of NFs is mostly because they are less parameter efficient (you need larger models to reach similar performance). Continuous NFs (like FFJORD) seem to be a lot more parameter efficient, but they are slow for other reasons, namely that Neural ODEs are slow.\n* Because they are so good at modelling the distribution of the data, they can also model errors/flukes in the data, which show up (with the frequency at which they showed in the data), while some less powerful models may actually be unaffected by these outliers.\n\nOverall, at least in image world (less clear about other modalities), they produce better quality results than VAEs, but worse than GANs. However, they may be more stable and thus easier to train than GANs. I think GANs are worth giving a try, though. Other models that I think are worth giving a try are: VQ-VAEs and energy-based models, which have different strengths and weaknesses, but have both shown very good performance at modeling complex distributions (again mostly in images). VQ-VAEs are also state of the art at music modelling, and close to SOTA at image modelling, so they are worth looking at I think. I am giving NFs a try mostly because they are SOTA for motion modelling currently, and they have nice theoretical properties (thanks to exact inference)\n\n# Data\n\n<!-- You can get a copy of the (already preprocessed) AIST++ data by running the file `./copy_from_gs.sh` in `data/`. It needs `gsutils` installed. With that you can begin running experiments using the provided scripts (you may need to add the flag `--fix_lengths` first time you run it as some modalities for the same sequence dont have quite the same length (coz my preprocessing isnt perfect:P), and may differ by one or two samples in length.\n\nThis data is extracted from a dataset of dance videos. More data comming soon.\n\nYou can also ask me for a copy of the MoGlow dataset if you want. I'm currently playing with the model, but soon will begin playing again with the data, and organize it more.  -->\n\nWe are compiling the dataset, which comprises several sources, and will release it soon. \n\n\n\n<!-- Some parts of the dataset are already publicly available. The AIST++ dataset is available [here](https://google.github.io/aistplusplus_dataset/factsfigures.html), and most of the VR dances are available from several playing lists including .... These dances can all be visualized in 3D in the VRChat world Papa Dance by lox9973. The VR dance videos can be converted to bvh (and retargetted to any humanoid skeleton) using [this Unity tool](https://github.com/guillefix/shadermotion-bvh-utils), and the features processed as explained [in the code](https://github.com/guillefix/transflower-lightning). However, we will release the full set of features soon, so that it will be easier to replicate and build on this dataset:) -->\n\n# Cite\n\nIf you use models or code from this repo, cite us using\n\n```\n@article{vallepérez2021transflower,\n      title={Transflower: probabilistic autoregressive dance generation with multimodal attention}, \n      author={Guillermo Valle-Pérez and Gustav Eje Henter and Jonas Beskow and André Holzapfel and Pierre-Yves Oudeyer and Simon Alexanderson},\n      year={2021},\n      eprint={2106.13871},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}\n```\n\n# Community\n\nI'll be continuing some extensions to this work, with the aim to develop open source tools and datasets. You can join [this community discord](https://discord.gg/HQ8Crcw), where we discuss progress in this project, which is part of an umbrella project to explore and develop the intersection between VR and AI:). See http://metagen.ai\n", "metadata": {"source": "github_readmes\\guillefix_transflower-lightning_README.md", "filename": "guillefix_transflower-lightning_README.md", "type": "readme_full"}}
{"id": "haoliuhl_instructrl_README.md", "paper_id": "haoliuhl_instructrl_README", "text": "# InstructRL\n\nThis is a Jax implementation for the *Instruct*RL method.\n\nPaper: [Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models](https://arxiv.org/abs/2210.13431).\n\n![model archiecture](./pictures/model.jpg)\n\nThis implementation has been tested on GPU and Google Cloud TPU and supports multi-host training with TPU Pods.\n\nThe code supports the following methods and baselines\n- From scratch: Training transformer policy from scratch w/ and w/o instructions.\n- CLIP-RL: Training transformer policy with pretrained OpenAI CLIP-VIT w/ and w/o instructions.\n- *Instruct*RL: Training transformer policy with pretrained multimodal MAE encoding w/ and w/o instructions.\n\nThe code also supports training and evaluating with both continuous and discretized robot action.\n\n## Installation\nIf this is on GPU, install CoppeliaSim with [coppeliasim script](./scripts/coppeliasim.sh), then install the dependencies with pip.\n```\ncat gpu_requirements.txt | xargs -n 1 -L 1 pip install\n```\n\nIf this is on TPU, install the dependencies using the [TPU setup script](./scripts/tpu_vm_setup.sh).\nAfter installing dependencies, add this repo directory to your `PYTHONPATH` environment variable\n```\nexport PYTHONPATH=\"$PYTHONPATH:$(pwd)\"\n```\n\n## Usage\nExperiments can be launched via the following commands.\n\nTraining a policy transformer using pretrained multimodal MAE encoding\n```\nexport PYTHONPATH=\"$PYTHONPATH:$PROJECT_DIR\"\nexport PYTHONPATH=\"$PYTHONPATH:$PROJECT_DIR/instructrl/models\"\necho $PYTHONPATH\nexport WANDB_API_KEY=''\n\nexport bucket_name='instruct-rl'\n\nexport experiment_name='instructrl'\n\nONLINE=True\nDATASET=\"reach_target\"\nMODEL_TYPE=\"vit_base\"\nTRANSFER_TYPE=\"m3ae_vit_b16\"\nBATCH_SIZE=2048\nINSTRUCTION=\"moving to one of the spheres\"\nNOTE=\"pt: $TRANSFER_TYPE inst: $INSTRUCTION batch size: $BATCH_SIZE policy: $MODEL_TYPE dataset: $DATASET\"\n\npython3 -m instructrl.instructrl_main \\\n    --is_tpu=True \\\n    --dataset_name=\"$DATASET\" \\\n    --model.model_type=\"$MODEL_TYPE\" \\\n    --model.transfer_type=\"$TRANSFER_TYPE\" \\\n    --window_size=4 \\\n    --val_every_epochs=1 \\\n    --test_every_epochs=1 \\\n    --instruct=\"$INSTRUCTION\" \\\n    --batch_size=\"$BATCH_SIZE\" \\\n    --weight_decay=0.0 \\\n    --lr=3e-4 \\\n    --auto_scale_lr=False \\\n    --lr_schedule=cos \\\n    --warmup_epochs=5 \\\n    --momentum=0.9 \\\n    --clip_gradient=10.0 \\\n    --epochs=200 \\\n    --dataloader_n_workers=16 \\\n    --dataloader_shuffle=False \\\n    --log_all_worker=False \\\n    --logging.online=\"$ONLINE\" \\\n    --logging.prefix='' \\\n    --logging.project=\"$experiment_name\" \\\n    --logging.gcs_output_dir=\"gs://$bucket_name/instructrl/experiment_output/$experiment_name\" \\\n    --logging.output_dir=\"$HOME/experiment_output/$experiment_name\" \\\n    --logging.random_delay=0.0 \\\n    --logging.notes=\"$NOTE\"\n```\n\nThe *model.transfer_type* argument determines pretrained transformers, with the following options supported\n- VIT training from scratch \"None\"\n- M3AE pretrained model \"m3ae_vit_b16\" (with larger models coming soon)\n- CLIP pretrained model \"clip_vit_b32\" and \"clip_vit_b16\"\n\nThe *model.model_type* argument determines the type of trained from scratch policy transformer, it can be one of vit_small, vit_base, and vit_large.\n\nFor large-scale training (e.g. training on near 100 tasks), it is recommended to use large TPU pod.\nWe provide the job script for launching large-scale training in [jobs](./jobs/tpu_control.sh).\n\nEvaluating trained model\n```\nexport PYTHONPATH=\"$PYTHONPATH:$PROJECT_DIR\"\nexport PYTHONPATH=\"$PYTHONPATH:$PROJECT_DIR/instructrl/models\"\necho $PYTHONPATH\nexport WANDB_API_KEY=''\n\nexport bucket_name='instruct-rl'\n\nexport experiment_name='instructrl'\n\nONLINE=True\nDATASET=\"reach_target\"\nMODEL_TYPE=\"vit_base\"\nTRANSFER_TYPE=\"m3ae_vit_b16\"\nINSTRUCTION=\"moving to one of the spheres\"\nCKPT=\"\"\nNOTE=\"Local rollout. pt: $TRANSFER_TYPE inst: $INSTRUCTION policy: $MODEL_TYPE dataset: $DATASET\"\n\npython3 -m instructrl.local_run \\\n    --load_checkpoint \"$CKPT\" \\\n    --dataset_name=\"$DATASET\" \\\n    --model.model_type=\"$MODEL_TYPE\" \\\n    --model.transfer_type=\"$TRANSFER_TYPE\" \\\n    --window_size=1 \\\n    --instruct=\"$INSTRUCTION\" \\\n    --log_all_worker=False \\\n    --data.path=\"$PROJECT_DIR/data/variation\" \\\n    --logging.online=\"$ONLINE\" \\\n    --logging.prefix='' \\\n    --logging.project=\"$experiment_name\" \\\n    --logging.gcs_output_dir=\"gs://$bucket_name/instructrl/experiment_output/$experiment_name\" \\\n    --logging.output_dir=\"$HOME/experiment_output/$experiment_name\" \\\n    --logging.random_delay=0.0 \\\n    --logging.notes=\"$NOTE\"\n```\n\n## Data generation\nOur preprocessed single-task and multi-task data will be released soon.\nIn order to facilitate large scale training on cloud, we store all the dataset\nas HDF5 files and read them from cloud storage buckets.\nThe HDF5 data contains images, states and actions.\nAn example of data generation can be found in [collect_data script](./data/collect_data.py).\n\n## Acknowledgement\nThe Multimodal MAE implementation is largely based on [m3ae](https://github.com/young-geng/m3ae_public) and the CLIP implementation is largely based on [scenic](https://github.com/google-research/scenic/tree/main/scenic/projects/baselines/clip).\n", "metadata": {"source": "github_readmes\\haoliuhl_instructrl_README.md", "filename": "haoliuhl_instructrl_README.md", "type": "readme_full"}}
{"id": "Haoyu-ha_ALMT_README.md", "paper_id": "Haoyu-ha_ALMT_README", "text": "# Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis\n\nPytorch implementation of paper: \n\n> [**Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis**](https://aclanthology.org/2023.emnlp-main.49.pdf)\n\n> This is a reorganized code, if you find any bugs please contact me. Thanks.\n\n\n## Content\n- [Note](#Note)\n- [Data Preparation](#Data-preparation)\n- [Environment](#Environment)\n- [Training](#Training)\n- [Citation](#Citation)\n\n\n## Note\n\n1. [2025.03.06] The demo code has been updated to fix some issues. We recommend reproducing with new code and environmental requirements.\n\n2. Based on the experience and insights gained from the ALMT, we have futher explored robust MSA by ensuring the integrity of the dominant modality under different noise intensities. This new work has been accepted at NeurIPS 2024, welcome to [this new work](https://github.com/Haoyu-ha/LNLN).\n\n3. The ALMT implementation has been added to [MMSA](https://github.com/thuiar/MMSA); you can also refer to the implementation and make a fairer comparison with other methods in the same framework.\n\n4. We observed that regression metrics (such as MAE and Corr) and classification metrics (such as acc2 and F1) focus on different aspects of model performance. A model that achieves the lowest error in sentiment intensity prediction does not necessarily perform best in classification tasks. To comprehensively demonstrate the capabilities of the model, we selected the best-performing model for each type of metric, meaning that acc2/F1 and MAE correspond to different epochs of the same training process. In addition, the code also compute and report the performance in the same epoch for reference.\n\n\n## Data Preparation\nMOSI/MOSEI/CH-SIMS Download: See [MMSA](https://github.com/thuiar/MMSA).\n\n## Environment\nThe basic training environment for the results in the paper is Pytorch 2.5.1 with CUDA 12.1, Python 3.11.10 with RTX A40. It should be noted that different hardware and software environments can cause the results to fluctuate.\n\n## Training\nYou can quickly run the code with the following command:\n\n### CH-SIMS\n```\npython train.py --config_file configs/sims.yaml --gpu_id 0\n```\n\n### MOSI\n```\npython train.py --config_file configs/mosi.yaml --gpu_id 0\n```\n\n### MOSEI\n```\npython train.py --config_file configs/mosei.yaml --gpu_id 0\n```\n\n## Citation\n\n- [Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis](https://aclanthology.org/2023.emnlp-main.49/)\n\nPlease cite our paper if you find our work useful for your research:\n\n```\n@inproceedings{zhang-etal-2023-learning-language,\n    title = \"Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis\",\n    author = \"Zhang, Haoyu  and\n              Wang, Yu  and\n              Yin, Guanghao  and\n              Liu, Kejun  and\n              Liu, Yuanyuan  and\n              Yu, Tianshu\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    year = \"2023\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"756--767\"\n}\n```\n", "metadata": {"source": "github_readmes\\Haoyu-ha_ALMT_README.md", "filename": "Haoyu-ha_ALMT_README.md", "type": "readme_full"}}
{"id": "huggingface_transformers_README.md", "paper_id": "huggingface_transformers_README", "text": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">简体中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">繁體中文</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">한국어</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Español</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">日本語</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">हिन्दी</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">Русский</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Português</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">తెలుగు</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Français</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_it.md\">Italiano</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Tiếng Việt</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">العربية</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">اردو</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_bn.md\">বাংলা</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n</h3>\n\nTransformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer\nvision, audio, video, and multimodal model, for both inference and training.\n\nIt centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the\npivot across frameworks: if a model definition is supported, it will be compatible with the majority of training\nframeworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),\nand adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.\n\nWe pledge to help support new state-of-the-art models and democratize their usage by having their model definition be\nsimple, customizable, and efficient.\n\nThere are over 1M+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+, and [PyTorch](https://pytorch.org/get-started/locally/) 2.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\n```py\n# venv\npython -m venv .my-env\nsource .my-env/bin/activate\n# uv\nuv venv .my-env\nsource .my-env/bin/activate\n```\n\nInstall Transformers in your virtual environment.\n\n```py\n# pip\npip install \"transformers[torch]\"\n\n# uv\nuv pip install \"transformers[torch]\"\n```\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\n```shell\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\n\n# pip\npip install '.[torch]'\n\n# uv\nuv pip install '.[torch]'\n```\n\n## Quickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\npipeline(\"the secret to baking a really good cake is \")\n[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]\n```\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line.\n> ```shell\n> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n> ```\n\n```py\nimport torch\nfrom transformers import pipeline\n\nchat = [\n    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n]\n\npipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0][\"generated_text\"][-1][\"content\"])\n```\n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\n<details>\n<summary>Automatic speech recognition</summary>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\npipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```\n\n</details>\n\n<details>\n<summary>Image classification</summary>\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"image-classification\", model=\"facebook/dinov2-small-imagenet1k-1-layer\")\npipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n[{'label': 'macaw', 'score': 0.997848391532898},\n {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n  'score': 0.0016551691805943847},\n {'label': 'lorikeet', 'score': 0.00018523589824326336},\n {'label': 'African grey, African gray, Psittacus erithacus',\n  'score': 7.85409429227002e-05},\n {'label': 'quail', 'score': 5.502637941390276e-05}]\n```\n\n</details>\n\n<details>\n<summary>Visual question answering</summary>\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\npipeline(\n    image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n    question=\"What is in the image?\",\n)\n[{'answer': 'statue of liberty'}]\n```\n\n</details>\n\n## Why should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, audio, video, and multimodal tasks.\n    - Low barrier to entry for researchers, engineers, and developers.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Share trained models instead of training from scratch.\n    - Reduce compute time and production costs.\n    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.\n\n1. Choose the right framework for every part of a models lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.\n    - Pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n</a><br>\n\n## Why shouldn't I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Example models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\n<details>\n<summary>Audio</summary>\n\n- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n- Text to speech with [Bark](https://huggingface.co/suno/bark)\n\n</details>\n\n<details>\n<summary>Computer vision</summary>\n\n- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n- Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)\n- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n\n</details>\n\n<details>\n<summary>Multimodal</summary>\n\n- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n\n</details>\n\n<details>\n<summary>NLP</summary>\n\n- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n- Translation with [T5](https://huggingface.co/google-t5/t5-base)\n- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n\n</details>\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the 🤗 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n", "metadata": {"source": "github_readmes\\huggingface_transformers_README.md", "filename": "huggingface_transformers_README.md", "type": "readme_full"}}
{"id": "HuiZhang0812_CreatiLayout_README.md", "paper_id": "HuiZhang0812_CreatiLayout_README", "text": "# CreatiLayout\n\n\n<img src='assets/figures/teaser.jpg' width='100%' />\n\n<br>\n<a href=\"https://arxiv.org/pdf/2412.03859\"><img src=\"https://img.shields.io/static/v1?label=Paper&message=2412.03859&color=red&logo=arxiv\"></a>\n<a href=\"https://creatilayout.github.io/\"><img src=\"https://img.shields.io/static/v1?label=Project%20Page&message=Github&color=blue&logo=github-pages\"></a>\n<a href=\"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Dataset-ffbd45.svg\" alt=\"HuggingFace\"></a>\n<a href=\"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Benchmark-ffbd45.svg\" alt=\"HuggingFace\"></a>\n<a href=\"https://huggingface.co/HuiZhang0812/CreatiLayout\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Model-ffbd45.svg\" alt=\"HuggingFace\"></a>\n<a href=\"https://huggingface.co/spaces/HuiZhang0812/CreatiLayout\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Space-ffbd45.svg\" alt=\"HuggingFace\"></a>\n\n\n> **CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation**\n> <br>\n> [Hui Zhang](https://huizhang0812.github.io/), \n> [Dexiang Hong](https://scholar.google.com.hk/citations?user=DUNijlcAAAAJ&hl=zh-CN),\n> [Yitong Wang](https://scholar.google.com/citations?user=NfFTKfYAAAAJ&hl=zh-CN),\n> [Jie Shao](https://openreview.net/profile?id=~Jie_Shao5),\n> [Xinglong Wu](https://scholar.google.com/citations?user=LVsp9RQAAAAJ&hl=zh-CN),\n> [Zuxuan Wu](https://zxwu.azurewebsites.net/),\n> and \n> [Yu-Gang Jiang](https://scholar.google.com/citations?user=f3_FP8AAAAAJ)\n> <br>\n> Fudan University & ByteDance Inc.\n> <br>\n\n## Introduction\nCreatiLayout is a layout-to-image framework for Diffusion Transformer models, offering high-quality and fine-grained controllable generation.\n\n**LayoutSAM Dataset** 📚: A large-scale layout dataset with 2.7 million image-text pairs and 10.7 million entities, featuring fine-grained annotations for open-set entities.\n\n**SiamLayout** 🌟: A novel layout integration network for MM-DiT treats the layout as an independent modality with its own set of transformer parameters, allowing the layout to play an equally important role as the global description in guiding the image.\n\n**Layout Designer** 🎨: A layout planner leveraging the power of large language models to convert various user inputs (e.g., center points, masks, scribbles) into standardized layouts. \n\n## 🔥 News\n- **2025-6-26**: CreatiLayout was accepted by **ICCV 2025** 🎉🎉.\n- **2025-3-10**: We release **CreatiLayout-FLUX**, which empowers FLUX.1-dev for layout-to-image generation and achieves more precise rendering of spatial relationships and attributes.\n- **2025-1-30**: We propose **CreatiLayout-LoRA**, which achieves layout control with fewer additional parameters.\n\n\n\n## Quick Start\n### Setup\n1. **Environment setup**\n```bash\nconda create -n creatilayout python=3.10 -y\nconda activate creatilayout\nconda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.1 -c pytorch -c nvidia\n```\n2. **Requirements installation**\n```bash\npip install -r requirements.txt\n```\n### Usage example\nYou can run the following code to generate an image:\n```python\npython test_sample.py\n```\nOr you can try gradio at <a href=\"https://huggingface.co/spaces/HuiZhang0812/CreatiLayout\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Space-ffbd45.svg\" alt=\"HuggingFace\"></a>.\n<div align=\"center\">\n  <img src=\"assets/figures/gradio_teaser.jpg\" width=\"100%\">\n</div>\n\n\n## Dataset\n### LayoutSAM <a href=\"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Dataset-ffbd45.svg\" alt=\"HuggingFace\"></a>\nThe LayoutSAM dataset is a large-scale layout dataset derived from the SAM dataset, containing 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a spatial position (i.e., bounding box) and a textual description. Traditional layout datasets often exhibit a closed-set and coarse-grained nature, which may limit the model's ability to generate complex attributes such as color, shape, and texture.\n<div align=\"center\">\n  <img src=\"assets/figures/data_samples.jpg\" width=\"100%\">\n</div>\n\n### LayoutSAM-eval Benchmark <a href=\"https://huggingface.co/datasets/HuiZhang0812/LayoutSAM-eval\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Benchmark-ffbd45.svg\" alt=\"HuggingFace\"></a>\nLayoutSAM-Eval is a comprehensive benchmark for evaluating the quality of Layout-to-Image (L2I) generation models. This benchmark assesses L2I generation quality from two perspectives: region-wise quality (spatial and attribute accuracy) and global-wise quality (visual quality and prompt following). It employs the VLM’s visual question answering to evaluate spatial and attribute adherence, and utilizes various metrics including IR score, Pick score, CLIP score, FID, and IS to evaluate global image quality.\n\nTo evaluate the model's layout-to-image generation capabilities through LayoutSAM-Eval, first you need to generate images for each data in the benchmark by running the following code:\n```python\npython test_SiamLayout_sd3_layoutsam_benchmark.py\n```\nThen, visual language models (VLM) are used to answer visual questions. This will assess each image's adherence to spatial and attribute specifications. You can do this by using the following code:\n```python\npython score_layoutsam_benchmark.py\n```\n\n\n## Models\n**Layout-to-Image generation:**\n| Model   | Base model    |  Description  |\n| ------------------------------------------------------------------------------------------------ | -------------- | -------------------------------------------------------------------------------------------------------- |\n| <a href=\"https://huggingface.co/HuiZhang0812/CreatiLayout\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Model-ffbd45.svg\" alt=\"HuggingFace\"></a> | Stable Diffusion 3 | SiamLayout-SD3 used in the paper\n| <a href=\"https://huggingface.co/HuiZhang0812/CreatiLayout\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Model-ffbd45.svg\" alt=\"HuggingFace\"></a> | Stable Diffusion 3 | SiamLayout-SD3-LoRA used in the paper\n| <a href=\"https://huggingface.co/HuiZhang0812/CreatiLayout\"><img src=\"https://img.shields.io/badge/🤗_HuggingFace-Model-ffbd45.svg\" alt=\"HuggingFace\"></a> | FLUX.1-dev | SiamLayout-FLUX used in the paper\n\n## ✒️ Citation\n\nIf you find our work useful for your research and applications, please kindly cite using this BibTeX:\n\n```latex\n@article{zhang2024creatilayout,\n  title={CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation},\n  author={Zhang, Hui and Hong, Dexiang and Gao, Tingwei and Wang, Yitong and Shao, Jie and Wu, Xinglong and Wu, Zuxuan and Jiang, Yu-Gang},\n  journal={arXiv preprint arXiv:2412.03859},\n  year={2024}\n}\n```\n", "metadata": {"source": "github_readmes\\HuiZhang0812_CreatiLayout_README.md", "filename": "HuiZhang0812_CreatiLayout_README.md", "type": "readme_full"}}
{"id": "HumaticsLAB_GTM-Transformer_README.md", "paper_id": "HumaticsLAB_GTM-Transformer_README", "text": "# GTM-Transformer\nOfficial Pytorch Implementation of [**Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends**](https://arxiv.org/abs/2109.09824) paper\n\n[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/well-googled-is-half-done-multimodal/new-product-sales-forecasting-on-visuelle)](https://paperswithcode.com/sota/new-product-sales-forecasting-on-visuelle?p=well-googled-is-half-done-multimodal)\n\n## Installation\n\nWe suggest the use of VirtualEnv.\n\n```bash\n\npython3 -m venv gtm_venv\nsource gtm_venv/bin/activate\n# gtm_venv\\Scripts\\activate.bat # If you're running on Windows\n\npip install numpy pandas matplotlib opencv-python permetrics Pillow scikit-image scikit-learn scipy tqdm transformers fairseq wandb\n\npip install torch torchvision\n\n# For CUDA11.1 (NVIDIA 3K Serie GPUs)\n# Check official pytorch installation guidelines for your system\npip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n\npip install pytorch-lightning\n\nexport INSTALL_DIR=$PWD\n\ncd $INSTALL_DIR\ngit clone https://github.com/HumaticsLAB/GTM-Transformer.git\ncd GTM-Transformer\nmkdir ckpt\nmkdir dataset\nmkdir results\n\nunset INSTALL_DIR\n```\n\n## Dataset\n\n**VISUELLE** dataset is publicly available to download [here](https://forms.gle/cVGQAmxhHf7eRJ937). Please download and extract it inside the dataset folder.\n\n## Training\nTo train the model of GTM-Transformer please use the following scripts. Please check the arguments inside the script before launch.\n\n```bash\npython train.py --data_folder dataset\n```\n\n\n## Inference\nTo evaluate the model of GTM-Transformer please use the following script .Please check the arguments inside the script before launch.\n\n```bash\npython forecast.py --data_folder dataset --ckpt_path ckpt/model.pth\n```\n\n## Citation\n```\n@misc{skenderi2021googled,\n      title={Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends}, \n      author={Geri Skenderi and Christian Joppi and Matteo Denitto and Marco Cristani},\n      year={2021},\n      eprint={2109.09824},\n}\n```\n", "metadata": {"source": "github_readmes\\HumaticsLAB_GTM-Transformer_README.md", "filename": "HumaticsLAB_GTM-Transformer_README.md", "type": "readme_full"}}
{"id": "ictnlp_DSTC8-AVSD_README.md", "paper_id": "ictnlp_DSTC8-AVSD_README", "text": "# DSTC8-AVSD\nWe rank the 1st in DSTC8 Audio-Visual Scene-Aware Dialog competition. This is the source code for our AAAI2020-DSTC8-AVSD paper [Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog.](<https://arxiv.org/abs/2002.00163>) Zekang Li, Zongjia Li, Jinchao Zhang, Yang Feng, Cheng Niu, Jie Zhou. AAAI2020.\n\n## News\nOur paper is accpeted by IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP). [url](<https://ieeexplore.ieee.org/abstract/document/9376902>)\n\n## Abstract\n\nAudio-Visual Scene-Aware Dialog (AVSD) is a task to generate responses when chatting about a given video, which is organized as a track of the 8th Dialog System Technology Challenge (DSTC8). To solve the task, we propose a universal multimodal transformer and introduce the multi-task learning method to learn joint representations among different modalities as well as generate informative and fluent responses. Our method extends the natural language generation pre-trained model to multimodal dialogue generation\ntask. Our system achieves the best performance in both objective and subjective evaluations in the challenge.\n\n![A dialogue sampled from the DSTC8-AVSD dataset. For each dialogue, there are video, audio, video caption, dialogue summary and 10 turns of conversations about the video.](./images/Figure1.png)\n\n## Model Architecture\n\n![](./images/Figure2.png)\n\n\n\n## How to Run\n\n### Requirements\n\nPython. 3.6\n\ntorch==1.0.1\npytorch-ignite==0.2.1\ntransformers==2.1.1\ntqdm==4.36.1\n\n```shell\npip install -r requirements.txt\n```\n\n### Data\n\nDownload [dataset](https://drive.google.com/drive/folders/1SlZTySJAk_2tiMG5F8ivxCfOl_OWwd_Q) of the DSTC8, including the training, validation, and test dialogues and the features of Charades videos extracted using VGGish and I3D models.\n\nAll the data should be saved into folder `data/` in the repo root folder.\n\n### Train\n\n```shell\npython train.py --log_path log/\n```\n\n### Generate\n\n```shell\npython generate.py --model_checkpoint log/ --output result.json --beam_search\n```\n\n\n\n## Citation\n\nIf you use this code in your research, you can cite our AAAI2020 DSTC8 workshop paper:\n\n```\n@article{li2020bridging,\n    title={Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog},\n    author={Zekang Li and Zongjia Li and Jinchao Zhang and Yang Feng and Cheng Niu and Jie Zhou},\n    year={2020},\n    eprint={2002.00163},\n    archivePrefix={arXiv},\n    journal={CoRR},\n    primaryClass={cs.CL}\n}\n```\n\n\n\n", "metadata": {"source": "github_readmes\\ictnlp_DSTC8-AVSD_README.md", "filename": "ictnlp_DSTC8-AVSD_README.md", "type": "readme_full"}}
{"id": "iiscleap_multimodal_emotion_recognition_README.md", "paper_id": "iiscleap_multimodal_emotion_recognition_README", "text": "# Multimodal Transformer With Learnable Frontend and Self Attention for Emotion Recognition \n\nThis repo contains the code for detecting emotion from the conversational dataset IEMOCAP for the implementation of the paper \"Multimodal Transformer With Learnable Frontend and Self Attention for Emotion Recognition\" submitted to ICASSP 2022. This repository contains the code when Session 5 is conisdered as test and Session 1 as validation.\n\n## Description of the code\n- The implementation has three stages, namely, training the unimodal audio and text models, training the Bi-GRU with self-attention and the multimodal transformer\n- With the **wav** files for the audio and the **csv** files for text, the first step would be to run **audio_model.py** and the notebook **sentiment_text.ipynb** for audio and text respectively\n- The representations from the trained models in the step above are used to create pickle files for the entire dataset\n- With these representations, two Bi-GRU models with self-attention (refer to **bigru_audio/text.ipynb**) is trained. The best models for both audio and text are already provided in the **unimodal_models** folder. \n- A multimodal transformer is trained on both the modalities of the dataset for the final accuracy results\n- Please note that usage of **IEMOCAP** requires permission. Once this is done, we can share the dataset files. For permission please visit [IEMOCAP release](https://sail.usc.edu/iemocap/iemocap_release.htm)\n\n## Running the code\n- Clone the repository `https://github.com/iiscleap/multimodal_emotion_recognition.git`\n- For the LEAF-CNN framework for audio sentiment classification, we use [this Pytorch implementation](https://github.com/denfed/leaf-audio-pytorch) for LEAF.\n  - Run ```python3 -m venv .leaf_venv```\n  - Run ```source .leaf_venv/bin/activate```\n  - Run ```pip install -r requirements_leaf.txt```\n  - Clone the repository ```https://github.com/denfed/leaf-audio-pytorch.git``` \n  - The files should be arranged as follows:\n      ```\n    Sess5\n    └───leaf_wavs_train\n        └───Ses01F_impro01_F000.wav\n        └───Ses01F_impro01_F005.wav\n        └───...\n    └───leaf_wavs_test\n        └───Ses05F_impro01_F000.wav\n        └───Ses05F_impro01_F008.wav\n        └───...\n    label_dict.json\n    leaf-audio-pytorch-main\n    │\n    └───__init__.py\n    └───setup.py\n    └───...\n    └───audio_model.py\n    └───leaf_audio_pytorch\n        └───...\n    ```\n- Running **sentiment_text.ipynb** provides the text unimodal model\n- For running the two Bi-GRU models with self-attention, run **bigru_audio.ipynb** to get ```best_model_aud0.tar``` and **bigru_text.ipynb** to get ```best_model_text0.tar```. These are to be placed in the folder **unimodal_models**.\n- For running the multimodal transformer we create another environment\n  - Run ```python3 -m venv .trans_venv```\n  - Run ```source .trans_venv/bin/activate```\n  - Run ```pip install -r requirements.txt```\n  - With the config file provided, run ```python3 main.py```\n  - The files at this stage should be arranged as follows:\n  ```\n    main.py\n    config.py\n    features\n    └───<PICKLE_FILE>\n    src\n    └───model_lstm_tranformers.py\n    └───read_data.py\n    └───test_lstm_transformers.py\n    └───...\n    unimodal_models\n    └───best_model_aud0.tar\n    └───best_model_text0.tar\n    ```\n\n\n\n\n", "metadata": {"source": "github_readmes\\iiscleap_multimodal_emotion_recognition_README.md", "filename": "iiscleap_multimodal_emotion_recognition_README.md", "type": "readme_full"}}
{"id": "imedslab_CLIMATv2_README.md", "paper_id": "imedslab_CLIMATv2_README", "text": "# CLIMATv2: Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting from Multimodal Data\n\nThis is the implementation of the paper CLIMATv2: https://arxiv.org/abs/2210.13889. Its previous version (CLIMATv1) can be found at https://arxiv.org/abs/2104.03642.\n\nThe concept of the framework is as follows\n\n<img src=\"asset/concept.png\" alt=\"ADNI inputs\" width=\"400\" style=\"display: block; margin: 0 auto\"/>\n\n\nThe differences of CLIMATv2 compared to CLIMATv1 are:\n- General practitioner (GP) is allowed to utilize multimodalities to perform diagnosis prediction (i.e, y_0)\n- The diagnosis predictions of Radiologist and GP is enforced to be consistent\n- Cross-entropy loss is replaced by CLUB (Calibrated Loss based on Upper Bound), which takes into account both **performance** and **calibration** during optimization.  \n \n<img src=\"asset/architectures.png\" alt=\"ADNI inputs\" width=\"800\" style=\"display: block; margin: 0 auto\"/>\n\n---\n\n\n## Setup\nRun commands:\n```bash\ngit clone git@github.com:Oulu-IMEDS/CLIMATv2.git\ncd ./CLIMATv2\nconda create -n CLIMATv2 python=3.7\nconda activate CLIMATv2\npip install -e .\n```\n\n---\n\n## [ADNI] Alzheimer's Disease Status Prognosis Prediction\n\n### Input\n\n<img src=\"asset/adni_inputs.png\" alt=\"ADNI inputs\" width=\"350\" style=\"display: block; margin: 0 auto\"/>\n\n### Data preparation\n\nYou can use the ADNI metadata prepared in `./adni/Metadata/adni_fdgpet_prognosis.csv`, or regenerated them using\n```bash\n# Modify input and output paths, then run\npython ./common/adni/preprocess_adni.py\n# Standard voxels if needed\npython ./common/adni/standardize_voxels.py\n```\n\n### Training\n\nCommand line:\n```bash\n# General setting using default values in configuration files in ./adni/configs/config_train.yaml\npython train.py config=seq_multi_prog_climatv2\n\n# Detailed setting\npython train.py config=seq_multi_prog_climatv2 comment=mycomment \\\n    bs=${BATCH_SIZE} num_workers=${NUM_WORKERS} root.path=/path/to/ANDI meta_root=/path/to/meta_dir/ fold_index=1 \\\n    backbone_name=shufflenetv2 max_depth=4 num_cls_num=4 prognosis_coef=1 cons_coef=0.5 \\ \n    loss_name=CLUB club.s=0.5\n```\n\n`config` can be\n- `seq_multi_prog_climatv1`: CLIMATv1\n- `seq_multi_prog_climatv2`: CLIMATv2\n\nProcessing:\n- `bs`: batch size\n- `num_workers`: the number of workers\n\nData setup:\n- `root.path`: root directory of images\n- `meta_root`: root directory of metadata (.csv or saved split configuration in .pkl)\n- `fold_index`: fold index (starting from 1)\n\nModel:\n- `backbone_name`: backbone for imaging feature extraction\n- `max_depth`: the number of CNN blocks in imaging feature extraction module\n- `n_meta_features`: the length of metadata features\n- `num_cls_num`: the number of [CLS] embebddings in transformer P\n\nCoefficients in loss \n- `prognosis_coef`: coefficient for prognosis prediction\n- `cons_coef`: coefficient for consistency term\n\n`loss_name` is either \n- `CLUB`: Calibrated loss based on upper bound (ours). `club.s`:  epsilon hyperparameter in CLUB.\n- `CE`: cross-entropy loss\n- `FL`: focal loss\n- `FLA`: adaptive focal loss\n- `MTL`: multi-task loss\n\nHyperparameters used in the paper:\n\n<img src=\"asset/adni_hyperparam.png\" alt=\"ADNI inputs\" width=\"350\" style=\"display: block; margin: 0 auto\"/>\n\n### Evaluation\n\n```bash\npython eval.py root.path=/path/to/imgs_dir/ meta_root=/path/to/metadata_dir/ \\\n    eval.root=/path/to/trained_models_dir/ eval.patterns=${PATTERN} eval.output=/path/to/output.json \\\n    use_only_baseline=True seed=${SEED} \\\n    save_predictions=${SAVE_PREDICTIONS} save_attn=${SAVE_ATTENTION_MAPS}\n```\n\nInput data for evaluation:\n- `root.path`: root directory of images\n- `meta_root`: root directory of metadata (.csv or saved split configuration in .pkl)\n- `eval.root`: root directory containing sub-directories of trained settings \n- `eval.patterns`: a common pattern of saved model files (e.g., `pn_avg_ba` for average balanced accuracies, or `pn_avg_mauc` for average mAUCs)\n- `eval.output`: path to file storing evaluation results\n- `use_only_baseline`: whether to use data at the baseline as input (always `True`)\n- `save_predictions`: whether to save predictions for visualization\n- `save_attn`: whether to save attention maps for visualization\n\n---\n\n## [OAI] Knee Osteoarthritis Structural Prognosis Prediction\n\n### Input\n\n<img src=\"asset/oai_inputs.png\" alt=\"OAI inputs\" width=\"450\" style=\"display: block; margin: 0 auto\"/>\n\n### Data preparation\n\nRun commands:\n```bash\n# Generate longitudinal data\npython ./common/prepare_1img_seq_metadata.py\n\n# Split data\npython ./common/do_split.py\n```\n\n### Training\n\n```bash\n# General setting using default values in configuration files in ./oai/configs/config_train.yaml\npython train.py config=seq_multi_prog_climatv2\n\n# Detailed setting\npython train.py config=seq_multi_prog_climatv2 \\\n    bs=64 num_workers=8 root.path=/path/to/OAI/ meta_root=/path/to/meta_dir backbone_name=resnet18 site=C \\ \n    prognosis_coef=1.0 cons_coef=0.5 loss_name=CLUB n_meta_features=128 \\ \n    num_cls_num=8 club.s=0.5 grading=KL \\\n    fold_index=1 seed=12345 \n```\n\nBesides the arguments used for ADNI, we have the additional arguments for OAI: \n\nData:\n- `site`: test acquisition site (`C`, with the most data, is chosen for testing, meaning that sites `A`, `B`, `D`, `E` are used for training and validation.)\n\n`grading` can be:\n- `KL`: Kellgren and Lawrence\n- `JSL`: Lateral joint space\n- `JSM`: Medial joint space\n- `OSFL`: Lateral osteophyte in femur\n- `OSFM`: Medial osteophyte in femur\n- `OSTL`: Lateral osteophyte in tibia\n- `OSTM`: Medial osteophyte in tibia\n\nList of augmentations applied to knee images \n(**Note**: all right knee images are vertically flipped):\n\n<img src=\"asset/oai_aug.png\" alt=\"OAI inputs\" width=\"450\" style=\"display: block; margin: 0 auto\"/>\n\n### Evaluation\n\nSame as above.\n\n---\n\n## Reference\nIf you find the manuscript or codes useful, please cite as follows\n```\n@article{nguyen2022clinically,\n  title={Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting from Multimodal Data},\n  author={Nguyen, Huy Hoang and Blaschko, Matthew B and Saarakkala, Simo and Tiulpin, Aleksei},\n  journal={arXiv preprint arXiv:2210.13889},\n  year={2022}\n}\n", "metadata": {"source": "github_readmes\\imedslab_CLIMATv2_README.md", "filename": "imedslab_CLIMATv2_README.md", "type": "readme_full"}}
{"id": "invictus717_MetaTransformer_README.md", "paper_id": "invictus717_MetaTransformer_README", "text": "<p align=\"center\" width=\"100%\">\n<img src=\"assets\\Meta-Transformer_banner.png\"  width=\"80%\" height=\"80%\">\n</p>\n\n<div>\n<div align=\"center\">\n    <a href='https://scholar.google.com/citations?user=KuYlJCIAAAAJ&hl=en' target='_blank'>Yiyuan Zhang<sup>1,2*</sup></a>&emsp;\n    <a href='https://kxgong.github.io/' target='_blank'>Kaixiong Gong<sup>1,2*</sup></a>&emsp;\n    <a href='http://kpzhang93.github.io/' target='_blank'>Kaipeng Zhang<sup>2,†</sup></a>&emsp;\n    </br>\n    <a href='http://www.ee.cuhk.edu.hk/~hsli/' target='_blank'>Hongsheng Li <sup>1,2</sup></a>&emsp;\n    <a href='https://mmlab.siat.ac.cn/yuqiao/index.html' target='_blank'>Yu Qiao <sup>2</sup></a>&emsp;\n    <a href='https://wlouyang.github.io/' target='_blank'>Wanli Ouyang<sup>2</sup></a>&emsp;\n    <a href='http://people.eecs.berkeley.edu/~xyyue/' target='_blank'>Xiangyu Yue<sup>1,†,‡</sup></a>\n</div>\n<div>\n\n<div align=\"center\">\n    <sup>1</sup>\n    <a href='http://mmlab.ie.cuhk.edu.hk/' target='_blank'>Multimedia Lab, The Chinese University of Hong Kong</a>&emsp;\n    </br>\n    <sup>2</sup> <a href='https://github.com/OpenGVLab' target='_blank'>OpenGVLab，Shanghai AI Laboratory \n    </a></br>\n    <sup>*</sup> Equal Contribution&emsp;\n    <sup>†</sup> Corresponding Author&emsp;\n    <sup>‡</sup> Project Lead&emsp;\n</div>\n\n-----------------\n\n[![arXiv](https://img.shields.io/badge/arxiv-2307.10802-b31b1b?style=plastic&color=b31b1b&link=https%3A%2F%2Farxiv.org%2Fabs%2F2307.10802)](https://arxiv.org/abs/2307.10802)\n[![website](https://img.shields.io/badge/Project-Website-brightgreen)](https://kxgong.github.io/meta_transformer/)\n[![blog-cn](https://img.shields.io/badge/%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83-%E7%AE%80%E4%BB%8B-brightgreen)](https://mp.weixin.qq.com/s/r38bzqdJxDZUvtDI0c9CEw)\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Space-blue)](https://huggingface.co/papers/2307.10802)\n[![OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/zhangyiyuan/MetaTransformer)\n![](https://img.shields.io/github/stars/invictus717/MetaTransformer?style=social)\n<a href=\"https://twitter.com/_akhaliq/status/1682248055637041152\"><img src=\"https://img.icons8.com/color/48/000000/twitter.png\" width=\"25\" height=\"25\"></a>\n<a href=\"https://www.youtube.com/watch?v=V8L8xbsTyls&ab_channel=CSBoard\"><img src=\"https://img.icons8.com/color/48/000000/youtube-play.png\" width=\"25\" height=\"25\"></a> <a href='https://huggingface.co/kxgong/Meta-Transformer'> <img src=\"assets\\icons\\huggingface.png\" width=\"25\" height=\"25\"> </a> <a href='https://open.spotify.com/episode/6JJxcy2zMtTwr4jXPQEXjh'> <img src=\"https://upload.wikimedia.org/wikipedia/commons/1/19/Spotify_logo_without_text.svg\" width=\"20\" height=\"20\"></a>\n\n\n## Meta-Transformer with Large Language Models ✨✨✨\n\nWe're thrilled to present [OneLLM](https://github.com/csuhan/OneLLM), ensembling Meta-Transformer framework with Multimodal Large Language Models, which performs multimodal joint training🚀, supports more modalities including fMRI, Depth and Normal Maps 🚀, and demonstrates very impressive performances on **25** benchmarks🚀🚀🚀. \n\n🔥🔥 The code, pretrained models, and datasets are publicly available at [OneLLM](https://github.com/csuhan/OneLLM).\n\n🔥🔥 Project Website is at [OneLLM](https://onellm.csuhan.com/).\n\n### 🌟 Single Foundation Model Supports A Wide Range of Applications\n\n\n\nAs a foundation model, Meta-Transformer can handle data from 12 modalities, which determines that it can support a wide range of applications. As shown in this figure, Meta-Transformer can provide services for downstream tasks including stock analysis 📈, weather forecasting ☀️ ☔ ☁️ ❄️ ⛄ ⚡, remote sensing 📡, autonomous driving 🚗, social network 🌍, speech recognition 🔉, etc.\n\n<p align=\"center\" width=\"100%\">\n<img src=\"assets\\Meta-Transformer_application.png\"  width=\"100%\" height=\"100%\">\n</p>\n\n**Table 1**: Meta-Transformer is capable of handling up to 12 modalities, including natural language <img src=\"assets\\icons\\text.jpg\" width=\"15\" height=\"15\">, RGB images <img src=\"assets\\icons\\img.jpg\" width=\"15\" height=\"15\">, point clouds <img src=\"assets\\icons\\pcd.jpg\" width=\"15\" height=\"15\">, audios <img src=\"assets\\icons\\audio.jpg\" width=\"15\" height=\"15\">, videos <img src=\"assets\\icons\\video.jpg\" width=\"15\" height=\"15\">, tabular data <img src=\"assets\\icons\\table.jpg\" width=\"15\" height=\"15\">, graph <img src=\"assets\\icons\\graph.jpg\" width=\"15\" height=\"15\">, time series data <img src=\"assets\\icons\\time.jpg\" width=\"15\" height=\"15\">, hyper-spectral images <img src=\"assets\\icons\\hyper.jpg\" width=\"15\" height=\"15\">, IMU <img src=\"assets\\icons\\imu.jpg\" width=\"15\" height=\"15\">, medical images <img src=\"assets\\icons\\xray.jpg\" width=\"15\" height=\"15\">, and infrared images <img src=\"assets\\icons\\infrared.jpg\" width=\"15\" height=\"15\">.\n<p align=\"left\">\n<img src=\"assets\\Meta-Transformer_cmp.png\" width=100%>\n</p>\n\n## 🚩🚩🚩 Shared-Encoder, Unpaired Data, More Modalities \n\n\n<div>\n  <img class=\"image\" src=\"assets\\Meta-Transformer_teaser.png\" width=\"52%\" height=\"100%\">\n  <img class=\"image\" src=\"assets\\Meta-Transformer_exp.png\" width=\"45.2%\" height=\"100%\">\n</div>\n\n\nThis repository is built to explore the potential and extensibility of transformers for multimodal learning. We utilize the advantages of Transformers to deal with length-variant sequences. Then we propose the *Data-to-Sequence* tokenization following a meta-scheme, then we apply it to 12 modalities including text, image, point cloud, audio, video, infrared, hyper-spectral, X-Ray, tabular, graph, time-series, and Inertial Measurement Unit (IMU) data.\n\n<p align=\"left\">\n<img src=\"assets\\Meta-Transformer_data2seq.png\" width=100%>\n</p>\n\nAfter obtaining the token sequence, we employ a modality-shared encoder to extract representation across different modalities. With task-specific heads, Meta-Transformer can handle various tasks on the different modalities, such as: classification, detection, and segmentation.\n\n<p align=\"left\">\n<img src=\"assets\\Meta-Transformer_framework.png\" width=100%>\n</p>\n\n\n\n# 🌟 News\n* **2023.8.17:** Release code to directly get embeddings from multiple modalities. We will further release code on utilizing Meta-Transformer for Human-Centric vision tasks.\n* **2023.8.2:** 🎉🎉🎉 The implementation of Meta-Transformer for image, point cloud, graph, tabular, time-series, X-Ray, hyper-spectrum, LiDAR data has been released. We also release a very powerful foundation model for Autonomous Driving 🚀🚀🚀.  \n* **2023.7.22:** Pretrained weights and a usage demo for our Meta-Transformer have been released. Comprehensive documentation and implementation of the image modality are underway and will be released soon. Stay tuned for more exciting updates!⌛⌛⌛\n* **2023.7.21:** Paper is released at [arxiv](https://arxiv.org/abs/2307.10802), and code will be gradually released.\n* **2023.7.8:** Github Repository Initialization.\n\n# 🔓 Model Zoo\n\n<!-- <details> -->\n<summary> Open-source Modality-Agnostic Models </summary>\n<br>\n<div>\n\n|      Model      |   Pretraining   | Scale | #Param |                                               Download | 国内下载源                                               |\n| :------------: | :----------: | :----------------------: | :----: | :---------------------------------------------------------------------------------------------------: | :--------: | \n| Meta-Transformer-B16  | LAION-2B |         Base          |  85M  |   [ckpt](https://drive.google.com/file/d/19ahcN2QKknkir_bayhTW5rucuAiX0OXq/view?usp=sharing)    | [ckpt](https://download.openxlab.org.cn/models/zhangyiyuan/MetaTransformer/weight//Meta-Transformer_base_patch16_encoder)\n| Meta-Transformer-L14  | LAION-2B |         Large          |  302M  |   [ckpt](https://drive.google.com/file/d/15EtzCBAQSqmelhdLz6k880A19_RpcX9B/view?usp=drive_link)   | [ckpt](https://download.openxlab.org.cn/models/zhangyiyuan/MetaTransformer/weight//Meta-Transformer_large_patch14_encoder)\n\n</div>\n\n<!-- </details> -->\n\n<!-- <details> -->\n* Demo of Use for Pretrained Encoder\n\n```python\nimport torch \nimport torch.nn as nn\nfrom timm.models.vision_transformer import Block\nfrom Data2Seq import Data2Seq\nvideo_tokenier = Data2Seq(modality='video',dim=768)\naudio_tokenier = Data2Seq(modality='audio',dim=768)\ntime_series_tokenier = Data2Seq(modality='time-series',dim=768)\n\nfeatures = torch.concat([video_tokenizer(video), audio_tokenizer(audio), time_series_tokenizer(time_data)],dim=1)\n# For base-scale encoder:\nckpt = torch.load(\"Meta-Transformer_base_patch16_encoder.pth\")\nencoder = nn.Sequential(*[\n            Block(\n                dim=768,\n                num_heads=12,\n                mlp_ratio=4.,\n                qkv_bias=True,\n                norm_layer=nn.LayerNorm,\n                act_layer=nn.GELU\n            )\n            for i in range(12)])\nencoder.load_state_dict(ckpt,strict=True)\n# For large-scale encoder:\nckpt = torch.load(\"Meta-Transformer_large_patch14_encoder.pth\")\nencoder = nn.Sequential(*[\n            Block(\n                dim=1024,\n                num_heads=16,\n                mlp_ratio=4.,\n                qkv_bias=True,\n                norm_layer=nn.LayerNorm,\n                act_layer=nn.GELU\n            )\n            for i in range(24)])\nencoder.load_state_dict(ckpt,strict=True)\nencoded_features = encoder(features)\n```\n<!-- </details> -->\n\n# 🕙 ToDo\n- [ x ] Meta-Transformer with Large Language Models.\n- [ x ] Multimodal Joint Training with Meta-Transformer.\n- [ x ] Support More Modalities and More Tasks.\n\n# Contact\n🚀🚀🚀 We aspire to shape this repository into **a formidable foundation for mainstream AI perception tasks across diverse modalities**. Your contributions can play a significant role in this endeavor, and we warmly welcome your participation in our project!\n\nTo contact us, never hestitate to send an email to `yiyuanzhang.ai@gmail.com` ,`kaixionggong@gmail.com`, `zhangkaipeng@pjlab.org.cn`, or `xyyue@ie.cuhk.edu.hk`!\n<br></br>\n\n&ensp;\n# Citation\nIf the code and paper help your research, please kindly cite:\n```\n@article{zhang2023meta,\n  title={Meta-transformer: A unified framework for multimodal learning},\n  author={Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu},\n  journal={arXiv preprint arXiv:2307.10802},\n  year={2023}\n}\n```\n# License\nThis project is released under the [Apache 2.0 license](LICENSE).\n# Acknowledgement\nThis code is developed based on excellent open-sourced projects including [MMClassification](https://github.com/open-mmlab/mmpretrain/tree/mmcls-1.x), [MMDetection](https://github.com/open-mmlab/mmdetection), [MMsegmentation](https://github.com/open-mmlab/mmsegmentation), [OpenPoints](https://github.com/guochengqian/openpoints), [Time-Series-Library](https://github.com/thuml/Time-Series-Library), [Graphomer](https://github.com/microsoft/Graphormer), [SpectralFormer](https://github.com/danfenghong/IEEE_TGRS_SpectralFormer), and [ViT-Adapter](https://github.com/czczup/ViT-Adapter).\n", "metadata": {"source": "github_readmes\\invictus717_MetaTransformer_README.md", "filename": "invictus717_MetaTransformer_README.md", "type": "readme_full"}}
{"id": "jayleicn_TVCaption_README.md", "paper_id": "jayleicn_TVCaption_README", "text": "TVCaption\n===\nPyTorch implementation of MultiModal Transformer (MMT), a method for multimodal (video + subtitle) captioning.\n\n[TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval](https://arxiv.org/abs/2001.09099)\n\n[Jie Lei](http://www.cs.unc.edu/~jielei/), [Licheng Yu](http://www.cs.unc.edu/~licheng/),\n[Tamara L. Berg](http://tamaraberg.com/), [Mohit Bansal](http://www.cs.unc.edu/~mbansal/)\n\n### TVC Dataset and Task\n\nWe extended TVR by collecting extra captions \nfor each annotated moment. This dataset, named TV show Captions (TVC),\nis a large-scale multimodal video captioning dataset, \ncontains 262K captions paired with 108K moments. \nWe show our annotated captions and model generated captions below. \nSimilar to TVR, the TVC task requires systems to gather information\nfrom both video and subtitle to generate relevant descriptions. \n![tvc example](./imgs/caption_prediction.jpg)\n\n### Method: MultiModal Transformer (MMT)\n\n<p align=\"center\" >\n  <img src=\"./imgs/tvc_model_overview.png\" width=\"600\"/>\n</p>\n\nwe designed a MultiModal Transformer (MMT) captioning model which\nfollows the classical encoder-decoder transformer architecture. It takes both\nvideo and subtitle as encoder inputs to generate the captions from the decoder.\n\n## Resources\n- Data: [TVC dataset](./data/)\n- Website (with leaderboard): [https://tvr.cs.unc.edu/tvc.html](https://tvr.cs.unc.edu/tvc.html)\n- Submission: [codalab evaluation server](https://competitions.codalab.org/competitions/23109)\n- Related works: [TVR (Moment Retrieval)](https://github.com/jayleicn/TVRetrieval), [MART (Video Paragraph Captioning)](https://github.com/jayleicn/recurrent-transformer), [TVQA (Localized VideoQA)](https://github.com/jayleicn/TVQA), [TVQA+ (Grounded VideoQA)](https://github.com/jayleicn/TVQAplus)\n\n## Getting started\n### Prerequisites\n0. Clone this repository\n```\ngit clone --recursive https://github.com/jayleicn/TVCaption.git\ncd TVCaption\n```\n\n1. Prepare feature files\nDownload [tvc_feature_release.tar.gz ](https://drive.google.com/file/d/1bSjxbKSxp1qEBCSwAmk8YlkRl1ztgrWO/view?usp=sharing) (23GB).\nAfter downloading the file, extract it to the `data` directory.\n```\ntar -xf path/to/tvc_feature_release.tar.gz -C data\n```\nYou should be able to see `video_feature` under `data/tvc_feature_release` directory. \nIt contains video features (ResNet, I3D, ResNet+I3D), these features are the same as the video features \nwe used for [TVR/XML](https://github.com/jayleicn/TVRetrieval). \nRead the code to learn details on how the features are extracted: \n[video feature extraction](https://github.com/jayleicn/TVRetrieval/tree/master/utils/video_feature).\n\n\n2. Install dependencies:\n- Python 2.7\n- PyTorch 1.1.0\n- nltk\n- easydict\n- tqdm\n- h5py\n- tensorboardX\n\n3. Add project root to `PYTHONPATH`\n```\nsource setup.sh\n```\nNote that you need to do this each time you start a new session.\n\n### Training and Inference\n\n1. Build Vocabulary\n```\nbash baselines/transformer_captioning/scripts/build_vocab.sh\n```\nRunning this command will build vocabulary `cache/tvc_word2idx.json` from TVC train set. \n \n\n2. MMT training\n```\nbash baselines/multimodal_transformer/scripts/train.sh CTX_MODE VID_FEAT_TYPE\n```\n`CTX_MODE` refers to the context (video, sub, video_sub) we use. \n`VID_FEAT_TYPE` video feature type (resnet, i3d, resnet_i3d). \n\nBelow is an example of training MMT with both video and subtitle, where we use \nthe concatenation of ResNet and I3D features for video.\n```\nbash baselines/multimodal_transformer/scripts/train.sh video_sub resnet_i3d\n```\nThis code will load all the data (~30GB) into RAM to speed up training,\nuse `--no_core_driver` to disable this behavior. \n\nTraining using the above config will stop at around epoch 22, around 3 hours with a single 2080Ti GPU.\nYou should get ~45.0 CIDEr-D and ~10.5 BLEU@4 scores on val split. \nThe resulting model and config will be saved at a dir: `baselines/multimodal_transformer/results/video_sub-res-*`\n\n3. MMT inference\nAfter training, you can inference using the saved model on val or test_public split:\n```\nbash baselines/multimodal_transformer/scripts/translate.sh MODEL_DIR_NAME SPLIT_NAME\n```\n`MODEL_DIR_NAME` is the name of the dir containing the saved model, \ne.g., `video_sub-res-*`.  `SPLIT_NAME` could be `val` or `test_public`. \n\n\n\n### Evaluation and Submission\n\nWe only release ground-truth for train and val splits, to get results on test-public split, \nplease submit your results follow the instructions here:\n[standalone_eval/README.md](standalone_eval/README.md)\n\n\n## Citations\nIf you find this code useful for your research, please cite our paper:\n```\n@inproceedings{lei2020tvr,\n  title={TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval},\n  author={Lei, Jie and Yu, Licheng and Berg, Tamara L and Bansal, Mohit},\n  booktitle={ECCV},\n  year={2020}\n}\n```\n\n## Acknowledgement\nThis research is supported by grants and awards from NSF, DARPA, ARO and Google.\n\nThis code borrowed components from the following projects: \n[recurrent-transformer](https://github.com/jayleicn/recurrent-transformer),\n[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py), \n[transformers](https://github.com/huggingface/transformers),\n[coco-caption](https://github.com/tylin/coco-caption),\nwe thank the authors for open-sourcing these great projects! \n\n\n## Contact\njielei [at] cs.unc.edu\n", "metadata": {"source": "github_readmes\\jayleicn_TVCaption_README.md", "filename": "jayleicn_TVCaption_README.md", "type": "readme_full"}}
{"id": "jinyanglii_OVTR_README.md", "paper_id": "jinyanglii_OVTR_README", "text": "# OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer\n\n\n<p align=\"center\"><img src=\"assets/Method.png\" width=\"600\"/></p>\n\n> [**OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer**](https://arxiv.org/abs/2503.10616)\n> \n> Jinyang Li, En Yu, Sijia Chen, Wenbing Tao\n> \n> [![Paper](https://img.shields.io/badge/arXiv-2503.10616-brightgreen.svg?style=flat-square)](https://arxiv.org/abs/2503.10616)\n\n- We propose the first end-to-end open-vocabulary multi-object tracking algorithm, introducing a novel perspective to the OVMOT field, achieving faster inference speeds, and possessing strong scalability with potential for further improvement.\n- We propose the category information propagation (CIP) strategy to enhance the stability of tracking and classification, along with the attention isolation strategies that ensure open-vocabulary perception and tracking operate in harmony.\n- We propose a dual-branch decoder guided by an alignment mechanism, empowering the model with strong open-vocabulary perception and multimodal interaction capabilities while eliminating the need for time-consuming preprocessing.\n\n<p align=\"center\"><img src=\"assets/Overview_ovtr.png\" width=\"700\"/></p>\n\n## 💡 News\n* We update the files to resolve environment configuration and code execution issues. **(2025-03-20)**\n* We release the code, scripts and checkpoints on TAO\n* Our paper is accepted by ICLR 2025!\n\n## 🌞 Main results\n\n### Open-Vocabulary Multiple Object Tracking on the TAO validation set\n| **Method** | Data  |  Embeds  | ${\\text{Prop}}_{\\text{novel}}$ | **FPS**  | TETA↑(novel)  | AssocA↑(novel) | ClsA↑(novel) | TETA↑(base)   | AssocA↑(base) | ClsA↑(base)    |\n| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n| OVTrack                 | G-LVIS, LVIS | 99.4M | ✓   | 3.1            | 27.8           | 33.6           | 1.5           | 35.5           | 36.9           | **20.2** |\n| **OVTR** [[weights]](https://drive.google.com/file/d/10GKAIBxAseTiXnJXV1MnxnJBTmOHVFh5/view?usp=sharing)                  | LVIS        | 1,732 |    | **3.4(12.4)**        | **31.4**       | **34.5**       | **5.4**       | **36.6**       | **37.6**       | 20.1     |    \n| **OVTR-Lite** [[weights]](https://drive.google.com/file/d/1x6DciXsRIOzT24typcuryqmtdVJKnXZI/view?usp=sharing)           | LVIS        | 1,732 |    | **12.4**       | **30.1**       | **34.4**       | **3.1**       | **35.6**       | **37.0**       | 18.6     |\n\n### Open-Vocabulary Multiple Object Tracking on the TAO test set\n| **Method** | Data  |  Embeds  | ${\\text{Prop}}_{\\text{novel}}$ | **FPS**  | TETA↑(novel)  | AssocA↑(novel) | ClsA↑(novel) | TETA↑(base)   | AssocA↑(base) | ClsA↑(base)    |\n| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n| OVTrack                 | G-LVIS,LVIS | 99.4M | ✓   | 3.1           | 24.1           | 28.7            | 1.8           | 32.6           | 35.4           | **16.9** |\n| **OVTR** [[weights]](https://drive.google.com/file/d/10GKAIBxAseTiXnJXV1MnxnJBTmOHVFh5/view?usp=sharing)                  | LVIS        | 1,732 |    | **3.4**        | **27.1** | **32.1**  | **2.1** | **34.5** | **37.5** | 14.9       |\n\nSpeed tests are performed on a single NVIDIA GeForce RTX 3090 GPU.\n\n## 🔧 Installation\n\n```shell\n# create a virtual env\nconda create -n OVTR python=3.9\n# activate the env\nconda activate OVTR\n\n# install OVTR\ngit clone https://github.com/jinyanglii/OVTR.git\ncd OVTR\npip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html\npip install -r requirements.txt\n# Different installation versions may lead to changes in evaluation scores.\n\n# create a model_zoo folder\nmkdir model_zoo\n```\n - Please install [CLIP](https://github.com/openai/CLIP) package following its official installation guide.\n - Compile the Deformable Attention CUDA ops:\n\n```shell\n# from https://github.com/fundamentalvision/Deformable-DETR\ncd ovtr/models/ops\nsh make.sh\n```\n\n\n## 💽 Data\n\nPlace the unzipped [TAO](https://taodataset.org/) dataset in the `data/` directory. If you choose to perform detection pretraining from scratch or generate CLIP image embeddings yourself, you may also place the [LVIS](https://www.lvisdataset.org/) dataset in the `data/` directory (optional).\n\nObtain the filtered images [lvis_filtered_train_images.h5](https://huggingface.co/datasets/jinyanglii/lvis_filtered_train_images/tree/main) and the processed annotations [lvis_clear_75_60.json](https://drive.google.com/file/d/1Eu6UbrwJ-h68Z0cKQX5X_QO06mXT7bEc/view?usp=sharing). The [TAO dataset annotations](https://drive.google.com/drive/folders/1I4kWTH09hrpe92P53p6VskCcqRkrZgyo?usp=sharing) generated by [OVTrack](https://github.com/SysCV/ovtrack/blob/main/docs/GET_STARTED.md) are also required.\n\nYou can also use [lvis_filter.ipynb](./process/lvis_filter.ipynb) to customize the annotation processing and generate `lvis_clear_75_60.json`. Before this, ensure [lvis_image_v1.json](https://drive.google.com/file/d/1gbj2_L0i9ediiTkFMdDRY-SXM8ovRP5v/view?usp=sharing) (which combines LVIS and COCO annotations) is prepared beforehand.\n\nFinally, you should get the following structure of Dataset and Annotations:\n\n```\ndata/\n  ├── Lvis_v1/\n  │ ├── train2017/\n  │ └── annotations/\n  │   └── lvis_v1_train.json\n  ├── lvis_filtered_train_images.h5 # Filter out images that only contain rare category targets\n  ├── TAO/\n  │ ├── val/\n  │ └── test/\n  ├── lvis_image_v1.json\n  ├── lvis_clear_75_60.json\n  ├── lvis_classes_v1.txt\n  ├── validation_ours_v1.json # From OVTrack\n  └── tao_test_burst_v1.json\n ```\n\n\n\n## Training\nPlace [ovtr_det_pretrain.pth](https://drive.google.com/file/d/1x5RQ5m6XlLYB_iOPDnbeEKSYQeT4HVwo/view?usp=sharing) in the `model_zoo/` folder.\n\nAlso, download the [CLIP text embeddings ('iou_neg5_ens.pth')](https://drive.google.com/file/d/1OYvyCQ_y65oq6SDJQKrVm3syzvXStL-0/view?usp=sharing) and [CLIP image embeddings](https://drive.google.com/file/d/1j5l-BPv-f43fb953hmIijxQ4gSUduWWe/view?usp=sharing) and place them in the `model_zoo/` folder. The prompts for `CLIP text embeddings` are sourced from [DetPro](https://github.com/dyabel/detpro).\n\nYou can also generate the `CLIP image embeddings` yourself by using [gen_clip_image_embeds.py](./process/gen_clip_image_embeds.py) for the preprocessing.\n\n - **Train a complete OVTR**\n```shell\ncd ovtr\nsh tools/ovtr_multi_frame_train.sh\n```\n\n - **Train Lite version (recommended)**\n```shell\ncd ovtr\nsh tools/ovtr_multi_frame_lite_train.sh\n```\n\n### Detection Pre-training\nYou can also choose to pre-train the detection perception part from scratch:\n\nObtain the [DINO dual-branch weights](https://drive.google.com/file/d/1iPMKa4ZUgs-no0PJot-KqRPdwwrDUa9V/view?usp=sharing) (generated by using [modify_dino_weight.py](./process/modify_dino_weight.py) to convert the `dino_ep33_4scale.pth` from [DINO](https://github.com/IDEA-Research/DINO), making it compatible with the dual-branch structure).\n\n```shell\ncd ovtr_det_bs2_pretrain\nsh tools/ovtr_detection_pretrain.sh\n```\n\n## Evaluation\n\nPlace the weights [ovtr_5_frame.pth](https://drive.google.com/file/d/10GKAIBxAseTiXnJXV1MnxnJBTmOHVFh5/view?usp=sharing) and [ovtr_lite.pth](https://drive.google.com/file/d/1x6DciXsRIOzT24typcuryqmtdVJKnXZI/view?usp=sharing) in the `model_zoo/` folder.\n\n### Evaluate the performance of OVTR in open-vocabulary multi-object tracking (OVMOT).\n - **Evaluate OVTR on the TAO validation set using the TETA metric.**\n```shell\ncd ovtr\nsh tools/ovtr_ovmot_eval_e15_val.sh\n```\n - **Evaluate OVTR on the TAO test set using the TETA metric.**\n```shell\ncd ovtr\nsh tools/ovtr_ovmot_eval_e15_test.sh\n```\n - **Evaluate OVTR-Lite on the TAO validation set using the TETA metric.**\n```shell\ncd ovtr\nsh tools/ovtr_ovmot_eval_lite_val.sh\n```\n - **Evaluate OVTR-Lite on the TAO test set using the TETA metric.**\n```shell\ncd ovtr\nsh tools/ovtr_ovmot_eval_lite_test.sh\n```\n\n## 🎬 Demo\n<img src=\"ovtr/results/track_demo.gif\" width=\"800\"/>\n\n\n - **Run a demo of OVTR.**\n```shell\ncd ovtr\nsh tools/ovtr_demo.sh\n```\n\n## Citation\n```bibtex\n@article{li2025ovtr,\n  title={OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer},\n  author={Li, Jinyang and Yu, En and Chen, Sijia and Tao, Wenbing},\n  journal={arXiv preprint arXiv:2503.10616},\n  year={2025}\n}\n```\n\n## Acknowledgement\n\n- [MOTR](https://github.com/megvii-research/MOTR)\n- [OVTrack](https://github.com/SysCV/ovtrack)\n- [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO)\n- [DetPro](https://github.com/dyabel/detpro)\n- [DINO](https://github.com/IDEA-Research/DINO)\n- [TETA](https://github.com/SysCV/tet)\n\n\n", "metadata": {"source": "github_readmes\\jinyanglii_OVTR_README.md", "filename": "jinyanglii_OVTR_README.md", "type": "readme_full"}}
{"id": "Kateridge_TransMF_AD_README.md", "paper_id": "Kateridge_TransMF_AD_README", "text": "# Transformer-based Multimodal Fusion for Early Diagnosis of Alzheimer’s Disease Using Structural MRI and PET\n\nThis repo contains PyTorch implementation of the paper: Transformer-based Multimodal Fusion for Early Diagnosis of Alzheimer’s Disease Using Structural MRI and PET. \n\n[Paper Link](https://ieeexplore.ieee.org/abstract/document/10230577/)\n\n![image](img/method.png)\n\n## Setup Instructions\n\n### Datasets\n\nDirectory structure\n\n   ````\n   ├── MRI                            # MRI images\n   │    ├── sub-ADNI001S0001.nii.gz\n   │    ├── sub-ADNI009S0001.nii.gz   \n   │    ├── ...\n   ├── PET                            # PET images\n   │    ├── sub-ADNI001S0001.nii.gz\n   │    ├── sub-ADNI009S0001.nii.gz\n   │    ├── ...\n   ├── ADNI.csv                       # label file \n   ````\n\nLabel file\n\n````\n| Subject          | Group | Age | ...\n————————————————————————————————————————\n| sub-ADNI001S0001 | AD    | xx  | ...\n| sub-ADNI009S0001 | CN    | xx  | ...\n| sub-ADNI002S0001 | pMCI  | xx  | ...\n| sub-ADNI003S0001 | sMCI  | xx  | ...\n| ...              | ...   | ..  | ...\n````\n\n### Dependancies\n\n``pip install -r ./requirements.txt``\n\n## Run the Model\n\n``python kfold_train_adversarial.py --randint False --aug True --batch_size 8 --name <expr_name> --task <ADCN/pMCIsMCI> --model <CNN/Transformer> --dataroot <data_dir>\n``\n\n## References\n\nIf you think our research work helpful, please consider citing our original paper.\n\n````\n@inproceedings{zhang2023transformer,\n  title={Transformer-Based Multimodal Fusion for Early Diagnosis of Alzheimer's Disease Using Structural MRI And PET},\n  author={Zhang, Yuanwang and Sun, Kaicong and Liu, Yuxiao and Shen, Dinggang},\n  booktitle={2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}\n````\n", "metadata": {"source": "github_readmes\\Kateridge_TransMF_AD_README.md", "filename": "Kateridge_TransMF_AD_README.md", "type": "readme_full"}}
{"id": "kimkyeonghun_MSA_README.md", "paper_id": "kimkyeonghun_MSA_README", "text": "# Multimodal Sentiment Analysis\n\n\n\nMultimodal Sentiment Analysis(MSA) is part of Sentiment Analysis. MSA uses several modality like visual, speech with text. It helps more better prediction when using only text.\n\n\n\nThe core of MSA is how to concatenate(or fuse) different modalities. But it is so hard because they usually have different frequency. At previous research, apply different method like LSTM or Transformer. Especially, Co-Transformer got better results than more older models. However, it also has problems that each modality affects the results about same portion.\n\n\n\nSo, I suggest unified-transformer with inputs that concatenated between different modalities. I guesses it can solve problem in Co-Transformer.\n\n\n\n## 1. Requirements\n\n\n\n- python : 3.7\n\n- pytorch : 1.5\n\n- transformers : 2.8.0\n\n\n\nIf you get \"Segment fault\", you must downgrade sentencepiece to version 0.1.91.\n\n\n\n## 2. Model\n\n\n\n", "metadata": {"source": "github_readmes\\kimkyeonghun_MSA_README.md", "filename": "kimkyeonghun_MSA_README.md", "type": "readme_full"}}
{"id": "kyegomez_Andromeda_README.md", "paper_id": "kyegomez_Andromeda_README", "text": "[![Multi-Modality](images/agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n\n# Andromeda: Ultra-Fast and Ultra-Intelligent SOTA Language Model 🚀🌌\n\n<div align=\"center\">\n\n[![Open Bounties](https://img.shields.io/endpoint?url=https%3A%2F%2Fconsole.algora.io%2Fapi%2Fshields%2Fkyegomez%2Fbounties%3Fstatus%3Dopen)](https://console.algora.io/org/kyegomez/bounties?status=open)\n[![Rewarded Bounties](https://img.shields.io/endpoint?url=https%3A%2F%2Fconsole.algora.io%2Fapi%2Fshields%2Fkyegomez%2Fbounties%3Fstatus%3Dcompleted)](https://console.algora.io/org/kyegomez/bounties?status=completed)\n[![GitHub issues](https://img.shields.io/github/issues/kyegomez/Andromeda)](https://github.com/kyegomez/Andromeda/issues) \n[![GitHub forks](https://img.shields.io/github/forks/kyegomez/Andromeda)](https://github.com/kyegomez/Andromeda/network) \n[![GitHub stars](https://img.shields.io/github/stars/kyegomez/Andromeda)](https://github.com/kyegomez/Andromeda/stargazers) \n[![GitHub license](https://img.shields.io/github/license/kyegomez/Andromeda)](https://github.com/kyegomez/Andromeda/blob/main/LICENSE)\n[![Share on Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Share%20%40kyegomez/Andromeda)](https://twitter.com/intent/tweet?text=Check%20out%20this%20amazing%20AI%20project:%20Andromeda&url=https%3A%2F%2Fgithub.com%2Fkyegomez%2FAndromeda) \n[![Share on Facebook](https://img.shields.io/badge/Share-%20facebook-blue)](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fgithub.com%2Fkyegomez%2FAndromeda) \n[![Share on LinkedIn](https://img.shields.io/badge/Share-%20linkedin-blue)](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fgithub.com%2Fkyegomez%2FAndromeda&title=&summary=&source=)\n![Discord](https://img.shields.io/discord/999382051935506503)\n[![Share on Reddit](https://img.shields.io/badge/-Share%20on%20Reddit-orange)](https://www.reddit.com/submit?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2FAndromeda&title=Andromeda%20-%20the%20next%20generation%20AI%20shields) \n[![Share on Hacker News](https://img.shields.io/badge/-Share%20on%20Hacker%20News-orange)](https://news.ycombinator.com/submitlink?u=https%3A%2F%2Fgithub.com%2Fkyegomez%2FAndromeda&t=Andromeda%20-%20the%20next%20generation%20AI%20shields) \n[![Share on Pinterest](https://img.shields.io/badge/-Share%20on%20Pinterest-red)](https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fgithub.com%2Fkyegomez%2FAndromeda&media=https%3A%2F%2Fexample.com%2Fimage.jpg&description=Andromeda%20-%20the%20next%20generation%20AI%20shields) \n[![Share on WhatsApp](https://img.shields.io/badge/-Share%20on%20WhatsApp-green)](https://api.whatsapp.com/send?text=Check%20out%20Andromeda%20-%20the%20next%20generation%20AI%20shields%20%23Andromeda%20%23AI%0A%0Ahttps%3A%2F%2Fgithub.com%2Fkyegomez%2FAndromeda)\n\n</div>\n\n\n\nWelcome to Andromeda, The Fastest, Most Creative, and Reliable Language Model Ever Built, train your own verison, conduct inference, and finetune your own verison with simple plug in and play scripts get started in 10 seconds:\n\n## Features\n\n- 💼 Handle Ultra Long Sequences (32,000-200,000+ context lengths)\n- ⚡ Ultra Fast Processing (32,000+ tokens in under 100ms)\n- 🎓 Superior Reasoning Capabilities\n\n## 🎯 Principles\n\n- **Efficiency**: Optimize with techniques like attention flashing, rotary position encodings, and deep normalization.\n- **Flexibility**: Adapt to various tasks and domains for wide applications.\n- **Scalability**: Designed to scale with resources and data sizes.\n- **Community-Driven**: Thrives on contributions from the open-source community.\n\n---\n\n\n## 💻 Install\n\n`python3.11 -m pip install --upgrade andromeda-torch`\n\n\n## Usage\n- Forward pass with random inputs\n```python\nimport torch\n\nfrom andromeda.configs import Andromeda1Billion\n\nmodel = Andromeda1Billion()\n\nx = torch.randint(0, 256, (1, 1024)).cuda()\n\nout = model(x)  # (1, 1024, 20000)\nprint(out)\n```\n\n- Tokenized inputs\n```python\nfrom andromeda_torch import Tokenizer\nfrom andromeda_torch.configs import Andromeda1Billion\n\nmodel = Andromeda1Billion()\ntokenizer = Tokenizer()\n\nencoded_text = tokenizer.encode(\"Hello world!\")\nout = model(encoded_text)\nprint(out)\n\n\n```\n\n\n\n## 📚 Training\n\n1. Set the environment variables:\n   - `ENTITY_NAME`: Your wandb project name\n   - `OUTPUT_DIR`: Directory to save the weights (e.g., `./weights`)\n   - `MASTER_ADDR`: For distributed training\n   - `MASTER_PORT` For master port distributed training\n   - `RANK`- Number of nodes services\n   - `WORLD_SIZE` Number of gpus\n\n2. Configure the training:\n   - Accelerate Config\n   - Enable Deepspeed 3\n   - Accelerate launch train_distributed_accelerate.py\n\nFor more information, refer to the [Training SOP](DOCs/TRAINING.md).\n\n---\n\n\n## Todo\n- [ ] Add Yarn Embeddings from zeta\n\n\n\n## 📈 Benchmarks\n\n### Speed\n- Andromeda utilizes one of the most reliable Attentions ever, flash attention 2.0 Triton. It consumes 50x less memory than GPT-3 and 10x less than LLAMA.\n\n![AndromedaBanner](images/andromeda_performance.png)\n\n- We can speed this up even more with dynamic sparse flash attention 2.0.\n\n# License\nApache License", "metadata": {"source": "github_readmes\\kyegomez_Andromeda_README.md", "filename": "kyegomez_Andromeda_README.md", "type": "readme_full"}}
{"id": "kyegomez_BitNet_README.md", "paper_id": "kyegomez_BitNet_README", "text": "[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n# BitNet\n![bitnet](/bitnet.png)\nPyTorch Implementation of the linear methods and model from the paper \"BitNet: Scaling 1-bit Transformers for Large Language Models\"\n\n[Paper link:](https://arxiv.org/pdf/2310.11453.pdf)\n\nBitLinear = tensor -> layernorm -> Binarize -> abs max quantization -> dequant\n\n\"The implementation of the BitNet architecture is quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch) in the Transformer. \" -- BitNet is really easy to implement just swap out the linears with the BitLinear modules! \n\n## **NEWS**\n- **New Iteration** 🔥 There is an all-new iteration from the paper \"[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)\", we're implementing it now. Join the Agora discord and contribute! [Join Here](https://discord.gg/hFzevCjG8c)\n- **New Optimizations** The first `BitLinear` has been optimized and we now have a Bit Attention `BitMGQA` That implements BitLinear into the attention mechanism. Multi Grouped Query Attention is also widely recognized as the best attention for its fast decoding and long context handling, thanks to Frank for his easy to use implementation!\n- **BitLinear 1.5 Launch 🔥**: The new BitLinear 1.5 is still in progress 🔥 [Here is the file]() There are still some bugs like with the dequantization algorithm and we still need to replace the multiplication with elementwisw addition, if you could help with this, this would be amazing.\n- **NOTICE**: A model obviously needs to be finetuned from scratch to use BitLinear, just changing the linear methods in an already trained model isn't going to work. Finetune or train from scratch.\n\n## Appreciation\n- Dimitry, Nullonix for analysis and code review and revision\n- Vyom, for providing 4080 to train!\n\n## Installation\n```bash\npip3 install bitnet\n```\n\n## Usage\nWe have a vast selection of example scripts here and in the [examples folder:](/examples/), let me know if you want assistance with a use-case in the discord!\n\n\n### `BitLinear`\n- Example of the BitLinear layer which is the main innovation of the paper!\n```python\nimport torch\n\nfrom bitnet import BitLinear\n\n# Input\nx = torch.randn(10, 1000, 512)\n\n# BitLinear layer\nlayer = BitLinear(512, 400)\n\n# Output\ny = layer(x)\n\nprint(y)\n\n```\n\n### BitLinearNew\n```python\nimport torch\nfrom bitnet import BitLinearNew\n\n# Create a random tensor of shape (16, 10)\nx = torch.randn(16, 1000, 512)\n\n# Create an instance of the BitLinearNew class with input size 10, output size 20, and 2 groups\nlayer = BitLinearNew(\n    512,\n    20,\n)\n\n# Perform a forward pass through the BitLinearNew layer with input x\noutput = layer(x)\n\n# Print the output tensor\nprint(output)\nprint(output.shape)\n```\n----\n\n### `BitNetTransformer`\n- Fully implemented Transformer as described in the diagram with MHA, and BitFeedforwards\n- Can be utilized not just for text but for images and maybe even video or audio processing\n- Complete with residuals and skip connections for gradient flow\n\n```python\n# Import the necessary libraries\nimport torch\nfrom bitnet import BitNetTransformer\n\n# Create a random tensor of integers\nx = torch.randint(0, 20000, (1, 1024))\n\n# Initialize the BitNetTransformer model\nbitnet = BitNetTransformer(\n    num_tokens=20000,  # Number of unique tokens in the input\n    dim=1024,  # Dimension of the input and output embeddings\n    depth=6,  # Number of transformer layers\n    heads=8,  # Number of attention heads\n    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network\n)\n\n# Pass the tensor through the transformer model\nlogits = bitnet(x)\n\n# Print the shape of the output\nprint(logits)\n\n```\n\n\n### `BitAttention`\nThis Attention has been modified to use BitLinear instead of the default linear projection. It's also using Multi-Grouped Query Attention instead of regular multi-head attention for faster decoding and longer context handling.\n\n```python\nimport torch\nfrom bitnet import BitMGQA\n\n# Create a random tensor of shape (1, 10, 512)\nx = torch.randn(1, 10, 512)\n\n# Create an instance of the BitMGQA model with input size 512, 8 attention heads, and 4 layers\ngqa = BitMGQA(512, 8, 4)\n\n# Pass the input tensor through the BitMGQA model and get the output and attention weights\nout, _ = gqa(x, x, x, need_weights=True)\n\n# Print the shapes of the output tensor and attention tensor\nprint(out)\n```\n\n### `BitFeedForward`\n- Feedforward as shown in the diagram with BitLinear and a GELU:\n- Linear -> GELU -> Linear\n- You can add dropouts, or layernorms, or other layers for a better ffn\n\n```python\nimport torch\nfrom bitnet import BitFeedForward\n\n# Create a random input tensor of shape (10, 512)\nx = torch.randn(10, 512)\n\n# Create an instance of the BitFeedForward class with the following parameters:\n# - input_dim: 512\n# - hidden_dim: 512\n# - num_layers: 4\n# - swish: True (use Swish activation function)\n# - post_act_ln: True (apply Layer Normalization after each activation)\n# - dropout: 0.1 (apply dropout with a probability of 0.1)\nff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)\n\n# Apply the BitFeedForward network to the input tensor x\ny = ff(x)\n\n# Print the shape of the output tensor y\nprint(y)  # torch.Size([10, 512])\n```\n\n## Inference\n```python\nfrom bitnet import BitNetInference\n\nbitnet = BitNetInference()\nbitnet.load_model(\"../model_checkpoint.pth\")  # Download model\noutput_str = bitnet.generate(\"The dog jumped over the \", 512)\nprint(output_str)\n```\n\n## Huggingface Usage\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nfrom bitnet import replace_linears_in_hf\n\n# Load a model from Hugging Face's Transformers\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Replace Linear layers with BitLinear\nreplace_linears_in_hf(model)\n\n# Example text to classify\ntext = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n\n# Perform inference\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    print(predictions)\n\n# Process predictions\npredicted_class_id = predictions.argmax().item()\nprint(f\"Predicted class ID: {predicted_class_id}\")\n\n# Optionally, map the predicted class ID to a label, if you know the classification labels\n# labels = [\"Label 1\", \"Label 2\", ...]  # Define your labels corresponding to the model's classes\n# print(f\"Predicted label: {labels[predicted_class_id]}\")\n```\n\n\n## Drop in Replacement for Pytorch Models\n```python\nimport torch\nfrom torch import nn\nfrom bitnet import replace_linears_in_pytorch_model\n\n# Define a simple model\nmodel = nn.Sequential(\n    nn.Linear(10, 20),\n    nn.ReLU(),\n    nn.Linear(20, 30),\n)\n\nprint(\"Before replacement:\")\nprint(model)\n\n# Replace nn.Linear with BitLinear\nreplace_linears_in_pytorch_model(model)\n\nprint(\"After replacement:\")\nprint(model)\n\n# Now you can use the model for training or inference\n# For example, pass a random input through the model\ninput = torch.randn(1, 10)\noutput = model(input)\n```\n\n\n### Optimized Cuda Kernel\n`python setup.py build_ext --inplace`\n\n```python\nimport torch\nimport gemm_lowbit_ext  # This imports the compiled module\n\n# Example usage\na = torch.randn(10, 20, dtype=torch.half, device='cuda')  # Example tensor\nb = torch.randn(20, 30, dtype=torch.half, device='cuda')  # Example tensor\nc = torch.empty(10, 30, dtype=torch.half, device='cuda')  # Output tensor\n\nw_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n\n# Call the custom CUDA GEMM operation\ngemm_lowbit_ext.gemm_lowbit(a, b, c, w_scale, x_scale)\n\nprint(c)  # View the result\n\n```\n\n\n## `BitLora`\nImplementation of BitLora!\n\n```python\nimport torch\nfrom bitnet import BitLora\n\n# Random text tensor\nx = torch.randn(1, 12, 200)\n\n# Create an instance of the BitLora model\nmodel = BitLora(in_features=200, out_features=200, rank=4, lora_alpha=1)\n\n# Perform the forward pass\nout = model(x)\n\n# Print the shape of the output tensor\nprint(out.shape)\n```\n\n\n## BitMamba\n```python\nimport torch\nfrom bitnet import BitMamba\n\n# Create a tensor of size (2, 10) with random values between 0 and 100\nx = torch.randint(0, 100, (2, 10))\n\n# Create an instance of the BitMamba model with input size 512, hidden size 100, output size 10, and depth size 6\nmodel = BitMamba(512, 100, 10, 6, return_tokens=True)\n\n# Pass the input tensor through the model and get the output\noutput = model(x)\n\n# Print the output tensor\nprint(output)\n\n# Print the shape of the output tensor\nprint(output.shape)\n\n```\n\n## `BitMoE`\n\n```python\nimport torch\nfrom bitnet.bit_moe import BitMoE\n\n# Create input tensor\nx = torch.randn(2, 4, 8)\n\n# Create BitMoE model with specified input and output dimensions\nmodel = BitMoE(8, 4, 2)\n\n# Forward pass through the model\noutput = model(x)\n\n# Print the output\nprint(output)\n```\n\n\n### 1 Bit Vision Transformers\nThis idea came to me out of nowhere but it seems to be pretty fun, as you can leverage bitlinear for vision tasks for ultra-compression. It would be nice to train this on imagenet if you could make a script, we'll provide the compute. Then the next stage would be to train a join vision language model gpt-4o\n\n```python\nimport torch\nfrom bitnet import OneBitViT\n\n# Create an instance of the OneBitViT model\nv = OneBitViT(\n    image_size=256,\n    patch_size=32,\n    num_classes=1000,\n    dim=1024,\n    depth=6,\n    heads=16,\n    mlp_dim=2048,\n)\n\n# Generate a random image tensor\nimg = torch.randn(1, 3, 256, 256)\n\n# Pass the image through the OneBitViT model to get predictions\npreds = v(img)  # (1, 1000)\n\n# Print the predictions\nprint(preds)\n\n```\n\n# License\nMIT\n\n# Citation\n```bibtex\n@misc{2310.11453,\nAuthor = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},\nTitle = {BitNet: Scaling 1-bit Transformers for Large Language Models},\nYear = {2023},\nEprint = {arXiv:2310.11453},\n}\n\n```\n\n\n# Todo\n- [x] Double check BitLinear implementation and make sure it works exactly as in paper \n- [x] Implement training script for `BitNetTransformer`\n- [x] Train on Enwiki8, copy and past code and data from Lucidrains repos\n- [x] Benchmark performance\n- [x] Look into Straight Through Estimator for non-differentiable backprop\n- [x] Implement BitFeedForward\n- [x] Clean up codebase \n- [x] Add unit tests for each module\n- [x] Implement the new BitNet1.5b from the [paper](https://arxiv.org/abs/2402.17764)\n- [ ] Implement the BitNet15b in Cuda\n- [ ] Implement the low bit gemm cuda kernel ", "metadata": {"source": "github_readmes\\kyegomez_BitNet_README.md", "filename": "kyegomez_BitNet_README.md", "type": "readme_full"}}
{"id": "kyegomez_MambaTransformer_README.md", "paper_id": "kyegomez_MambaTransformer_README", "text": "[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n# Mamba Transformer\n\n![Mamba Transformer](/mm_transformer.png)\n\nIntegrating Mamba/SSMs with Transformer for Enhanced Long Context and High-Quality Sequence Modeling.\n\nThis is 100% novel architecture that I have designed to combine the strengths and weaknesses out of SSMs and Attention for an all-new advanced architecture with the purpose of surpassing our old limits. Faster processing speed, longer context lengths, lower perplexity over long sequences, enhanced and superior reasoning while remaining small and compact.\n\nThe architecture is essentially: `x -> norm -> mamba -> norm -> transformer -> norm -> ffn -> norm -> out`.\n\nI added in many normalizations as I believe by default training stability would be severly degraded due to 2 foreign architecture's integrating with one another.\n\n\n## Install\n`pip3 install mambatransformer`\n\n\n### Usage\n```python\nimport torch\nfrom mamba_transformer import MambaTransformer\n\n# Generate a random tensor of shape (1, 10) with values between 0 and 99\nx = torch.randint(0, 100, (1, 10))\n\n# Create an instance of the MambaTransformer model\nmodel = MambaTransformer(\n    num_tokens=100,  # Number of tokens in the input sequence\n    dim=512,  # Dimension of the model\n    heads=8,  # Number of attention heads\n    depth=4,  # Number of transformer layers\n    dim_head=64,  # Dimension of each attention head\n    d_state=512,  # Dimension of the state\n    dropout=0.1,  # Dropout rate\n    ff_mult=4,  # Multiplier for the feed-forward layer dimension\n    return_embeddings=False,  # Whether to return the embeddings,\n    transformer_depth=2,  # Number of transformer blocks\n    mamba_depth=10,  # Number of Mamba blocks,\n    use_linear_attn=True,  # Whether to use linear attention\n)\n\n# Pass the input tensor through the model and print the output shape\nout = model(x)\n\nprint(out.shape)\n\n\n# After many training\nmodel.eval()\n\n# Would you like to train this model? Zeta Corporation offers unmatchable GPU clusters at unbeatable prices, let's partner!\n\n# Tokenizer\nmodel.generate(text)\n\n\n```\n\n# License\nMIT\n\n\n\n", "metadata": {"source": "github_readmes\\kyegomez_MambaTransformer_README.md", "filename": "kyegomez_MambaTransformer_README.md", "type": "readme_full"}}
{"id": "kyegomez_NaViT_README.md", "paper_id": "kyegomez_NaViT_README", "text": "[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n\n# NaViT\nMy implementation of \"Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution\"\n\n[Paper Link](https://arxiv.org/pdf/2307.06304.pdf)\n\n# Appreciation\n* Lucidrains\n* Agorians\n\n# Install\n`pip install navit-torch`\n\n# Usage\n```python\nimport torch\nfrom navit.main import NaViT\n\n\nn = NaViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    heads = 16,\n    mlp_dim=2048,\n    dropout=0.1,\n    emb_dropout=0.1,\n    token_dropout_prob=0.1\n)\n\nimages = [\n    [torch.randn(3, 256, 256), torch.randn(3, 128, 128)],\n    [torch.randn(3, 256, 256), torch.randn(3, 256, 128)],\n    [torch.randn(3, 64, 256)]\n]\n\npreds = n(images)\n```\n\n# Dataset Strategy\nHere is a table of the key datasets and their metadata used for pretraining and evaluating NaViT:\n\n| Dataset | Type | Size | Details | Source |  \n|-|-|-|-|-|\n| JFT-4B | Image classification | 4 billion images | Private dataset from Google | [1] |\n| WebLI | Image-text | 73M image-text pairs | Web-crawled dataset | [2] |\n| ImageNet | Image classification | 1.3M images, 1000 classes | Standard benchmark | [3] |\n| ImageNet-A | Image classification | 7,500 images | Out-of-distribution variant | [4] |  \n| ObjectNet | Image classification | 50K images, 313 classes | Out-of-distribution variant | [5] |\n| LVIS | Object detection | 120K images, 1000 classes | Large vocabulary instance segmentation | [6] |\n| ADE20K | Semantic segmentation | 20K images, 150 classes | Scene parsing dataset | [7] |\n| Kinetics-400 | Video classification | 300K videos, 400 classes | Action recognition dataset | [8] |\n| FairFace | Face attribute classification | 108K images, 9 attributes | Balanced dataset for facial analysis | [9] |\n| CelebA | Face attribute classification | 200K images, 40 attributes | Face attributes dataset | [10] |\n\n[1] Zhai et al. \"Scaling Vision Transformers\". 2022. https://arxiv.org/abs/2106.04560  \n[2] Chen et al. \"PaLI\". 2022. https://arxiv.org/abs/2209.06794\n[3] Deng et al. \"ImageNet\". 2009. http://www.image-net.org/\n[4] Hendrycks et al. \"Natural Adversarial Examples\". 2021. https://arxiv.org/abs/1907.07174\n[5] Barbu et al. \"ObjectNet\". 2019. https://arxiv.org/abs/1612.03916\n[6] Gupta et al. \"LVIS\". 2019. https://arxiv.org/abs/1908.03195 \n[7] Zhou et al. \"ADE20K\". 2017. https://arxiv.org/abs/1608.05442\n[8] Kay et al. \"Kinetics\". 2017. https://arxiv.org/abs/1705.06950\n[9] Kärkkäinen and Joo. \"FairFace\". 2019. https://arxiv.org/abs/1908.04913\n[10] Liu et al. \"CelebA\". 2015. https://arxiv.org/abs/1410.5408\n\n# Todo\n- create example trainining script\n\n# License\nMIT\n\n# Citations\n```\n@misc{2307.06304,\nAuthor = {Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey Gritsenko and Mario Lučić and Neil Houlsby},\nTitle = {Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution},\nYear = {2023},\nEprint = {arXiv:2307.06304},\n}\n```\n", "metadata": {"source": "github_readmes\\kyegomez_NaViT_README.md", "filename": "kyegomez_NaViT_README.md", "type": "readme_full"}}
{"id": "leaderj1001_CLIP_README.md", "paper_id": "leaderj1001_CLIP_README", "text": "# CLIP (Contrastive Language–Image Pre-training)\n\n## Experiments (Evaluation)\n  | Model | Dataset | Acc (%) |\n  |:-:|:-:|:-:|\n  | ViT-B/32 (Paper) | CIFAR100 | 65.1 |\n  | ViT-B/32 (Our) | CIFAR100 | 61.71 |\n  | ViT-B/32 (Paper | CIFAR10 | 91.3 |\n  | ViT-B/32 (Our) | CIFAR10 | 88.8 |\n  \n## Overview\n<img width=\"1333\" alt=\"model\" src=\"https://user-images.githubusercontent.com/22078438/104386323-45c66b00-5578-11eb-9261-0c2c067cabc4.png\">\n\n\n## Training\n - Work In Process\n\n## Usage\n - Evaluation\n ```\n python evaluation.py --dataset CIFAR100 --cuda True\n ```\n  - args\n    - dataset (str): CIFAR10, CIFAR100 (default: CIFAR100)\n    - num_workers (int): default: 0\n    - batch_size (int): default: 128\n    - cuda (bool): False\n  - Training\n    - Prepare Data\n      - Visual Genome Dataset [link](http://visualgenome.org)\n      - Download (images, region descriptions)\n    - training\n    ```\n    python main.py --base_dir ./ --cuda True\n    ```\n     \n\n## Reference\n - [paper link](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)\n - Author: Alec Radford, Jong Wook Kim, Chris Hallacy, Girish Sastry, Amanda Askell, Pamela Mishkin, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Jack Clark, Gretchen Krueger, Ilya Sutskever\n - OpenAI\n", "metadata": {"source": "github_readmes\\leaderj1001_CLIP_README.md", "filename": "leaderj1001_CLIP_README.md", "type": "readme_full"}}
{"id": "lucidrains_zorro-pytorch_README.md", "paper_id": "lucidrains_zorro-pytorch_README", "text": "<img src=\"./zorro.png\" width=\"450px\"></img>\n\n## Zorro - Pytorch\n\nImplementation of <a href=\"https://arxiv.org/abs/2301.09595\">Zorro</a>, Masked Multimodal Transformer, in Pytorch. This is a Deepmind work that claims a special masking strategy within a transformer help them achieve SOTA on a few multimodal benchmarks.\n\n## Appreciation\n\n- <a href=\"https://stability.ai/\">Stability.ai</a> for the generous sponsorship to work and open source cutting edge artificial intelligence research\n\n## Install\n\n```bash\n$ pip install zorro-pytorch\n```\n\n## Usage\n\n```python\nimport torch\nfrom zorro_pytorch import Zorro, TokenTypes as T\n\nmodel = Zorro(\n    dim = 512,                        # model dimensions\n    depth = 6,                        # depth\n    dim_head = 64,                    # attention dimension heads\n    heads = 8,                        # attention heads\n    ff_mult = 4,                      # feedforward multiple\n    num_fusion_tokens = 16,           # number of fusion tokens\n    audio_patch_size = 16,            # audio patch size, can also be Tuple[int, int]\n    video_patch_size = 16,            # video patch size, can also be Tuple[int, int]\n    video_temporal_patch_size = 2,    # video temporal patch size\n    video_channels = 3,               # video channels\n    return_token_types = (\n        T.AUDIO,\n        T.AUDIO,\n        T.FUSION,\n        T.GLOBAL,\n        T.VIDEO,\n        T.VIDEO,\n        T.VIDEO,\n    ) # say you want to return 2 tokens for audio, 1 token for fusion, 3 for video - for whatever self-supervised learning, supervised learning, etc etc\n)\n\nvideo = torch.randn(2, 3, 8, 32, 32) # (batch, channels, time, height, width)\naudio = torch.randn(2, 1024 * 10)    # (batch, time)\n\nreturn_tokens = model(audio = audio, video = video) # (2, 6, 512) - all 6 tokes as indicated above is returned\n\n# say you only want 1 audio and 1 video token, for contrastive learning\n\naudio_token, video_token = model(audio = audio, video = video, return_token_indices = (0, 3)).unbind(dim = -2) # (2, 512), (2, 512)\n\n```\n\n## Citations\n\n```bibtex\n@inproceedings{Recasens2023ZorroTM,\n  title  = {Zorro: the masked multimodal transformer},\n  author = {Adri{\\`a} Recasens and Jason Lin and Jo{\\~a}o Carreira and Drew Jaegle and Luyu Wang and Jean-Baptiste Alayrac and Pauline Luc and Antoine Miech and Lucas Smaira and Ross Hemsley and Andrew Zisserman},\n  year   = {2023}\n}\n```\n", "metadata": {"source": "github_readmes\\lucidrains_zorro-pytorch_README.md", "filename": "lucidrains_zorro-pytorch_README.md", "type": "readme_full"}}
{"id": "MarSaKi_VLN-BEVBert_README.md", "paper_id": "MarSaKi_VLN-BEVBert_README", "text": "<div align=\"center\">\n\n<h1>BEVBert: Multimodal Map Pre-training for <br /> Language-guided Navigation</h1>\n\n<div>\n    <a href='https://marsaki.github.io/' target='_blank'>Dong An</a>;\n    <a href='https://sites.google.com/site/yuankiqi/home' target='_blank'>Yuankai Qi</a>;\n    <a href='https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN'>Yangguang Li</a>;\n    <a href='https://yanrockhuang.github.io/' target='_blank'>Yan Huang</a>;\n    <a href='http://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN' target='_blank'>Liang Wang</a>;\n    <a href='https://scholar.google.com/citations?user=W-FGd_UAAAAJ&hl=en' target='_blank'>Tieniu Tan</a>;\n    <a href='https://amandajshao.github.io/' target='_blank'>Jing Shao</a>;\n</div>\n\n<h3><strong>Accepted to <a href='https://iccv2023.thecvf.com/' target='_blank'>ICCV 2023</a></strong></h3>\n\n<h3 align=\"center\">\n  <a href=\"https://arxiv.org/pdf/2212.04385.pdf\" target='_blank'>Paper</a>\n</h3>\n</div>\n\n## Abstract\n\nLarge-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent’s spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effectiveness of the map-based pre-training route for VLN, and the proposed method achieves state-ofthe-art on four VLN benchmarks (R2R, R2R-CE, RxR, REVERIE).\n\n## Method\n\n![](assets/method.png)\n\n## TODOs\n\n* [X] Release VLN (R2R, RxR, REVERIE) code.\n* [X] Release VLN-CE (R2R-CE) code.\n* [X] Data preprocessing code.\n* [X] Release checkpoints and preprocessed datasets.\n\n## Setup\n\n### Installation\n\n1. Create a virtual environment. We develop this project with Python 3.6.\n\n   ```bash\n   conda env create -f environment.yaml\n   ```\n2. Install the latest version of [Matterport3DSimulator](https://github.com/peteanderson80/Matterport3DSimulator), including the Matterport3D RGBD datasets (for step 6).\n3. Download the Matterport3D scene meshes. `download_mp.py` must be obtained from the Matterport3D [project webpage](https://niessner.github.io/Matterport/). `download_mp.py` is also used for downloading RGBD datasets in step 2.\n\n```bash\n# run with python 2.7\npython download_mp.py --task habitat -o data/scene_datasets/mp3d/\n# Extract to: ./data/scene_datasets/mp3d/{scene}/{scene}.glb\n```\n\nFollow the [Habitat Installation Guide](https://github.com/facebookresearch/habitat-lab#installation) to install [`habitat-sim`](https://github.com/facebookresearch/habitat-sim) and [`habitat-lab`](https://github.com/facebookresearch/habitat-lab). We use version [`v0.1.7`](https://github.com/facebookresearch/habitat-lab/releases/tag/v0.1.7) in our experiments. In brief:\n\n4. Install `habitat-sim` for a machine with multiple GPUs or without an attached display (i.e. a cluster):\n\n   ```bash\n   conda install -c aihabitat -c conda-forge habitat-sim=0.1.7 headless\n   ```\n5. Clone `habitat-lab` from the github repository and install. The command below will install the core of Habitat Lab as well as the habitat_baselines.\n\n   ```bash\n   git clone --branch v0.1.7 git@github.com:facebookresearch/habitat-lab.git\n   cd habitat-lab\n   python setup.py develop --all # install habitat and habitat_baselines\n   ```\n6. Grid feature preprocessing for metric mapping (~100G).\n\n   ```bash\n   # for R2R, RxR, REVERIE\n   python precompute_features/grid_mp3d_clip.py\n   python precompute_features/grid_mp3d_imagenet.py\n   python precompute_features/grid_depth.py\n   python precompute_features/grid_sem.py\n\n   # for R2R-CE pre-training\n   python precompute_features/grid_habitat_clip.py\n   python precompute_features/save_habitat_img.py --img_type depth\n   python precompute_features/save_depth_feature.py\n   ```\n7. Download preprocessed instruction datasets and trained weights [[link]](https://drive.google.com/file/d/1jYg_dMlCDZoOtrkmmq40k-_-m6xerdUI/view?usp=sharing). The directory structure has been organized. For R2R-CE experiments, follow [ETPNav](https://github.com/MarSaKi/ETPNav) to configure VLN-CE datasets in `bevbert_ce/data` foler, and put the trained CE weights [[link]](https://drive.google.com/file/d/1-2u1NWmwpX09Rg7uT5mABo-CBTsLthGm/view?usp=sharing) in `bevbert_ce/ckpt`.\n\nGood luck on your VLN journey with BEVBert!\n\n## Running\n\nPre-training. Download precomputed image features [[link]](https://drive.google.com/file/d/1S8jD1Mln0mbTsB5I_i2jdQ8xBbnw-Dyr/view?usp=sharing) into folder `img_features`.\n\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/pt_r2r.bash 2333  # R2R\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/pt_rxr.bash 2333  # RxR\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/pt_rvr.bash 2333  # REVERIE\n\ncd bevbert_ce/pretrain \nCUDA_VISIBLE_DEVICES=0,1,2,3 bash run_pt/run_r2r.bash 2333  # R2R-CE\n```\n\nFine-tuning and Testing, the trained weights can be found in step 7.\n\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/ft_r2r.bash 2333  # R2R\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/ft_rxr.bash 2333  # RxR\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/ft_rvr.bash 2333  # REVERIE\n\ncd bevbert_ce\nCUDA_VISIBLE_DEVICES=0,1,2,3 bash run_r2r/main.bash [train/eval/infer] 2333  # R2R-CE\n```\n\n# Contact Information\n\n* dong DOT an AT cripac DOT ia DOT ac DOT cn, [Dong An](https://marsaki.github.io/)\n* yhuang AT nlpr DOT ia DOT ac DOT cn, [Yan Huang](https://yanrockhuang.github.io/)\n\n# Acknowledge\n\nOur implementations are partially inspired by [DUET](https://github.com/cshizhe/VLN-DUET), [S-MapNet](https://github.com/vincentcartillier/Semantic-MapNet) and [ETPNav](https://github.com/MarSaKi/ETPNav).\n\nThank them for open sourcing their great works!\n\n# Citation\n\nIf you find this repository is useful, please consider citing our paper:\n\n```\n@article{an2023bevbert,\n  title={BEVBert: Multimodal Map Pre-training for Language-guided Navigation},\n  author={An, Dong and Qi, Yuankai and Li, Yangguang and Huang, Yan and Wang, Liang and Tan, Tieniu and Shao, Jing},\n  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  year={2023}\n}\n```\n", "metadata": {"source": "github_readmes\\MarSaKi_VLN-BEVBert_README.md", "filename": "MarSaKi_VLN-BEVBert_README.md", "type": "readme_full"}}
{"id": "mbodiai_embodied-agents_README.md", "paper_id": "mbodiai_embodied-agents_README", "text": "<div align=\"left\">\n <img src=\"assets/logo.png\" width=200;/>\n  <div>&nbsp;</div>\n  <div align=\"left\">\n    <sup>\n      <a href=\"https://api.mbodi.ai\">\n        <i><font size=\"4\">Benchmark, Explore, and Send API Requests Now</font></i>\n      </a>\n    </sup>\n  </div>\n  <div>&nbsp;</div>\n\n[![license](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mbodied)](https://pypi.org/project/mbodied/)\n[![PyPI](https://img.shields.io/pypi/v/mbodied)](https://pypi.org/project/mbodied)\n[![Downloads](https://static.pepy.tech/badge/mbodied)](https://pepy.tech/project/mbodied)\n[![MacOS](https://github.com/mbodiai/opensource/actions/workflows/macos.yml/badge.svg?branch=main)](https://github.com/mbodiai/opensource/actions/workflows/macos.yml)\n[![Ubuntu](https://github.com/mbodiai/opensource/actions/workflows/ubuntu.yml/badge.svg)](https://github.com/mbodiai/opensource/actions/workflows/ubuntu.yml)\n\n📖 **Docs**: [docs](https://api.mbodi.ai/docs)\n\n🚀 **Simple Robot Agent Example:** [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/drive/1KN0JohcjHX42wABBHe-CxXP-NWJjbZts?usp=sharing) </br>\n💻 **Simulation Example with [SimplerEnv](https://github.com/simpler-env/SimplerEnv):** [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/drive/18oiuw1yTxO5x-eT7Z8qNyWtjyYd8cECI?usp=sharing) </br>\n🤖 **Motor Agent using OpenVLA:** [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/drive/1flnMrqyepGOO8J9rE6rehzaLdZPsw6lX?usp=sharing)</br>\n⏺️ **Record Dataset on a Robot:** [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/drive/15UuFbMUJGEjqJ_7I_b5EvKvLCKnAc8bB?usp=sharing)</br>\n\n🫡 **Support, Discussion, and How-To's:** </br>\n[![](https://dcbadge.limes.pink/api/server/BPQ7FEGxNb?theme=discord&?logoColor=pink)](https://discord.gg/BPQ7FEGxNb)\n\n</div>\n\n**Updates:**\n\n**April 18, 2025 — embodied-agents v1.5**\n\n- Updated sensory endpoints\n- Added support for Google Gemini as the language agent backend\n- Enabled tool calling for Language Agent (OpenAI)\n- Added Retrieval-Augmented Generation (RAG) functionalities — see [example](examples/6_robot_with_rag.py)\n\n**Aug 28, 2024, embodied-agents v1.2**\n\n- New [Doc site](https://api.mbodi.ai/docs) is up!\n- Added the features to record dataset on [robot](mbodied/robots/robot.py) natively.\n- Added multiple new Sensory Agents, e.g., [depth estimation](mbodied/agents/sense/depth_estimation_agent.py), [object detection](mbodied/agents/sense/object_detection_agent.py), [image segmentation](mbodied/agents/sense/segmentation_agent.py) with public [API endpoints](https://api.mbodi.ai/sense/) hosted. And a simple cli `mbodied` for trying them.\n- Added [Auto Agent](mbodied/agents/auto/auto_agent.py) for dynamic agents selection.\n\n**June 30, 2024, embodied-agents v1.0**:\n\n- Added Motor Agent supporting OpenVLA with free [API endpoint](https://api.mbodi.ai/community-models) hosted.\n- Added Sensory Agent supporting e.g., 3D object pose detection.\n- Improved automatic dataset recording.\n- Agent now can make remote act calls to API servers e.g., Gradio, vLLM.\n- Bug fixes and performance improvements have been made.\n- PyPI project is renamed to `mbodied`.\n\n# embodied agents\n\n**embodied agents** is a toolkit for integrating large multi-modal models into existing robot stacks with just a few lines of code. It provides consistency, reliability, scalability and is configurable to any observation and action space.\n\n<img src=\"assets/new_demo.gif\" alt=\"Demo GIF\" style=\"width: 550px;\">\n\n- [embodied agents](#embodied-agents)\n  - [Overview](#overview)\n    - [Motivation](#motivation)\n    - [Goals](#goals)\n    - [Limitations](#limitations)\n    - [Scope](#scope)\n    - [Features](#features)\n    - [Endpoints](#endpoints)\n    - [Support Matrix](#support-matrix)\n    - [Roadmap](#roadmap)\n  - [Installation](#installation)\n  - [Getting Started](#getting-started)\n    - [Customize a Motion to fit a robot's action space.](#customize-a-motion-to-fit-a-robots-action-space)\n    - [Run a robotics transformer model on a robot.](#run-a-robotics-transformer-model-on-a-robot)\n    - [Notebooks](#notebooks)\n  - [The Sample Class](#the-sample-class)\n  - [API Reference](#api-reference)\n  - [Directory Structure](#directory-structure)\n  - [Contributing](#contributing)\n\n## Overview\n\nThis repository is broken down into 3 main components: **Agents**, **Data**, and **Hardware**. Inspired by the efficiency of the central nervous system, each component is broken down into 3 meta-modalities: **Language**, **Motion**, and **Sense**. Each agent has an `act` method that can be overridden and satisfies:\n\n- **Language Agents** always return a string.\n- **Motor Agents** always return a `Motion`.\n- **Sensory Agents** always return a `SensorReading`.\n\nFor convenience, we also provide **AutoAgent** which dynamically initializes the right agent for the specified task. See [API Reference](#auto-agent) below for more.\n\nA call to `act` or `async_act` can perform local or remote inference synchronously or asynchronously. Remote execution can be performed with [Gradio](https://www.gradio.app/docs/python-client/introduction), [httpx](https://www.python-httpx.org/), or different LLM clients. Validation is performed with [Pydantic](https://docs.pydantic.dev/latest/).\n\n<img src=\"assets/architecture.jpg\" alt=\"Architecture Diagram\" style=\"width: 700px;\">\n\n- Language Agents natively support OpenAI, Anthropic, Gemini, Ollama, vLLM, Gradio, etc\n- Motor Agents natively support OpenVLA, RT1(upcoming)\n- Sensory Agents support Depth Anything, YOLO, Segment Anything 2\n\nJump to [getting started](#getting-started) to get up and running on [real hardware](https://colab.research.google.com/drive/1KN0JohcjHX42wABBHe-CxXP-NWJjbZts?usp=sharing) or [simulation](https://colab.research.google.com/drive/1gJlfEvsODZWGn3rK8Nx4A0kLnLzJtJG_?usp=sharing). Be sure to join our [Discord](https://discord.gg/BPQ7FEGxNb) for 🥇-winning discussions :)\n\n**⭐ Give us a star on GitHub if you like us!**\n\n### Motivation\n\n<details>\n<summary>There is a significant barrier to entry for running SOTA models in robotics</summary>\n\nIt is currently unrealistic to run state-of-the-art AI models on edge devices for responsive, real-time applications. Furthermore,\nthe complexity of integrating multiple models across different modalities is a significant barrier to entry for many researchers,\nhobbyists, and developers. This library aims to address these challenges by providing a simple, extensible, and efficient way to\nintegrate large models into existing robot stacks.\n\n</details>\n\n### Goals\n\n<details>\n<summary>Facilitate data-collection and sharing among roboticists.</summary>\n\nThis requires reducing much of the complexities involved with setting up inference endpoints, converting between different model formats, and collecting and storing new datasets for future availability.\n\nWe aim to achieve this by:\n\n1. Providing simple, Python-first abstrations that are modular, extensible and applicable to a wide range of tasks.\n2. Providing endpoints, weights, and interactive Gradio playgrounds for easy access to state-of-the-art models.\n3. Ensuring that this library is observation and action-space agnostic, allowing it to be used with any robot stack.\n\nBeyond just improved robustness and consistency, this architecture makes asynchronous and remote agent execution exceedingly simple. In particular we demonstrate how responsive natural language interactions can be achieved in under 30 lines of Python code.\n\n</details>\n\n### Limitations\n\n_Embodied Agents are not yet capable of learning from in-context experience_:\n\n- Frameworks for advanced RAG techniques are clumsy at best for OOD embodied applications however that may improve.\n- Amount of data required for fine-tuning is still prohibitively large and expensive to collect.\n- Online RL is still in its infancy and not yet practical for most applications.\n\n### Scope\n\n- This library is intended to be used for research and prototyping.\n- This library is still experimental and under active development. Breaking changes may occur although they will be avoided as much as possible. Feel free to report issues!\n\n### Features\n\n- Extensible, user-friendly Python SDK with explicit typing and modularity\n- Asynchronous and remote thread-safe agent execution for maximal responsiveness and scalability.\n- Full-compatibility with HuggingFace Spaces, Datasets, Gymnasium Spaces, Ollama, and any OpenAI-compatible API.\n- Automatic dataset-recording and optionally uploads dataset to huggingface hub.\n\n### Endpoints\n\n- [OpenVLA](https://api.mbodi.ai/community-models/)\n- [Sensory Tools](https://api.mbodi.ai/sense/) (Depth Estimation, Image Segmentation, Object Detection)\n\n### Roadmap\n\n- [x] OpenVLA Motor Agent\n- [x] Automatic dataset recording on Robot\n- [x] Yolo, SAM2, DepthAnything Sensory Agents\n- [x] Auto Agent\n- [x] Google Gemini Backend\n- [x] Retrieval-Augmented Generation (RAG)\n- [ ] Pi0 Motor Agent\n- [ ] ROS integration\n- [ ] More Motor Agents, e.g., RT1, Octo\n- [ ] More device support, e.g., OpenCV camera\n- [ ] Fine-tuning Scripts\n\n## Installation\n\n```shell\npip install mbodied\n\n# With extra dependencies, e.g., torch, opencv-python, etc.\npip install mbodied[extras]\n\n# For audio support\npip install mbodied[audio]\n```\n\nOr install from source:\n\n```shell\npip install git+https://github.com/mbodiai/embodied-agents.git\n```\n\n## Getting Started\n\n### Customize a Motion to fit a robot's action space.\n\n```python\nfrom mbodied.types.motion.control import HandControl, FullJointControl\nfrom mbodied.types.motion import AbsoluteMotionField, RelativeMotionField\n\nclass FineGrainedHandControl(HandControl):\n    comment: str = Field(None, description=\"A comment to voice aloud.\")\n    index: FullJointControl = AbsoluteMotionField([0,0,0],bounds=[-3.14, 3.14], shape=(3,))\n    thumb: FullJointControl = RelativeMotionField([0,0,0],bounds=[-3.14, 3.14], shape=(3,))\n```\n\n### Run a robotics transformer model on a robot.\n\n```python\nimport os\nfrom mbodied.agents import LanguageAgent\nfrom mbodied.agents.motion import OpenVlaAgent\nfrom mbodied.agents.sense.audio import AudioAgent\nfrom mbodied.robots import SimRobot\n\ncognition = LanguageAgent(\n  context=\"You are an embodied planner that responds with a python list of strings and nothing else.\",\n  api_key=os.getenv(\"OPENAI_API_KEY\"),\n  model_src=\"openai\",\n  recorder=\"auto\",\n)\naudio = AudioAgent(use_pyaudio=False, api_key=os.getenv(\"OPENAI_API_KEY\")) # pyaudio is buggy on mac\nmotion = OpenVlaAgent(model_src=\"https://api.mbodi.ai/community-models/\")\n\n# Subclass and override do() and capture() methods.\nrobot = SimRobot()\n\ninstruction = audio.listen()\nplan = cognition.act(instruction, robot.capture())\n\nfor step in plan.strip('[]').strip().split(','):\n  print(\"\\nMotor agent is executing step: \", step, \"\\n\")\n  for _ in range(10):\n    hand_control = motion.act(step, robot.capture())\n    robot.do(hand_control)\n```\n\nExample Scripts:\n\n- [1_simple_robot_agent.py](examples/1_simple_robot_agent.py): A very simple language based cognitive agent taking instruction from user and output voice and actions.\n- [2_openvla_motor_agent_example.py](examples/2_openvla_motor_agent_example.py): Run robotic transformers, i.e. OpenVLA, in several lines on the robot.\n- [3_reason_plan_act_robot.py](examples/3_reason_plan_act_robot.py): Full example of language based cognitive agent and OpenVLA motor agent executing task.\n- [4_language_reason_plan_act_robot.py](examples/4_language_reason_plan_act_robot.py): Full example of all languaged based cognitive and motor agent executing task.\n- [5_teach_robot_record_dataset.py](examples/5_teach_robot_record_dataset.py): Example of collecting dataset on robot's action at a specific frequency by just yelling at the robot!\n- [6_robot_with_rag.py](examples/6_robot_with_rag.py): Example with RAG to retrieve skills to run on the robot.\n\n### Notebooks\n\nReal Robot Hardware: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1KN0JohcjHX42wABBHe-CxXP-NWJjbZts?usp=sharing)\n\nSimulation with: [SimplerEnv](https://github.com/simpler-env/SimplerEnv.git) : [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/18oiuw1yTxO5x-eT7Z8qNyWtjyYd8cECI?usp=sharing)\n\nRun OpenVLA with embodied-agents in simulation: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1flnMrqyepGOO8J9rE6rehzaLdZPsw6lX?usp=sharing)\n\nRecord dataset on a robot: [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/drive/15UuFbMUJGEjqJ_7I_b5EvKvLCKnAc8bB?usp=sharing)\n\n## The [Sample](mbodied/base/sample.py) Class\n\nThe Sample class is a base model for serializing, recording, and manipulating arbitrary data. It is designed to be extendable, flexible, and strongly typed. By wrapping your observation or action objects in the [Sample](mbodied/base/sample.py) class, you'll be able to convert to and from the following with ease:\n\n- A Gym space for creating a new Gym environment.\n- A flattened list, array, or tensor for plugging into an ML model.\n- A HuggingFace dataset with semantic search capabilities.\n- A Pydantic BaseModel for reliable and quick json serialization/deserialization.\n\nTo learn more about all of the possibilities with embodied agents, check out the [documentation](https://mbodi-ai-mbodied-agents.readthedocs-hosted.com/en/latest/)\n\n### 💡 Did you know\n\n- You can `pack` a list of `Sample`s or Dicts into a single `Sample` or `Dict` and `unpack` accordingly?\n- You can `unflatten` any python structure into a `Sample` class so long you provide it with a valid json schema?\n\n## API Reference\n\n#### Creating a Sample\n\nCreating a Sample requires just wrapping a python dictionary with the `Sample` class. Additionally, they can be made from kwargs, Gym Spaces, and Tensors to name a few.\n\n```python\nfrom mbodied.types.sample import Sample\n# Creating a Sample instance\nsample = Sample(observation=[1,2,3], action=[4,5,6])\n\n# Flattening the Sample instance\nflat_list = sample.flatten()\nprint(flat_list) # Output: [1, 2, 3, 4, 5, 6]\n\n# Generating a simplified JSON schema\n>>> schema = sample.schema()\n{'type': 'object', 'properties': {'observation': {'type': 'array', 'items': {'type': 'integer'}}, 'action': {'type': 'array', 'items': {'type': 'integer'}}}}\n\n# Unflattening a list into a Sample instance\nSample.unflatten(flat_list, schema)\n>>> Sample(observation=[1, 2, 3], action=[4, 5, 6])\n```\n\n#### Serialization and Deserialization with Pydantic\n\nThe Sample class leverages Pydantic's powerful features for serialization and deserialization, allowing you to easily convert between Sample instances and JSON.\n\n```python\n# Serialize the Sample instance to JSON\nsample = Sample(observation=[1,2,3], action=[4,5,6])\njson_data = sample.model_dump_json()\nprint(json_data) # Output: '{\"observation\": [1, 2, 3], \"action\": [4, 5, 6]}'\n\n# Deserialize the JSON data back into a Sample instance\njson_data = '{\"observation\": [1, 2, 3], \"action\": [4, 5, 6]}'\nsample = Sample.model_validate(from_json(json_data))\nprint(sample) # Output: Sample(observation=[1, 2, 3], action=[4, 5, 6])\n```\n\n#### Converting to Different Containers\n\n```python\n# Converting to a dictionary\nsample_dict = sample.to(\"dict\")\nprint(sample_dict) # Output: {'observation': [1, 2, 3], 'action': [4, 5, 6]}\n\n# Converting to a NumPy array\nsample_np = sample.to(\"np\")\nprint(sample_np) # Output: array([1, 2, 3, 4, 5, 6])\n\n# Converting to a PyTorch tensor\nsample_pt = sample.to(\"pt\")\nprint(sample_pt) # Output: tensor([1, 2, 3, 4, 5, 6])\n```\n\n#### Gym Space Integration\n\n```python\ngym_space = sample.space()\nprint(gym_space)\n# Output: Dict('action': Box(-inf, inf, (3,), float64), 'observation': Box(-inf, inf, (3,), float64))\n```\n\nSee [sample.py](mbodied/base/sample.py) for more details.\n\n### Message\n\nThe [Message](mbodied/types/message.py) class represents a single completion sample space. It can be text, image, a list of text/images, Sample, or other modality. The Message class is designed to handle various types of content and supports different roles such as user, assistant, or system.\n\nYou can create a `Message` in versatile ways. They can all be understood by mbodi's backend.\n\n```python\nfrom mbodied.types.message import Message\n\nMessage(role=\"user\", content=\"example text\")\nMessage(role=\"user\", content=[\"example text\", Image(\"example.jpg\"), Image(\"example2.jpg\")])\nMessage(role=\"user\", content=[Sample(\"Hello\")])\n```\n\n### Backend\n\nThe [Backend](mbodied/base/backend.py) class is an abstract base class for Backend implementations. It provides the basic structure and methods required for interacting with different backend services, such as API calls for generating completions based on given messages. See [backend directory](mbodied/agents/backends) on how various backends are implemented.\n\n### Agent\n\n[Agent](mbodied/base/agent.py) is the base class for various agents listed below. It provides a template for creating agents that can talk to a remote backend/server and optionally record their actions and observations.\n\n### Language Agent\n\nThe [Language Agent](mbodied/agents/language/language_agent.py) can connect to different backends or transformers of your choice. It includes methods for recording conversations, managing context, looking up messages, forgetting messages, storing context, and acting based on an instruction and an image.\n\nNatively supports API services: OpenAI, Anthropic, Gemini vLLM, Ollama, HTTPX, or any gradio endpoints. More upcoming!\n\nTo use OpenAI for your robot backend:\n\n```python\nfrom mbodied.agents.language import LanguageAgent\n\nagent = LanguageAgent(context=\"You are a robot agent.\", model_src=\"openai\")\n```\n\nTo execute an instruction:\n\n```python\ninstruction = \"pick up the fork\"\nresponse = robot_agent.act(instruction, image)\n```\n\nLanguage Agent can connect to vLLM as well. For example, suppose you are running a vLLM server Mistral-7B on 1.2.3.4:1234. All you need to do is:\n\n```python\nagent = LanguageAgent(\n    context=context,\n    model_src=\"openai\",\n    model_kwargs={\"api_key\": \"EMPTY\", \"base_url\": \"http://1.2.3.4:1234/v1\"},\n)\nresponse = agent.act(\"Hello, how are you?\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\n```\n\nExample using Ollama:\n\n```python\nagent = LanguageAgent(\n    context=\"You are a robot agent.\", model_src=\"ollama\",\n    model_kwargs={\"endpoint\": \"http://localhost:11434/api/chat\"}\n)\nresponse = agent.act(\"Hello, how are you?\", model=\"llama3.1\")\n```\n\n### Motor Agent\n\n[Motor Agent](mbodied/agents/motion/motor_agent.py) is similar to Language Agent but instead of returning a string, it always returns a `Motion`. Motor Agent is generally powered by robotic transformer models, e.g., OpenVLA, RT1, Octo, etc.\nSome small models, like RT1, can run on edge devices. However, some, like OpenVLA, may be challenging to run without quantization. See [OpenVLA Agent](mbodied/agents/motion/openvla_agent.py) and an [example OpenVLA server](examples/servers/gradio_example_openvla.py)\n\n### Sensory Agent\n\nThese agents interact with the environment to collect sensor data. They always return a `SensorReading`, which can be various forms of processed sensory input such as images, depth data, or audio signals.\n\nCurrently, we have:\n\n- [depth estimation](mbodied/agents/sense/depth_estimation_agent.py)\n- [object detection](mbodied/agents/sense/object_detection_agent.py)\n- [image segmentation](mbodied/agents/sense/segmentation_agent.py)\n\nagents that process robot's sensor information.\n\n### Auto Agent\n\n[Auto Agent](mbodied/agents/auto/auto_agent.py) dynamically selects and initializes the correct agent based on the task and model.\n\n```python\nfrom mbodied.agents.auto.auto_agent import AutoAgent\n\n# This makes it a LanguageAgent\nagent = AutoAgent(task=\"language\", model_src=\"openai\")\nresponse = agent.act(\"What is the capital of France?\")\n\n# This makes it a motor agent: OpenVlaAgent\nauto_agent = AutoAgent(task=\"motion-openvla\", model_src=\"https://api.mbodi.ai/community-models/\")\naction = auto_agent.act(\"move hand forward\", Image(size=(224, 224)))\n\n# This makes it a sensory agent: DepthEstimationAgent\nauto_agent = AutoAgent(task=\"sense-depth-estimation\", model_src=\"https://api.mbodi.ai/sense/\")\ndepth = auto_agent.act(image=Image(size=(224, 224)))\n```\n\nAlternatively, you can use `get_agent` method in [auto_agent](mbodied/agents/auto/auto_agent.py) as well.\n\n```python\nlanguage_agent = get_agent(task=\"language\", model_src=\"openai\")\n```\n\n### Motions\n\nThe [motion_controls](mbodied/types/motion_controls.py) module defines various motions to control a robot as Pydantic models. They are also subclassed from `Sample`, thus possessing all the capability of `Sample` as mentioned above. These controls cover a range of actions, from simple joint movements to complex poses and full robot control.\n\n### Robot\n\nYou can integrate your custom robot hardware by subclassing [Robot](mbodied/robot/robot.py) quite easily. You only need to implement `do()` function to perform actions (and some additional methods if you want to record dataset on the robot). In our examples, we use a [mock robot](mbodied/robot/sim_robot.py). We also have an [XArm robot](mbodied/robot/xarm_robot.py) as an example.\n\n#### Recording a Dataset\n\nRecording a dataset on a robot is very easy! All you need to do is implement the `get_observation()`, `get_state()`, and `prepare_action()` methods for your robot. After that, you can record a dataset on your robot anytime you want. See [examples/5_teach_robot_record_dataset.py](examples/5_teach_robot_record_dataset.py) and this colab: [<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/drive/15UuFbMUJGEjqJ_7I_b5EvKvLCKnAc8bB?usp=sharing) for more details.\n\n```python\nfrom mbodied.robots import SimRobot\nfrom mbodied.types.motion.control import HandControl, Pose\n\nrobot = SimRobot()\nrobot.init_recorder(frequency_hz=5)\nwith robot.record(\"pick up the fork\"):\n  motion = HandControl(pose=Pose(x=0.1, y=0.2, z=0.3, roll=0.1, pitch=0.2, yaw=0.3))\n  robot.do(motion)\n```\n\n### Recorder\n\nDataset [Recorder](mbodied/data/recording.py) is a lower level recorder to record your conversation and the robot's actions to a dataset as you interact with/teach the robot. You can define any observation space and action space for the Recorder. See [gymnasium](https://github.com/Farama-Foundation/Gymnasium) for more details about spaces.\n\n```python\nfrom mbodied.data.recording import Recorder\nfrom mbodied.types.motion.control import HandControl\nfrom mbodied.types.sense.vision import Image\nfrom gymnasium import spaces\n\nobservation_space = spaces.Dict({\n    'image': Image(size=(224, 224)).space(),\n    'instruction': spaces.Text(1000)\n})\naction_space = HandControl().space()\nrecorder = Recorder('example_recorder', out_dir='saved_datasets', observation_space=observation_space, action_space=action_space)\n\n# Every time robot makes a conversation or performs an action:\nrecorder.record(observation={'image': image, 'instruction': instruction,}, action=hand_control)\n```\n\nThe dataset is saved to `./saved_datasets`.\n\n### Replayer\n\nThe [Replayer](mbodied/data/replaying.py) class is designed to process and manage data stored in HDF5 files generated by `Recorder`. It provides a variety of functionalities, including reading samples, generating statistics, extracting unique items, and converting datasets for use with HuggingFace. The Replayer also supports saving specific images during processing and offers a command-line interface for various operations.\n\nExample for iterating through a dataset from Recorder with Replayer:\n\n```python\nfrom mbodied.data.replaying import Replayer\n\nreplayer = Replayer(path=str(\"path/to/dataset.h5\"))\nfor observation, action in replayer:\n   ...\n```\n\n## Directory Structure\n\n```shell\n├─ assets/ ............. Images, icons, and other static assets\n├─ examples/ ........... Example scripts and usage demonstrations\n├─ resources/ .......... Additional resources for examples\n├─ src/\n│  └─ mbodied/\n│     ├─ agents/ ....... Modules for robot agents\n│     │  ├─ backends/ .. Backend implementations for different services for agents\n│     │  ├─ language/ .. Language based agents modules\n│     │  ├─ motion/ .... Motion based agents modules\n│     │  └─ sense/ ..... Sensory, e.g. audio, processing modules\n│     ├─ data/ ......... Data handling and processing\n│     ├─ hardware/ ..... Hardware modules, i.e. camera\n│     ├─ robot/ ........ Robot interface and interaction\n│     └─ types/ ........ Common types and definitions\n└─ tests/ .............. Unit tests\n```\n\n## Contributing\n\nWe welcome issues, questions and PRs. See the [contributing guide](CONTRIBUTING.md) for more information.\n", "metadata": {"source": "github_readmes\\mbodiai_embodied-agents_README.md", "filename": "mbodiai_embodied-agents_README.md", "type": "readme_full"}}
{"id": "MCG-NJU_Dynamic-MDETR_README.md", "paper_id": "MCG-NJU_Dynamic-MDETR_README", "text": "###  Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding (T-PAMI 2024)\n\n[Fengyuan Shi](https://shifengyuan1999.github.io/), \n[Ruopeng Gao](https://ruopenggao.com/),\n[Weilin Huang](https://www.whuang.org/),\n[Limin Wang](https://wanglimin.github.io/)\n<br/>\n\n[![arXiv](https://img.shields.io/badge/ArXiv-2209.13959-red)](https://arxiv.org/abs/2209.13959)\n\n<p align=\"center\">\n<img src=\"./assets/framework.png\" width=\"1080px\"/>\n</p>\n\n\n\n## Requirements\n\n```shell\nconda create -n dynamic-mdetr python=3.10\nconda activate dynamic-mdetr\nbash install.txt\n```\n\n## Getting Started\nPlease refer to [GETTING_STARGTED.md](docs/GETTING_STARTED.md) to learn how to prepare the datasets and pretrained checkpoints.\n\n## Training\n```shell\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n# refcocog-g\npython -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --model_type ResNet --batch_size 16 --lr_bert 0.00001 --aug_crop --aug_scale --aug_translate --backbone resnet50 --detr_model ./checkpoints/detr-r50-gref.pth --bert_enc_num 12 --detr_enc_num 6 --dataset gref --max_query_len 40 --output_dir outputs/refcocog_gsplit_r50 --stages 3 --vl_fusion_enc_layers 3 --uniform_learnable True --in_points 36 --lr 1e-4 --different_transformer True --lr_drop 60 --vl_dec_layers 1 --vl_enc_layers 1 --clip_max_norm 1.0\n# refcocog-u\npython -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --model_type ResNet --batch_size 16 --lr_bert 0.00001 --aug_crop --aug_scale --aug_translate --backbone resnet50 --detr_model ./checkpoints/detr-r50-gref.pth --bert_enc_num 12 --detr_enc_num 6 --dataset gref_umd --max_query_len 40 --output_dir outputs/refcocog_usplit_r50 --stages 3 --vl_fusion_enc_layers 3 --uniform_learnable True --in_points 36 --lr 1e-4 --different_transformer True --lr_drop 60 --vl_dec_layers 1 --vl_enc_layers 1 --clip_max_norm 1.0\n# refcoco\npython -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --model_type ResNet --batch_size 16 --lr_bert 0.00001 --aug_crop --aug_scale --aug_translate --backbone resnet50 --detr_model ./checkpoints/detr-r50-unc.pth --bert_enc_num 12 --detr_enc_num 6 --dataset unc --max_query_len 20 --output_dir outputs/refcoco_r50 --stages 3 --vl_fusion_enc_layers 3 --uniform_learnable True --in_points 36 --lr 1e-4 --different_transformer True --lr_drop 60 --vl_dec_layers 1 --vl_enc_layers 1 --clip_max_norm 1.0\n# refcoco plus\npython -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --model_type ResNet --batch_size 16 --lr_bert 0.00001 --aug_crop --aug_scale --aug_translate --backbone resnet50 --detr_model ./checkpoints/detr-r50-unc.pth --bert_enc_num 12 --detr_enc_num 6 --dataset unc+ --max_query_len 20 --output_dir outputs/refcoco_plus_r50 --stages 3 --vl_fusion_enc_layers 3 --uniform_learnable True --in_points 36 --lr 1e-4 --different_transformer True --lr_drop 60 --vl_dec_layers 1 --vl_enc_layers 1 --clip_max_norm 1.0\n# referit\npython -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --model_type ResNet --batch_size 16 --lr_bert 0.00001 --aug_crop --aug_scale --aug_translate --backbone resnet50 --detr_model ./checkpoints/detr-r50-referit.pth --bert_enc_num 12 --detr_enc_num 6 --dataset referit --max_query_len 20 --output_dir outputs/referit_r50 --stages 3 --vl_fusion_enc_layers 3 --uniform_learnable True --in_points 36 --lr 1e-4 --different_transformer True --lr_drop 60 --vl_dec_layers 1 --vl_enc_layers 1 --clip_max_norm 1.0 --weight_decay 0.0 --vl_dropout 0.0 --dropout 0.0\n# flickr\npython -m torch.distributed.launch --nproc_per_node=8 --use_env train.py --model_type ResNet --batch_size 16 --lr_bert 0.00001 --aug_crop --aug_scale --aug_translate --backbone resnet50 --detr_model ./checkpoints/detr-r50-referit.pth --bert_enc_num 12 --detr_enc_num 6 --dataset flickr --max_query_len 20 --output_dir outputs/flickr_r50 --stages 3 --vl_fusion_enc_layers 3 --uniform_learnable True --in_points 36 --lr 1e-4 --different_transformer True --epochs 60 --lr_drop 40  --vl_dec_layers 1 --vl_enc_layers 1 --clip_max_norm 1.0\n```\n\n## Inference\n[Checkpoints](https://drive.google.com/drive/folders/1stGPq4Sz_Vu60QliUzey8m6iYXrrF3Ua?usp=drive_link)\n```shell\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n# refcocog-g\npython -m torch.distributed.launch --nproc_per_node=8 --use_env eval.py --model_type ResNet --batch_size 16 --backbone resnet50 --bert_enc_num 12 --detr_enc_num 6 --dataset gref --max_query_len 40 --output_dir outputs/refcocog_gsplit_r50 --stages 3 --vl_fusion_enc_layers 3 --uniform_learnable True --in_points 36 --lr 1e-4 --different_transformer True --lr_drop 60 --vl_dec_layers 1 --vl_enc_layers 1 --eval_model outputs/refcocog_gsplit_r50/best_checkpoint.pth --eval_set val\n```\n\n## Citation\nIf you make use of our work, please cite our paper.\n```bibtex\n@article{shi2024dynamic,\n  title={Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding},\n  author={Shi, Fengyuan and Gao, Ruopeng and Huang, Weilin and Wang, Limin},\n  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n  volume={46},\n  number={2},\n  pages={1181--1198},\n  year={2024},\n  publisher={IEEE}\n}\n```\n\n## Acknowledgments\nThis project is built upon [TransVG](https://github.com/djiajunustc/TransVG). Thanks for their wonderful work!", "metadata": {"source": "github_readmes\\MCG-NJU_Dynamic-MDETR_README.md", "filename": "MCG-NJU_Dynamic-MDETR_README.md", "type": "readme_full"}}
{"id": "microsoft_torchscale_README.md", "paper_id": "microsoft_torchscale_README", "text": "# TorchScale - A Library of Foundation Architectures\n\n<p>\n  <a href=\"https://github.com/microsoft/torchscale/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n  <a href=\"https://pypi.org/project/torchscale\"><img alt=\"MIT License\" src=\"https://badge.fury.io/py/torchscale.svg\" /></a>\n</p>\n\nTorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively.\n\nFundamental research to develop new architectures for foundation models and A(G)I, focusing on modeling generality and capability, as well as training stability and efficiency.\n- Stability - [**DeepNet**](https://arxiv.org/abs/2203.00555): scaling Transformers to 1,000 Layers and beyond\n- Generality - [**Foundation Transformers (Magneto)**](https://arxiv.org/abs/2210.06423): towards true general-purpose modeling across tasks and modalities (including language, vision, speech, and multimodal)\n- Capability - A [**Length-Extrapolatable**](https://arxiv.org/abs/2212.10554) Transformer\n- Efficiency - [**X-MoE**](https://arxiv.org/abs/2204.09179): scalable & finetunable sparse Mixture-of-Experts (MoE)\n\n### The Revolution of Model Architecture\n- [**BitNet**](https://arxiv.org/abs/2310.11453): 1-bit Transformers for Large Language Models\n- [**RetNet**](https://arxiv.org/abs/2307.08621): Retentive Network: A Successor to Transformer for Large Language Models\n- [**LongNet**](https://arxiv.org/abs/2307.02486): Scaling Transformers to 1,000,000,000 Tokens\n\n## News\n\n- December, 2023: [LongNet](torchscale/model/LongNet.py) and [LongViT](examples/longvit/README.md) released\n- October, 2023: Update RMSNorm and SwiGLU as the default module in RetNet\n- November, 2022: TorchScale 0.1.1 released [[Paper](https://arxiv.org/abs/2211.13184)] [[PyPI](https://pypi.org/project/torchscale/)]\n\n## Installation\n\nTo install:\n```\npip install torchscale\n```\n\nAlternatively, you can develop it locally:\n```\ngit clone https://github.com/microsoft/torchscale.git\ncd torchscale\npip install -e .\n```\n\nFor faster training install [Flash Attention](https://github.com/Dao-AILab/flash-attention) for Turing, Ampere, Ada, or Hopper GPUs:\n```\npip install flash-attn\n```\nor [xFormers](https://github.com/facebookresearch/xformers) for Volta, Turing, Ampere, Ada, or Hopper GPUs:\n```\n# cuda 11.8 version\npip3 install -U xformers --index-url https://download.pytorch.org/whl/cu118\n# cuda 12.1 version\npip3 install -U xformers --index-url https://download.pytorch.org/whl/cu121\n```\n\n## Getting Started\n\nIt takes only several lines of code to create a model with the above fundamental research features enabled. Here is how to quickly obtain a BERT-like encoder:\n\n```python\n>>> from torchscale.architecture.config import EncoderConfig\n>>> from torchscale.architecture.encoder import Encoder\n\n>>> config = EncoderConfig(vocab_size=64000)\n>>> model = Encoder(config)\n\n>>> print(model)\n```\n\nWe also support the `Decoder` architecture and the `EncoderDecoder` architecture:\n\n```python\n# Creating a decoder model\n>>> from torchscale.architecture.config import DecoderConfig\n>>> from torchscale.architecture.decoder import Decoder\n\n>>> config = DecoderConfig(vocab_size=64000)\n>>> decoder = Decoder(config)\n>>> print(decoder)\n\n# Creating a encoder-decoder model\n>>> from torchscale.architecture.config import EncoderDecoderConfig\n>>> from torchscale.architecture.encoder_decoder import EncoderDecoder\n\n>>> config = EncoderDecoderConfig(vocab_size=64000)\n>>> encdec = EncoderDecoder(config)\n>>> print(encdec)\n```\n\nIt takes only several lines of code to create a RetNet model:\n\n```python\n# Creating a RetNet model\n>>> import torch\n>>> from torchscale.architecture.config import RetNetConfig\n>>> from torchscale.architecture.retnet import RetNetDecoder\n\n>>> config = RetNetConfig(vocab_size=64000)\n>>> retnet = RetNetDecoder(config)\n\n>>> print(retnet)\n```\n\nFor LongNet models ([Flash Attention](https://github.com/Dao-AILab/flash-attention) required):\n```python\n>>> import torch\n>>> from torchscale.architecture.config import EncoderConfig, DecoderConfig\n>>> from torchscale.model.longnet import LongNetEncoder, LongNetDecoder\n\n# Creating a LongNet encoder with the dilated pattern of segment_length=[2048,4096] and dilated_ratio=[1,2]\n>>> config = EncoderConfig(vocab_size=64000, segment_length='[2048,4096]', dilated_ratio='[1,2]', flash_attention=True)\n>>> longnet = LongNetEncoder(config)\n\n# Creating a LongNet decoder with the dilated pattern of segment_length=[2048,4096] and dilated_ratio=[1,2]\n>>> config = DecoderConfig(vocab_size=64000, segment_length='[2048,4096]', dilated_ratio='[1,2]', flash_attention=True)\n>>> longnet = LongNetDecoder(config)\n```\n\n## Key Features\n\n- [DeepNorm to improve the training stability of Post-LayerNorm Transformers](https://arxiv.org/abs/2203.00555)\n  * enabled by setting *deepnorm=True* in the `Config` class. \n  * It adjusts both the residual connection and the initialization method according to the model architecture (i.e., encoder, decoder, or encoder-decoder).\n\n- [SubLN for the model generality and the training stability](https://arxiv.org/abs/2210.06423)\n  * enabled by *subln=True*. This is enabled by default. \n  * It introduces another LayerNorm to each sublayer and adjusts the initialization according to the model architecture.\n  * Note that SubLN and DeepNorm cannot be used in one single model.\n\n- [X-MoE: efficient and finetunable sparse MoE modeling](https://arxiv.org/abs/2204.09179)\n  * enabled by *use_xmoe=True*. \n  * It replaces every *'moe_freq'* `FeedForwardNetwork` layers with the X-MoE layers.\n\n- [Multiway architecture for multimodality](https://arxiv.org/abs/2208.10442)\n  * enabled by *multiway=True*.\n  * It provides a pool of Transformer's parameters used for different modalities.\n\n- [Extrapolatable position embedding (Xpos)](https://arxiv.org/abs/2212.10554)\n  * enabled by *xpos_rel_pos=True*.\n\n- [Relative position bias](https://arxiv.org/abs/1910.10683)\n  * enabled by adjusting *rel_pos_buckets* and *max_rel_pos*.\n\n- [SparseClip: improving the gradient clipping for sparse MoE models](https://arxiv.org/abs/2211.13184)\n  * we provide a [sample code](examples/fairseq/utils/sparse_clip.py) that can be easily adapted to the FairSeq (or other) repo.\n\n- [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621)\n  * created by `config = RetNetConfig(vocab_size=64000)` and `retnet = RetNetDecoder(config)`.\n\n- [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)\n  \nMost of the features above can be used by simply passing the corresponding parameters to the config. For example:\n\n```python\n>>> from torchscale.architecture.config import EncoderConfig\n>>> from torchscale.architecture.encoder import Encoder\n\n>>> config = EncoderConfig(vocab_size=64000, deepnorm=True, multiway=True)\n>>> model = Encoder(config)\n\n>>> print(model)\n```\n\n## Examples\n\nWe have examples of how to use TorchScale in the following scenarios/tasks:\n\n- Language\n\n  * [Decoder/GPT](examples/fairseq/README.md#example-gpt-pretraining)\n\n  * [Encoder-Decoder/Neural Machine Translation](examples/fairseq/README.md#example-machine-translation)\n\n  * [Encoder/BERT](examples/fairseq/README.md#example-bert-pretraining)\n\n- Vision\n\n  * [LongViT](examples/longvit/README.md)\n\n  * ViT/BEiT [In progress]\n\n- Speech\n\n- Multimodal\n\n  * [Multiway Transformers/BEiT-3](https://github.com/microsoft/unilm/tree/master/beit3)\n\nWe plan to provide more examples regarding different tasks (e.g. vision pretraining and speech recognition) and various deep learning toolkits (e.g. [DeepSpeed](https://github.com/microsoft/DeepSpeed) and [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)). Any comments or PRs are welcome!\n\n\n## Acknowledgments\n\nSome implementations in TorchScale are either adapted from or inspired by the [FairSeq](https://github.com/facebookresearch/fairseq) repository and the [UniLM](https://github.com/microsoft/unilm) repository.\n\n## Citations\n\nIf you find this repository useful, please consider citing our work:\n\n```\n@article{torchscale,\n  author    = {Shuming Ma and Hongyu Wang and Shaohan Huang and Wenhui Wang and Zewen Chi and Li Dong and Alon Benhaim and Barun Patra and Vishrav Chaudhary and Xia Song and Furu Wei},\n  title     = {{TorchScale}: {Transformers} at Scale},\n  journal   = {CoRR},\n  volume    = {abs/2211.13184},\n  year      = {2022}\n}\n```\n\n```\n@article{deepnet,\n  author    = {Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Dongdong Zhang and Furu Wei},\n  title     = {{DeepNet}: Scaling {Transformers} to 1,000 Layers},\n  journal   = {CoRR},\n  volume    = {abs/2203.00555},\n  year      = {2022},\n}\n```\n\n```\n@article{magneto,\n  author    = {Hongyu Wang and Shuming Ma and Shaohan Huang and Li Dong and Wenhui Wang and Zhiliang Peng and Yu Wu and Payal Bajaj and Saksham Singhal and Alon Benhaim and Barun Patra and Zhun Liu and Vishrav Chaudhary and Xia Song and Furu Wei},\n  title     = {Foundation {Transformers}},\n  journal   = {CoRR},\n  volume    = {abs/2210.06423},\n  year      = {2022}\n}\n```\n\n```\n@inproceedings{xmoe,\n  title={On the Representation Collapse of Sparse Mixture of Experts},\n  author={Zewen Chi and Li Dong and Shaohan Huang and Damai Dai and Shuming Ma and Barun Patra and Saksham Singhal and Payal Bajaj and Xia Song and Xian-Ling Mao and Heyan Huang and Furu Wei},\n  booktitle={Advances in Neural Information Processing Systems},\n  year={2022},\n  url={https://openreview.net/forum?id=mWaYC6CZf5}\n}\n```\n\n```\n@article{retnet,\n  author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},\n  title     = {Retentive Network: A Successor to {Transformer} for Large Language Models},\n  journal   = {ArXiv},\n  volume    = {abs/2307.08621},\n  year      = {2023}\n}\n```\n\n```\n@article{longnet,\n  author={Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Nanning Zheng and Furu Wei},\n  title     = {{LongNet}: Scaling Transformers to 1,000,000,000 Tokens},\n  journal   = {ArXiv},\n  volume    = {abs/2307.02486},\n  year      = {2023}\n}\n```\n\n```\n@article{longvit,\n  title     = {When an Image is Worth 1,024 x 1,024 Words: A Case Study in Computational Pathology},\n  author    = {Wenhui Wang and Shuming Ma and Hanwen Xu and Naoto Usuyama and Jiayu Ding and Hoifung Poon and Furu Wei},\n  journal   = {ArXiv},\n  volume    = {abs/2312.03558},\n  year      = {2023}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [Furu Wei](mailto:fuwei@microsoft.com) and [Shuming Ma](mailto:shumma@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos is subject to those third-party's policies.\n", "metadata": {"source": "github_readmes\\microsoft_torchscale_README.md", "filename": "microsoft_torchscale_README.md", "type": "readme_full"}}
{"id": "MILVLG_mt-captioning_README.md", "paper_id": "MILVLG_mt-captioning_README", "text": "# mt-captioning\n\nThis repository corresponds to the PyTorch implementation of the paper *Multimodal Transformer with Multi-View Visual Representation for Image Captioning*. By using the bottom-up-attention visual features (with slight improvement), our single-view Multimodal Transformer model (MT_sv) delivers 130.9 CIDEr on the Kapathy's test split of MSCOCO dataset. Please check our [paper](https://arxiv.org/abs/1905.07841v1) for details.\n\n## Table of Contents\n\n0. [Prerequisites](#Prerequisites)\n1. [Training](#Training)\n2. [Testing](#Testing)\n\n## Prerequisites\n\n#### Requirements\n\n- [Python 3](https://www.python.org/downloads/)\n- [PyTorch](http://pytorch.org/) >= 1.4.0\n- [Cuda](https://developer.nvidia.com/cuda-toolkit) >= 9.2 and [cuDNN](https://developer.nvidia.com/cudnn)\n\nThe annotation files can be downloaded [here](https://awma1-my.sharepoint.com/:u:/g/personal/yuz_l0_tn/ES91VBvL885MvEVSXeozrXEBRdeQcvj0OplbE2ujooMylQ?e=mRSClL) and unzipped to the datasets folder.\n\nThe visual features are extracted by our [bottom-up-attention.pytorch](https://github.com/MILVLG/bottom-up-attention.pytorch) repo using the following scripts:\n\n```bash\n# 1.extract the bbox from the image\n$ python3 extract_features.py --mode caffe \\\n          --config-file configs/bua-caffe/extract-bua-caffe-r101-bbox-only.yaml \\\n          --image-dir <image_dir> --out-dir <bbox_dir> --resume\n\n# 2. extract the roi feature by bbox\n$ python3 extract_features.py --mode caffe \\\n         --config-file configs/bua-caffe/extract-bua-caffe-r101-gt-bbox.yaml \\\n         --image-dir <image_dir> --gt-bbox-dir <bbox_dir> --out-dir <output_dir> --resume\n```\nWe provided a pre-extracted features in the `datasets/mscoco/features/val2014` folder for the image in `datasets/mscoco/image` to help validating the correctness of the extracted features.\n\nWe use the ResNet-101 as our backbone and extract features for the MSCOCO dataset to the `datasets/mscoco/features/frcn-r101` folder.\n\nFinally, the `datasets` folder will have the following structure:\n\n```angular2html\n|-- datasets\n   |-- mscoco\n   |  |-- features\n   |  |  |-- frcn-r101\n   |  |  |  |-- train2014\n   |  |  |  |  |-- COCO_train2014_....jpg.npz\n   |  |  |  |-- val2014\n   |  |  |  |  |-- COCO_val2014_....jpg.npz\n   |  |  |  |-- test2015\n   |  |  |  |  |-- COCO_test2015_....jpg.npz\n   |  |-- annotations\n   |  |  |-- coco-train-idxs.p\n   |  |  |-- coco-train-words.p\n   |  |  |-- cocotalk_label.h5\n   |  |  |-- cocotalk.json\n   |  |  |-- vocab.json\n   |  |  |-- glove_embeding.npy\n```\n\n## Training\n\nThe following script will train a model with cross-entropy loss :\n\n```bash\n$ python train.py --caption_model svbase --ckpt_path <checkpoint_dir> --gpu_id 0\n```\n\n1. `caption_model` refers to the model while been trained, such as `svbase` and `umv`\n\n2. `ckpt_path` refers to the dir to save checkpoint.\n\n3. `gpu_id` refers to the gpu id.\n\nBased on the model trained with cross-entropy loss, the following script will load the pre-trained model and then fine-tune the model with self-critical loss:\n\n```bash\n$ python train.py --caption_model svbase --learning_rate 1e-5 --ckpt_path <checkpoint_dir> --start_from <checkpoint_dir_rl> --gpu_id 0 --max_epochs 25\n```\n\n1. `caption_model` refers to the model while been trained.\n\n2. `learning_rate` refers to the learning rate use in self-critical.\n\n3. `ckpt_path` refers to the dir to save checkpoint.\n\n4. `gpu_id` refers to the gpu id.\n\n## Testing\n\nGiven the trained model, the following script will report the performance on the `val` split of MSCOCO:\n\n```bash\n$ python test.py --ckpt_path <checkpoint_dir> --gpu_id 0\n```\n\n1. `ckpt_path` refers to the dir to save checkpoint.\n\n2. `gpu_id` refers to the gpu id.\n\n## Pre-trained models\n\nWe provided the pre-trained model for the single-view MT model at present. More models will be added in the future.\n\nModel |  Backbone  | BLEU@1 | METEOR | CIDEr |Download\n:-:|:-:|:-:|:-:|:-:|:-:\nMT_sv|ResNet-101|80.8|29.1|130.9|[model](https://awma1-my.sharepoint.com/:u:/g/personal/yuz_l0_tn/EUakyWWLZ7dGkoO_bljASwABpKPNgKARbuiAyQvaA6dDYg?e=mQYtBy)\n\n## Citation\n\nIf this repository is helpful for your research, we'd really appreciate it if you could cite the following paper:\n```\n@article{yu2019multimodal,\n  title={Multimodal transformer with multi-view visual representation for image captioning},\n  author={Yu, Jun and Li, Jing and Yu, Zhou and Huang, Qingming},\n  journal={IEEE Transactions on Circuits and Systems for Video Technology},\n  year={2019},\n  publisher={IEEE}\n}\n```\n\n## Acknowledgement\nWe thank Ruotian Luo for his [self-critical.pytorch](https://github.com/ruotianluo/self-critical.pytorch), [cider](https://github.com/ruotianluo/cider/tree/e9b736d038d39395fa2259e39342bb876f1cc877) and [coco-caption](https://github.com/ruotianluo/coco-caption/tree/ea20010419a955fed9882f9dcc53f2dc1ac65092) repos.\n", "metadata": {"source": "github_readmes\\MILVLG_mt-captioning_README.md", "filename": "MILVLG_mt-captioning_README.md", "type": "readme_full"}}
{"id": "MuiscNN_Lamucal_README.md", "paper_id": "MuiscNN_Lamucal_README", "text": "# Lamucal\n\nGet chords, beats, lyrics, melody, and tabs for any song.    \n\nA transformer-based hybrid multimodal model, various transformer models address different problems in the field of music information retrieval, these models generate corresponding information dependencies that mutually influence each other.\n\n> The online experience, [See the site here](https://lamucal.com)  \n\n<img src='./image/model.png'  style=\"width: 950px;\" >   \n\n`U-Net` network model for audio source separation, `Pitch-Net`, `Beat-Net`, `Chord-Net` and `Segment-Net` based on the transformer model. Apart from establishing the correlation between the frequency and time, the most important aspect is to establish the mutual influence between different networks.   \n\nThe entire AI-powered process is implemented in `aitabs.py`, while the various network structure models can be referenced in the `models` folder.   \n> **Note**: `U-Net` and `Segment-Net` use the stft spectrum of audio as input. `Beat-Net` uses three spectrograms of drums, bass, and other instruments as input,`Chord-Net` uses one spectrogram of the background music.\n\n\n## Features\n- **Chord**, music chord detection, including major, minor, 7, maj7, min7, 6, m6, sus2, sus4, 5, and inverted chords. Determining the **key** of a song.       \n\n- **Beat**, music beat, downbeat detection and **tempo** (BPM) tracking   \n\n- **Pitch**, tracking the pitch of the melody in the vocal track.  \n\n- **Music Structure**, music segment boundaries and labels, include intro, verse, chorus, bridge and etc.    \n\n- **Lyrics**, music lyrics recognition and automatic lyrics to audio alignment, use ASR (whisper) to recognize the lyrics of the vocal track. The alignment of lyrics and audio is achieved through fine-tuning the wav2vec2 pre-training model. Currently, it supports dozens of languages, including English, Spanish, Portuguese, Russian, Japanese, Korean, Arabic, Chinese, and more.   \n\n- **AI Tabs**, Generate playable sheet music, including chord charts and six-line staves, using chords, beats, music structure information, lyrics, rhythm, etc. It supports editing functionalities for chords, rhythm, and lyrics.   \n\n- **Other**, audio source separation, speed adjustment, pitch shifting, etc.      \n\nFor more AI-powered feature experiences, see the [website](https://lamucal.com): \n\n\n\n", "metadata": {"source": "github_readmes\\MuiscNN_Lamucal_README.md", "filename": "MuiscNN_Lamucal_README.md", "type": "readme_full"}}
{"id": "NUSTM_HIMT_README.md", "paper_id": "NUSTM_HIMT_README", "text": "# HIMT\nThe code for our TAFFC 2022 paper (https://ieeexplore.ieee.org/abstract/document/9765342/):\n\nJianfei Yu, Kai Chen, and Rui Xia. \"Hierarchical Interactive Multimodal Transformer for Aspect-Based Multimodal Sentiment Analysis.\" IEEE Transactions on Affective Computing (2022).\n\n----------Update------------\n\n2023.03.23  We upload **mywordbag.txt** and **myembedding.txt** to [Baidu Netdist](https://pan.baidu.com/s/1F3rI0oSA2GTvToXlhXAsmQ) with code kbtf which are needed when running our code.\n\n## Usage\nWe use [Faster R-CNN](https://github.com/peteanderson80/bottom-up-attention) to extract image features and corresponding semantic labels. You can download from [Baidu Netdist](https://pan.baidu.com/s/1F3rI0oSA2GTvToXlhXAsmQ) with the extraction code kbtf. Before you run the bash file, you need to change the parameter **--img_path** to your saved path.\n\nFor the textual input of each sample, please refer to [TomBERT](https://github.com/jefferyYu/TomBERT) to download them from the **absa_data** folder, and then change the parameter **--data_dir** to your saved path.\n\n## Training and Inference\n```\nbash run_himt.sh\n```\n\n## Acknowledgements\n\n- Most of the codes are based on the codes provided by huggingface: https://github.com/huggingface/transformers.\n", "metadata": {"source": "github_readmes\\NUSTM_HIMT_README.md", "filename": "NUSTM_HIMT_README.md", "type": "readme_full"}}
{"id": "OmicsML_scMoFormer_README.md", "paper_id": "OmicsML_scMoFormer_README", "text": "# scMoFormer\n\nThis is the official implementation of *Single-Cell Multimodal Prediction via Transformers* [Arxiv](https://arxiv.org/abs/2303.00233).\n\n# Run our code\n\nTo facilitate the reproducibility of our work, please execute the script `run.sh`.\n\n*Note: Script `./utils/download_data.py` requires [Kaggle](https://www.kaggle.com/competitions/open-problems-multimodal/) api for CITE dataset.\n\n# Data Availability\n\nThe CITE dataset is available on [Kaggle](https://www.kaggle.com/competitions/open-problems-multimodal/) and the GEX2ADT dataset is downloaded via [DANCE](https://github.com/OmicsML/dance).", "metadata": {"source": "github_readmes\\OmicsML_scMoFormer_README.md", "filename": "OmicsML_scMoFormer_README.md", "type": "readme_full"}}
{"id": "open-mmlab_Multimodal-GPT_README.md", "paper_id": "open-mmlab_Multimodal-GPT_README", "text": "# 🤖 Multi-modal GPT\n\nTrain a multi-modal chatbot with visual and language instructions!\n\nBased on the open-source multi-modal model [OpenFlamingo](https://github.com/mlfoundations/open_flamingo), we create various **visual instruction** data with open datasets, including VQA, Image Captioning, Visual Reasoning, Text OCR, and Visual Dialogue. Additionally, we also train the language model component of OpenFlamingo using only **language-only instruction** data.\n\nThe **joint training** of visual and language instructions effectively improves the performance of the model! For more details please refer to our [technical report](https://arxiv.org/abs/2305.04790).\n\nWelcome to join us!\n\n</div>\n\n<div align=\"center\">\n\nEnglish | [简体中文](README_zh-CN.md)\n\n</div>\n\n<div align=\"center\">\n  <a href=\"https://openmmlab.medium.com/\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png\" width=\"3%\" alt=\"\" /></a>\n  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n  <a href=\"https://discord.com/channels/1037617289144569886/1046608014234370059\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png\" width=\"3%\" alt=\"\" /></a>\n  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n  <a href=\"https://twitter.com/OpenMMLab\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png\" width=\"3%\" alt=\"\" /></a>\n  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n  <a href=\"https://www.youtube.com/openmmlab\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png\" width=\"3%\" alt=\"\" /></a>\n  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n  <a href=\"https://space.bilibili.com/1293512903\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png\" width=\"3%\" alt=\"\" /></a>\n  <img src=\"https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png\" width=\"3%\" alt=\"\" />\n  <a href=\"https://www.zhihu.com/people/openmmlab\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png\" width=\"3%\" alt=\"\" /></a>\n</div>\n\n## Features\n\n- Support various vision and language instruction data\n- Parameter efficient fine-tuning with LoRA\n- Tuning vision and language at the same time, complement each other\n\n\n## Installation\n\nTo install the package in an existing environment, run\n\n```bash\ngit clone https://github.com/open-mmlab/Multimodal-GPT.git\ncd Multimodal-GPT\npip install -r requirements.txt\npip install -v -e .\n```\n\nor create a new conda environment\n\n```bash\nconda env create -f environment.yml\n```\n\n\n## Launch Demo Locally\n\n1. Download the pre-trained weights.\n\n    Use [this script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) for converting LLaMA weights to Hugging Face format.\n\n    Download the OpenFlamingo pre-trained model from [openflamingo/OpenFlamingo-9B](https://huggingface.co/openflamingo/OpenFlamingo-9B).\n\n    Download our LoRA Weight from [here](https://download.openmmlab.com/mmgpt/v0/mmgpt-lora-v0-release.pt).\n\n    Then place these models in `checkpoints` folders like this:\n\n    ```\n    checkpoints\n    ├── llama-7b_hf\n    │   ├── config.json\n    │   ├── pytorch_model-00001-of-00002.bin\n    │   ├── ......\n    │   └── tokenizer.model\n    ├── OpenFlamingo-9B\n    │   └──checkpoint.pt\n    ├──mmgpt-lora-v0-release.pt\n\n2. launch the gradio demo\n\n    ```bash\n    python app.py\n    ```\n\n## Examples\n\n### Recipe:\n![image4](https://user-images.githubusercontent.com/12907710/234554562-8f3be88f-d563-47ba-97d9-ade8d47c46b0.png)\n\n### Travel plan:\n![image3](https://user-images.githubusercontent.com/12907710/234523464-80c4e3f0-f99f-4498-96ef-dc43ef89c64b.png)\n\n### Movie:\n![image2](https://user-images.githubusercontent.com/12907710/234523468-e11905a6-491f-4b87-934f-90da7d14d1c3.png)\n\n### Famous person:\n![image](https://user-images.githubusercontent.com/12907710/234523475-fd91f979-a344-4228-813f-6b55a1bc250f.png)\n\n\n## Fine-tuning\n\n### Prepare datasets\n\n1. [A-OKVQA](https://allenai.org/project/a-okvqa/home)\n\n    Download annotation from [this link](https://prior-datasets.s3.us-east-2.amazonaws.com/aokvqa/aokvqa_v1p0.tar.gz) and unzip to `data/aokvqa/annotations`.\n\n    It also requires images from coco dataset which can be downloaded from [here](https://cocodataset.org/#home). \n\n2. [COCO Caption](https://cs.stanford.edu/people/karpathy/deepimagesent/)\n\n    Download from [this link](https://cs.stanford.edu/people/karpathy/deepimagesent/coco.zip) and unzip to `data/coco`.\n\n    It also requires images from coco dataset which can be downloaded from [here](https://cocodataset.org/#home).\n\n3. [OCR VQA](https://ocr-vqa.github.io/)\n\n    Download from [this link](https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing) and place in `data/OCR_VQA/`.\n\n4. [LlaVA](https://llava-vl.github.io/)\n\n    Download from [liuhaotian/LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) and place in `data/llava/`.\n\n    It also requires images from coco dataset which can be downloaded from [here](https://cocodataset.org/#home).\n\n5. [Mini-GPT4](https://minigpt-4.github.io/)\n\n    Download from [Vision-CAIR/cc_sbu_align](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align) and place in `data/cc_sbu_align/`.\n\n6. [Dolly 15k](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)\n\n    Download from [databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) and place it in `data/dolly/databricks-dolly-15k.jsonl`.\n\n7. [Alpaca GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n\n    Download it from [this link](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/raw/main/data/alpaca_gpt4_data.json) and place it in `data/alpaca_gpt4/alpaca_gpt4_data.json`.\n\nYou can also customize the data path in the [configs/dataset_config.py](configs/dataset_config.py).\n\n8. [Baize](https://github.com/project-baize/baize-chatbot)\n\n    Download it from [this link](https://github.com/project-baize/baize-chatbot/blob/main/data/quora_chat_data.json) and place it in `data/baize/quora_chat_data.json`.\n\n\n## Start training\n\n```bash\ntorchrun --nproc_per_node=8 mmgpt/train/instruction_finetune.py \\\n  --lm_path checkpoints/llama-7b_hf \\\n  --tokenizer_path checkpoints/llama-7b_hf \\\n  --pretrained_path checkpoints/OpenFlamingo-9B/checkpoint.pt \\\n  --run_name train-my-gpt4 \\\n  --learning_rate 1e-5 \\\n  --lr_scheduler cosine \\\n  --batch_size 1 \\ \n  --tuning_config configs/lora_config.py \\\n  --dataset_config configs/dataset_config.py \\\n  --report_to_wandb\n```\n\n\n## Acknowledgements\n\n- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)\n- [LAVIS](https://github.com/salesforce/LAVIS)\n- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n- [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)\n- [LLaVA](https://github.com/haotian-liu/LLaVA/tree/main)\n- [Instruction Tuning with GPT-4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n\nIf you find our project useful for your research and applications, please cite using this BibTeX:\n\n```bibtex\n@misc{gong2023multimodalgpt,\n      title={MultiModal-GPT: A Vision and Language Model for Dialogue with Humans}, \n      author={Tao Gong and Chengqi Lyu and Shilong Zhang and Yudong Wang and Miao Zheng and Qian Zhao and Kuikun Liu and Wenwei Zhang and Ping Luo and Kai Chen},\n      year={2023},\n      eprint={2305.04790},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n", "metadata": {"source": "github_readmes\\open-mmlab_Multimodal-GPT_README.md", "filename": "open-mmlab_Multimodal-GPT_README.md", "type": "readme_full"}}
{"id": "OPPO-Mente-Lab_X2I_README.md", "paper_id": "OPPO-Mente-Lab_X2I_README", "text": "<div align=\"center\">\n  <h1>X2I (ICCV 2025)</h1>\n<a href='https://export.arxiv.org/abs/2503.06134'><img src='https://img.shields.io/badge/arXiv-2503.06134-b31b1b.svg'></a> &nbsp;\n<a href='https://huggingface.co/OPPOer/X2I'><img src='https://img.shields.io/badge/🤗%20HuggingFace-X2I-ffd21f.svg'></a>\n</div>\n\n\n> **X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation**\n> <br>\n[Jian Ma](https://scholar.google.com/citations?hl=zh-CN&user=XtzIT8UAAAAJ)<sup>1</sup>*, \n[Qirong Peng](https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=gUPpazEAAAAJ)<sup>1</sup>*, \n[Xu Guo](https://github.com/Guoxu1233)<sup>2</sup>, \n[Chen Chen](https://scholar.google.com/citations?user=CANDhfAAAAAJ&hl=zh-CN)<sup>1</sup>,\n[Haonan Lu](https://scholar.google.com/citations?user=EPBgKu0AAAAJ&hl=en)<sup>1</sup>,\n[Zhenyu Yang](https://scholar.google.com/citations?user=rZ15gC4AAAAJ)<sup>1</sup>\n<br>\n<sup>1</sup>OPPO AI Center, <sup>2</sup>Tsinghua University\n<br>\n\n<div align=\"center\">\n  <img src=\"assets/figures/intro.jpg\" alt=\"X2I Framework\">\n</div>\n\n## Abstract\n<b>Powered by MLLM, our X2I acquires the ability to process multimodal inputs (text/image/video/audio) and generate corresponding images.</b>\n\n<details><summary>CLICK for full abstract</summary>\nThe text-to-image models' capability to generate realistic images based on textual prompts and the multimodal understanding ability of Multimodal Language Models (MLLM) are well-recognized. However, there is currently a lack of a concise and efficient framework that transfers the multimodal understanding ability of MLLM to the T2I model, enabling it to comprehend multimodal inputs. In this paper, we design the X2I framework to endow Diffusion Transformer Models with MLLM's understanding abilities, encompassing information from various sources such as multilingual text, lengthy documents, OCR-generated content, images, videos, and audio. The framework training is divided into two phases. In the first phase, alignment training requires only 20 hours with 8 A100 GPUs and uses a corpus of 100,000 purely English texts to distill the inference capabilities of the teacher model. Through our efficiently trained lightweight alignment network structure, our model not only retains the teacher model's text-to-image generation capabilities almost without loss but also acquires various multimodal understanding abilities. It can also perform certain image instruction editing and generation tasks. Furthermore, X2I can be utilized for lora training for text-to-image and image-to-image tasks, addressing a gap in the industry for this direction.In the second phase, a simple branch network is designed to enhance the fidelity of images generated during instruction editing. At the end of the first phase of training, we use extensive experiments to demonstrate the method's effectiveness, efficiency, versatility, and transferability.\n</details>\n\n## Changelog\n- **[2025.03.23]** 🔥 🔥 🔥 We release the [X2I-Comfyui](https://github.com/OPPO-Mente-Lab/X2I/tree/main/x2i_comfyui). Try it now! Please give us a star!\n- **[2025.03.15]** 🔥 Release checkpoints on huggingface!\n- **[2025.03.08]** 🔥 Release training and inference code!\n- **[2025.03.08]** 🔥 Release our Paper!\n\n## TODO\n- [x] Release training and inference code of MiniCPM-o-2.6\n- [x] Release training and inference code of QwenVL-2.5\n- [x] Release training and inference code of InternVL-2.5\n- [x] Release checkpoints on [huggingface](https://huggingface.co/OPPOer/X2I)\n- [x] ComfyUI\n\n## Model Zoo Table\n\n| Model              |                                    Checkpoints                                     |\n|:-------------------|:----------------------------------------------------------------------------------:|\n| X2I-MiniCPM-o-2.6  |                  [Checkpoints](https://huggingface.co/OPPOer/X2I/tree/main)                  | \n| X2I-InternVL2.5-1B |                  [Checkpoints](https://huggingface.co/OPPOer/X2I/tree/main)                  | \n| X2I-InternVL2.5-4B |                  [Checkpoints](https://huggingface.co/OPPOer/X2I/tree/main)                  | \n| X2I-QwenVL2.5-3B   |                  [Checkpoints](https://huggingface.co/OPPOer/X2I/tree/main)                  |  \n| X2I-QwenVL2.5-7B   |                  [Checkpoints](https://huggingface.co/OPPOer/X2I/tree/main)                  |  \n\n## Model Architecture\n![framework](assets/figures/method.jpg \"framework\")\n## Environment\n\nPrepare the environment, install the required libraries:\n\n```shell\n$ cd x2i\n$ conda create --name x2i python==3.11\n$ conda activate x2i\n$ # Install PyTorch 2.4.1 by selecting the appropriate command according to your environment's CUDA version. Refer to: https://pytorch.org/get-started/previous-versions/ for guidance.\n$ pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu118\n$ pip install -r requirements.txt\n\nNote: If you are using MiniCPM, please downgrade transformers to version 4.48.0 using:\n$ pip install transformers==4.48.0\n```\n\n## Inference\n\nX2I provides inference scripts for **QwenVL**, **InternVL**, and **MiniCPM** frameworks. The example demonstrates usage with MiniCPM-o-2_6 via [`inference_minicpm.py`](./inference_minicpm.py). For other models:\n\n- **Intern2.5VL-1Bor4B-** → use [`inference_internvl.py`](./inference_internvl.py)\n- **Qwen2.5VL-3Bor7B** → use [`inference_qwenvl.py`](./inference_qwenvl.py)\n\nAll scripts follow analogous command patterns. Simply replace the script filename while maintaining consistent parameter configurations.\n```shell\n$ cd infer\n$ python inference_minicpm.py\n```\n\nIt will download openbmb/MiniCPM-o-2_6, shuttleai/shuttle-3-diffusion.\nIf you want to use local model, you can inference like this:\n\n```shell\n$ python inference_minicpm.py  --minicpm_path \"local MiniCPM-o 2.6 path\" --flux_path \"local shuttle-3-diffusion or FLUX.1 schnell or FLUX.1 dev path\"  --num_steps 4 --num_gen_imgs 1 --task \"all\"\n```\n- **minicpm_path:** The path of MiniCPM-o 2.6, default: `openbmb/MiniCPM-o-2_6`\n- **flux_path:** The path of FLUX.1 schnell or FLUX.1 dev or shuttle-3-diffusion, default: `shuttleai/shuttle-3-diffusion`\n- **num_step:** The number of steps required to generate an image. default: `4`, If using FLUX.1 dev, change to `28`\n- **num_gen_imgs:** The number of images generated per prompt. default: `1`\n- **task:** The type of image generation task. contain: `text2image/image2image/imagetext2image/video2image/audio2image/x2image/all`, default: `all`.\n\n### Text2image\n\nX2I supports generating images in multiple languages. <br/>\nYou can run the text2image task like this:\n\n```shell\n$ python inference_minicpm.py  --minicpm_path \"local MiniCPM-o 2.6 path\" --flux_path \"local shuttle-3-diffusion or FLUX.1 schnell or FLUX.1 dev path\"  --num_steps 4 --num_gen_imgs 1 --task \"text2image\"\n```\n\n### Image2image\n\nX2I supports reference-guided image generation, celebrity, and multi-image composition tasks. <br/>\nYou can run the image2image task like this:\n\n\n```shell\n$ python inference_minicpm.py  --minicpm_path \"local MiniCPM-o 2.6 path\" --flux_path \"local shuttle-3-diffusion or FLUX.1 schnell or FLUX.1 dev path\"  --num_steps 4 --num_gen_imgs 1 --task \"image2image\"\n```\n\n\n### Imagetext2image\n\nX2I supports user-prompt-driven expression editing, along with single image or multi-image editing and fusion tasks. Furthermore, X2I support image generation based on multilingual text content in images. <br/>\nYou can run the imagetext2image task like this:\n\n```shell\n$ python inference_minicpm.py  --minicpm_path \"local MiniCPM-o 2.6 path\" --flux_path \"local shuttle-3-diffusion or FLUX.1 schnell or FLUX.1 dev path\"  --num_steps 4 --num_gen_imgs 1 --task \"imagetext2image\"\n```\n\n### Video2image\n\nX2I can directly generate images based on the semantic content of input video sequences. <br/>\nYou can run the video2image task like this:\n\n```shell\n$ python inference_minicpm.py  --minicpm_path \"local MiniCPM-o 2.6 path\" --flux_path \"local shuttle-3-diffusion or FLUX.1 schnell or FLUX.1 dev path\"  --num_steps 4 --num_gen_imgs 1 --task \"video2image\"\n```\n\n### Audio2image\n\nLeveraging the audio comprehension capabilities of MLLMs such as MiniCPM-o, X2I can directly generate images based on audio.<br/>\nYou can run the audio2image task like this:\n\n```shell\n$ python inference_minicpm.py  --minicpm_path \"local MiniCPM-o 2.6 path\" --flux_path \"local shuttle-3-diffusion or FLUX.1 schnell or FLUX.1 dev path\"  --num_steps 4 --num_gen_imgs 1 --task \"audio2image\"\n```\n\n### X2image\n\nX2I can comprehend hybrid inputs combining audio, images, videos, and text prompts to generate images.<br/>\nYou can run the x2image task like this:\n\n\n```shell\n$ python inference_minicpm.py  --minicpm_path \"local MiniCPM-o 2.6 path\" --flux_path \"local shuttle-3-diffusion or FLUX.1 schnell or FLUX.1 dev path\"  --num_steps 4 --num_gen_imgs 1 --task \"x2image\"\n```\n### Reasoning2image\nX2I also supports image generation using MLLM's reasoning capabilities based on the answers obtained after reasoning.<br/>\nYou can run the reasoning2image like this:\n```shell\n$ python inference_qwenvl.py --use_answer True --task \"all\"\n```\n<div align=\"center\">\n  <img src=\"assets/figures/reasoning.png\" alt=\"X2I Framework\">\n</div>\n\n### Multi-turn2image\nEquipped with multi-turn dialogue capabilities inherent in MLLMs, X2I demonstrates preserved fidelity and contextual coherence during conversational interactions, as illustrated in the figure below.<br/>\nYou can run the multi-turn2image like this:\n```shell\n$ python inference_multi_turn.py\n```\n<div align=\"center\">\n  <img src=\"assets/figures/multi_turn.png\" alt=\"X2I Framework\">\n</div>\n\n## Train\nWe organize the dataset using the **[WebDataset](https://github.com/webdataset/webdataset)** format. \nPlease replace the dataset in the training script.\nThen you can run:\n\n   - **For MiniCPM training**  \n     ```shell\n     cd train\n     bash train_minicpm.sh\n     ```\n\n   - **For QwenVL training**  \n     ```shell\n     cd train\n     bash train_qwenvl.sh\n     ```\n\n   - **For InternVL training**  \n     ```shell\n     cd train\n     bash train_internvl.sh\n     ```\n   - **For LightControl training**  \n     ```shell\n     cd lightcontrol\n     bash train_lightcontrol.sh\n     ```\n## Acknowledgements \nThis code is builds on the code from the [diffusers](https://github.com/huggingface/diffusers), \n[MiniCPM-o](https://github.com/OpenBMB/MiniCPM-o),\n[InternVL](https://github.com/OpenGVLab/InternVL),\n[QwenVL](https://github.com/QwenLM/Qwen-VL),\n[PEA-Diffusion](https://github.com/OPPO-Mente-Lab/PEA-Diffusion), and \n[Subject-Diffusion](https://github.com/OPPO-Mente-Lab/Subject-Diffusion).\n\n\n", "metadata": {"source": "github_readmes\\OPPO-Mente-Lab_X2I_README.md", "filename": "OPPO-Mente-Lab_X2I_README.md", "type": "readme_full"}}
{"id": "pandayuanyu_HCFusion_README.md", "paper_id": "pandayuanyu_HCFusion_README", "text": "# HCFusion\nOffical code for Multimodal Image Fusion based on  Hybrid CNN-Transformer and Non-local Cross-modal Attention\n", "metadata": {"source": "github_readmes\\pandayuanyu_HCFusion_README.md", "filename": "pandayuanyu_HCFusion_README.md", "type": "readme_full"}}
{"id": "papermsucode_mdmmt_README.md", "paper_id": "papermsucode_mdmmt_README", "text": "# Introduction\n\nIn this repository we present the testing code for article \n[MDMMT: Multidomain Multimodal Transformer for Video Retrieval](https://arxiv.org/abs/2103.10699).\n\n[Presentation](./MDMMT_HVU_CVPR_2021.pdf) from CVPR-2021 Workshop \"[Large Scale Holistic Video Understanding](https://holistic-video-understanding.github.io/workshops/cvpr2021.html)\".\n\nThis code helps:\n1. Create embeddings with CLIP, irCSN152 and VGGish;\n2. Create caption index files;\n3. Run test with created embeddings and captions index files.\n\nOur pretrained model is available [here](https://drive.google.com/file/d/1dVDouFZFEDrjqhvtjNFP633y0JjHf4Ya/view?usp=sharing).\n\n\n# Citation\n```\n@misc{dzabraev2021mdmmt,\n      title={MDMMT: Multidomain Multimodal Transformer for Video Retrieval}, \n      author={Maksim Dzabraev and Maksim Kalashnikov and Stepan Komkov and Aleksandr Petiushko},\n      year={2021},\n      eprint={2103.10699},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\nExpected testing results:\n\n```\nMSRVTT\nt2v/R1: 22.87123745819398\nt2v/R5: 49.67056856187291\nt2v/R10: 61.66722408026756\nt2v/R50: 83.95819397993311\nt2v/MedR: 6.0\nt2v/MeanR: 53.69550323486328\nv2t/R1: 5.075250836120401\nv2t/R5: 13.777591973244148\nv2t/R10: 19.695652173913043\nv2t/R50: 41.44314381270903\nv2t/MedR: 84.0\nv2t/MeanR: 695.3896484375\n\n\nLSMDC\nt2v/R1: 17.31731731731732\nt2v/R5: 38.23823823823824\nt2v/R10: 47.447447447447445\nt2v/R50: 72.87287287287288\nt2v/MedR: 12.0\nt2v/MeanR: 59.398399353027344\nv2t/R1: 16.716716716716718\nv2t/R5: 37.73773773773774\nv2t/R10: 45.545545545545544\nv2t/R50: 72.27227227227228\nv2t/MedR: 14.0\nv2t/MeanR: 60.97697448730469\n\nActivityNet\nt2v/R1: 19.673503557974048\nt2v/R5: 45.22812892423608\nt2v/R10: 56.7182921724571\nt2v/R50: 80.8915864378401\nt2v/MedR: 7.0\nt2v/MeanR: 72.35956573486328\nv2t/R1: 19.715362076182505\nv2t/R5: 44.72582670573462\nv2t/R10: 57.157806613645874\nv2t/R50: 80.87065717873587\nv2t/MedR: 7.0\nv2t/MeanR: 68.22499084472656\n``` \n\n\n\n# Downloads\n\n```bash\nmkdir -p ckpts\n# https://github.com/facebookresearch/VMZ/\nwget https://github.com/bjuncek/VMZ/releases/download/test_models/irCSN_152_ig65m_from_scratch_f125286141.pth -O ckpts/irCSN_152_ig65m_from_scratch_f125286141.pth\n\n# https://github.com/tensorflow/models/tree/master/research/audioset/vggish\nwget https://storage.googleapis.com/audioset/vggish_model.ckpt -O ckpts/vggish_model.ckpt\n\ngit clone https://github.com/openai/CLIP models/CLIP\ngit clone https://github.com/tensorflow/models/ models/tensorflow_models\n```\n\n# Environment\nIt is recommended to use conda to install packages.\nIt is recommended to create two environments. The first one for audio dumping, and the second for video\n\n## Audio environment\n\nUse this environment for producing embeddings with `tf_vggish`\n\n```\ntqdm\nffmpeg=4.2.2\ntensorflow-gpu\ntf_slim\nresampy\nsix\npysoundfile\nnumpy=1.20.2 # !!! Make sure that intel-mkl is not used. It causes segfault in np.fft.rfft !!!\n```\n\n## Video environment\n\nUse this environment for producing embeddings with `CLIP` and `irCSN152`\n\n```bash\ntqdm\npytorch=1.7.1 # !!! It is recommended to use pytorch=1.7.1; 1.8+ is not working with CLIP !!!\ntorchvision\nffmpeg=4.2.2\nftfy\nregex\n```\n\n\n\n# Create lists\n\nReplace `\"<*_DATASET_ROOT>/` with directory where raw video files are located.\n\n```bash\ncat lists/LSMDC/fnames.lst | awk '{print \"<LSMDC_DATASET_ROOT>/\" $0}' > LSMDC.lst\ncat lists/ActivityNet/fnames.lst | awk '{print \"<ActivityNet_DATASET_ROOT>/\" $0}' > ActivityNet_val.lst\ncat lists/msrvtt/fnames.lst | awk '{print \"<msrvtt_DATASET_ROOT>/\" $0}' > msrvtt_test.lst\n```\n\n\n# Embeddings\n\n```bash\npython dumper.py \\\n    --model_type=VMZ_irCSN_152 \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/msrvtt/VMZ_irCSN_152/test  \\\n    --lst=msrvtt_test.lst \\\n    --nworker_per_gpu=2 \\\n    --per_batch_size=8 \\\n    --fps=32 \\\n    --frame_size=224 \\\n    --frame_crop_size=224 \\\n    --frames_per_clip=32\n\npython dumper.py \\\n    --model_type=CLIP \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/msrvtt/CLIP/test  \\\n    --lst=msrvtt_test.lst \\\n    --nworker_per_gpu=8 \\\n    --per_batch_size=128 \\\n    --fps=1 \\\n    --frame_size=228 \\\n    --frame_crop_size=228 \\\n    --frames_per_clip=1\n\nPYTHONPATH=\\\nmodels/tensorflow_models/research/audioset/vggish:\\\nmodels/tensorflow_models/research/audioset/:\\\n$PYTHONPATH \\\npython dumper.py \\\n    --model_type=tf_vggish \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/msrvtt/tf_vggish/test \\\n    --lst=msrvtt_test.lst \\\n    --nworker_per_gpu=2\n\n\n\npython dumper.py \\\n    --model_type=VMZ_irCSN_152 \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/ActivityNet/VMZ_irCSN_152/test \\\n    --lst=ActivityNet_val.lst \\\n    --nworker_per_gpu=3 \\\n    --per_batch_size=8 \\\n    --fps=32 \\\n    --frame_size=224 \\\n    --frame_crop_size=224 \\\n    --frames_per_clip=32\n\npython dumper.py \\\n    --model_type=CLIP \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/ActivityNet/CLIP/test \\\n    --lst=ActivityNet_val.lst \\\n    --nworker_per_gpu=3 \\\n    --per_batch_size=128 \\\n    --fps=1 \\\n    --frame_size=228 \\\n    --frame_crop_size=228 \\\n    --frames_per_clip=1 \\\n    --num_readers=8\n\nPYTHONPATH=\\\nmodels/tensorflow_models/research/audioset/vggish:\\\nmodels/tensorflow_models/research/audioset/:\\\n$PYTHONPATH \\\npython dumper.py \\\n    --model_type=tf_vggish \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/ActivityNet/tf_vggish/test \\\n    --lst=ActivityNet_val.lst \\\n    --nworker_per_gpu=3 \\\n    --per_batch_size=32\n\n\n\npython dumper.py \\\n    --model_type=VMZ_irCSN_152 \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/LSMDC/VMZ_irCSN_152/test \\\n    --lst=LSMDC.lst \\\n    --nworker_per_gpu=2 \\\n    --per_batch_size=8 \\\n    --fps=32 \\\n    --frame_size=224 \\\n    --frame_crop_size=224 \\\n    --frames_per_clip=32\n\npython dumper.py \\\n    --model_type=CLIP \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/LSMDC/CLIP/test \\\n    --lst=LSMDC.lst \\\n    --nworker_per_gpu=2 \\\n    --per_batch_size=128 \\\n    --fps=1 \\\n    --frame_size=228 \\\n    --frame_crop_size=228 \\\n    --frames_per_clip=1 \\\n    --num_readers=8\n\nPYTHONPATH=\\\nmodels/tensorflow_models/research/audioset/vggish:\\\nmodels/tensorflow_models/research/audioset/:\\\n$PYTHONPATH \\\npython dumper.py \\\n    --model_type=tf_vggish \\\n    --gpus=0,1,2,3,4,5,6,7 \\\n    --dst_prefix=/ssd/ssd_srv79/dza/dumps/LSMDC/tf_vggish/test \\\n    --lst=LSMDC.lst \\\n    --nworker_per_gpu=2 \\\n    --per_batch_size=32\n\n\n```\n\n# Create caption index\n\n```bash\npython create_capts.py \\\n\t--dataset=msrvtt \\\n\t--output_root=/tmp/capts/msrvtt/ \\\n\t--modality=VIDEO:2048:/ssd/ssd_srv79/dza/dumps/msrvtt/VMZ_irCSN_152/test \\\n\t--modality=CLIP:512:/ssd/ssd_srv79/dza/dumps/msrvtt/CLIP/test \\\n\t--modality=tf_vggish:128:/ssd/ssd_srv79/dza/dumps/msrvtt/tf_vggish/test\nmkdir -p /tmp/capts/msrvtt/symlinked-feats/\ncp lists/msrvtt/test_list_full.txt /tmp/capts/msrvtt/symlinked-feats/test_list_full.txt\n\npython create_capts.py \\\n\t--dataset=ActivityNet \\\n\t--output_root=/tmp/capts/ActivityNet/ \\\n\t--modality=VIDEO:2048:/ssd/ssd_srv79/dza/dumps/ActivityNet/VMZ_irCSN_152/test \\\n\t--modality=CLIP:512:/ssd/ssd_srv79/dza/dumps/ActivityNet/CLIP/test \\\n\t--modality=tf_vggish:128:/ssd/ssd_srv79/dza/dumps/ActivityNet/tf_vggish/test\nmkdir -p /tmp/capts/ActivityNet/symlinked-feats/\ncp lists/ActivityNet/val.vids /tmp/capts/ActivityNet/symlinked-feats/val.vids\n\npython create_capts.py \\\n\t--dataset=lsmdc_publictest \\\n\t--output_root=/tmp/capts/LSMDC/ \\\n\t--modality=VIDEO:2048:/ssd/ssd_srv79/dza/dumps/LSMDC/VMZ_irCSN_152/test \\\n\t--modality=CLIP:512:/ssd/ssd_srv79/dza/dumps/LSMDC/CLIP/test \\\n\t--modality=tf_vggish:128:/ssd/ssd_srv79/dza/dumps/LSMDC/tf_vggish/test\nmkdir -p /tmp/capts/LSMDC/symlinked-feats/\ncp lists/LSMDC/test.vids /tmp/capts/LSMDC/symlinked-feats/test.vids\n```\n\n\n# Test\n\n```bash\npython test.py --dataset_root=/tmp/capts/msrvtt/  --checkpoint=<PATH_TO_MODEL>/mdmmt_3mod.pth  --dataset_name=MSRVTT_full --gpu=2\npython test.py --dataset_root=/tmp/capts/LSMDC/  --checkpoint=<PATH_TO_MODEL>/mdmmt_3mod.pth  --dataset_name=lsmdc_publictest --gpu=2\npython test.py --dataset_root=/tmp/capts/ActivityNet/  --checkpoint=<PATH_TO_MODEL>/mdmmt_3mod.pth  --dataset_name=ActivityNet --gpu=2\n\n```\n\n# WARNING\n\nDo not use numpy with mkl backend. Sometimes np.fft.rfft produce segmentation fault.\n", "metadata": {"source": "github_readmes\\papermsucode_mdmmt_README.md", "filename": "papermsucode_mdmmt_README.md", "type": "readme_full"}}
{"id": "pliang279_HighMMT_README.md", "paper_id": "pliang279_HighMMT_README", "text": "# HighMMT\n\nHighMMT is a general-purpose model for high-modality (large number of modalities beyond the prototypical language, visual, and acoustic modalities) and partially-observable (across many tasks, where each task is defined only over a small subset of all modalities we are interested in modeling) scenarios.\n\nHighMMT uses multitask learning with shared unimodal and multimodal layers to enable stable parameter counts (addressing scalability) and cross-modal transfer learning to enable information sharing across modalities and tasks (addressing partial observability).\n\nThe same HighMMT model (architecture and parameters) is able to simultaneously encode joint representations between different subsets spanning images, text, audio, sets, time-series, and graphs.\n\n## Paper\n\n[**High-Modality Multimodal Transformer: Quantifying Modality \\& Interaction Heterogeneity for High-Modality Representation Learning**](https://openreview.net/forum?id=ttzypy3kT7)<br>\nPaul Pu Liang, Yiwei Lyu, Xiang Fan, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov<br>\nTMLR 2022.\n\nIf you find this repository useful, please cite our paper:\n```\n@article{liang2022high,\n  title={High-Modality Multimodal Transformer: Quantifying Modality \\& Interaction Heterogeneity for High-Modality Representation Learning},\n  author={Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Tsaw, Jeffrey and Liu, Yudong and Mo, Shentong and Yogatama, Dani and Morency, Louis-Philippe and Salakhutdinov, Russ},\n  journal={Transactions on Machine Learning Research},\n  year={2022}\n}\n```\n\n## Contributors\n\nCorrespondence to: \n  - [Paul Pu Liang](http://www.cs.cmu.edu/~pliang/) (pliang@cs.cmu.edu)\n  - [Yiwei Lyu](https://github.com/lvyiwei1) (ylyu1@andrew.cmu.edu)\n  - [Xiang Fan](https://github.com/sfanxiang) (xiangfan@cmu.edu)\n  - [Jeffrey Tsaw](https://github.com/jeffreytsaw) (jtsaw@andrew.cmu.edu)\n  - Yudong Liu (yudongl@andrew.cmu.edu)\n  - [Shentong Mo](https://scholar.google.com/citations?user=6aYncPAAAAAJ&hl=en) (shentonm@andrew.cmu.edu)\n  - [Dani Yogatama](https://dyogatama.github.io/)\n  - [Louis-Philippe Morency](https://www.cs.cmu.edu/~morency/)\n  - [Ruslan Salakhutdinov](https://www.cs.cmu.edu/~rsalakhu/)\n\n## Usage\n### Environment Setup Using Conda\n```\nconda env create -f env_HighMMT.yml\n```\n###\n\n### Quick Start\nThe instructions for running the code and data retreival can be found after typing\n```\n./run.sh help\n```\nYou can also find detailed instructions below\n###\n\n### Data Download\nthree datasets: robotics, enrico and RTFM can be setup directly using script ./download_datasets.sh\nRun \n```\n./download_datasets.sh help\n```\nfor instructions\nTo setup each dataset, run \"./download_datasets.sh <datasetname>\"\nFor example\n```\n./download_datasets.sh robotics\n```\ndownloads the robotics dataset to the directory datasets/robotics\nThis repo is built on top of the MultiBench repository, so to download the dataset, follow the same instructions as https://github.com/pliang279/MultiBench.git\n### Easy setting experiment code\n\nFrom the root of this repo, run\n```sh\npython private_test_scripts/perceivers/roboticstasks.py model.pt\n```\nThe model will be saved to `model.pt`.\n\n### Medium setting experiment code\n\nTo run medium tasks, please run\n```\npython private_test_scripts/perceivers/medium_tasks.py\n```\n\n### Hard setting experiment code\n\nTo run multitask training on 1/2/3/4 tasks, please run\n```\npython private_test_scripts/perceivers/singletask.py\npython private_test_scripts/perceivers/twomultitask.py\npython private_test_scripts/perceivers/threemultitask.py\npython private_test_scripts/perceivers/fourmultitask.py\n```\n\n### Parameter Sharing Experiments\nTo run the parameter sharing experiments, please run \n```\npython private_test_scripts/perceivers/shared_fourmulti.py\n```\n\nA baseline can be trained as a starting point for finetuning by running the fourmultitask.py file like described above. You can specify the baseline in shared_fourmulti.py. \n\nParameter groupings can also be specified in the shared_fourmulti.py file.\n\n### Heterogeneity Matrix \n\nTo run get the heterogeneity matrix between individual modalitiesa and pairs of modalities, please run\n```\npython private_test_scripts/perceivers/tasksim.py\n```\n", "metadata": {"source": "github_readmes\\pliang279_HighMMT_README.md", "filename": "pliang279_HighMMT_README.md", "type": "readme_full"}}
{"id": "Rubics-Xuan_TransBTS_README.md", "paper_id": "Rubics-Xuan_TransBTS_README", "text": "# TransBTS（MICCAI2021）& TransBTSV2 (To Be Updated)\n\nThis repo is the official implementation for: \n1) [TransBTS: Multimodal Brain Tumor Segmentation Using Transformer](https://arxiv.org/abs/2103.04430). \n\n2) [TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images](https://arxiv.org/abs/2201.12785). \n\nThe details of the our TransBTS and TransBTSV2 can be found at the models directory ([TransBTS](https://github.com/Wenxuan-1119/TransBTS/tree/main/models/TransBTS) and [TransBTSV2](https://github.com/Wenxuan-1119/TransBTS/tree/main/models/TransBTSV2)) in this repo or in the original paper.\n\n## Requirements\n- python 3.7\n- pytorch 1.6.0\n- torchvision 0.7.0\n- pickle\n- nibabel\n\n## Data Acquisition\n- The multimodal brain tumor datasets (**BraTS 2019** & **BraTS 2020**) could be acquired from [here](https://ipp.cbica.upenn.edu/).\n\n- The liver tumor dataset **LiTS 2017** could be acquired from [here](https://competitions.codalab.org/competitions/17094#participate-get-data).\n\n- The kidney tumor dataset **KiTS 2019** could be acquired from [here](https://kits19.grand-challenge.org/data/).\n\n## Data Preprocess (BraTS 2019 & BraTS 2020)\nAfter downloading the dataset from [here](https://ipp.cbica.upenn.edu/), data preprocessing is needed which is to convert the .nii files as .pkl files and realize data normalization.\n\n`python3 preprocess.py`\n\n## Training\nRun the training script on BraTS dataset. Distributed training is available for training the proposed TransBTS, where --nproc_per_node decides the numer of gpus and --master_port implys the port number.\n\n`python3 -m torch.distributed.launch --nproc_per_node=4 --master_port 20003 train.py`\n\n## Testing \nIf  you want to test the model which has been trained on the BraTS dataset, run the testing script as following.\n\n`python3 test.py`\n\nAfter the testing process stops, you can upload the submission file to [here](https://ipp.cbica.upenn.edu/) for the final Dice_scores.\n\n## Citation\nIf you use our code or models in your work or find it is helpful, please cite the corresponding paper:\n\n- **TransBTS**:\n```\n@inproceedings{wang2021transbts,\n  title={TransBTS: Multimodal Brain Tumor Segmentation Using Transformer},\n  author={Wang, Wenxuan and Chen, Chen and Ding, Meng and Yu, Hong and Zha, Sen and Li, Jiangyun},\n  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2021: 24th International Conference, Strasbourg, France, September 27--October 1, 2021, Proceedings, Part I 24},\n  pages={109--119},\n  year={2021},\n  organization={Springer}\n}\n```\n\n- **TransBTSV2**:\n```\n@article{li2022transbtsv2,\n  title={TransBTSV2: Wider Instead of Deeper Transformer for Medical Image Segmentation},\n  author={Li, Jiangyun and Wang, Wenxuan and Chen, Chen and Zhang, Tianxiang and Zha, Sen and Yu, Hong and Wang, Jing},\n  journal={arXiv preprint arXiv:2201.12785},\n  year={2022}\n}\n```\n\n## Reference\n1.[setr-pytorch](https://github.com/gupta-abhay/setr-pytorch)\n\n2.[BraTS2017](https://github.com/MIC-DKFZ/BraTS2017)\n\n\n", "metadata": {"source": "github_readmes\\Rubics-Xuan_TransBTS_README.md", "filename": "Rubics-Xuan_TransBTS_README.md", "type": "readme_full"}}
{"id": "saharhzm_CrossModalSleepTransformer_README.md", "paper_id": "saharhzm_CrossModalSleepTransformer_README", "text": "# Cross-Modal SleepTransformer\nPublished in Journal of Biomedical Informatics\nhttps://www.sciencedirect.com/science/article/abs/pii/S1532046424001072\n\n![image](https://github.com/user-attachments/assets/d35e4031-bc01-4b3d-897c-9f3bf9a5f005)\n\n**Overview**\n\nThe Cross-modal SleepTransformer is a deep learning model designed for sleep stage classification based on multimodal physiological signals. It utilizes a transformer architecture, and cross-attention mechanism to effectively process and integrate information from multiple input modalities, including EEG, EOG, EMG, ECG, and respiratory signals. By leveraging both raw signal data and handcrafted features extracted from these modalities, our model aims to accurately classify different sleep stages, including Wake, N1, N2, N3, and REM.\n\n\n**Features**\n\n- Integrates multiple physiological signals for sleep stage classification.\n- Utilizes transformer encoder-decoder architecture for efficient information processing and integration.\n- Supports both raw signal data and handcrafted features as input modalities.\n- Provides flexibility in handling various types of physiological data.\n- Facilitates accurate classification of different sleep stages.\n\n\n**Requirements**\n\n- Python 3.x\n- TensorFlow 2.x\n- NumPy\n- Scipy\n- Pandas\n- Scikit-learn\n- Imbalanced-learn\n- Matplotlib\n\n\n\n**Usage**\n\n1. Prepare your dataset: Ensure your dataset is properly formatted and includes the necessary physiological signals (e.g., EEG, EOG, EMG, ECG).\n2. Preprocess the data: Perform any required preprocessing steps, such as segmentation, and normalization on your dataset.\n3. Train and test the model: Use the provided scripts to train and test the model on your dataset.\n\n", "metadata": {"source": "github_readmes\\saharhzm_CrossModalSleepTransformer_README.md", "filename": "saharhzm_CrossModalSleepTransformer_README.md", "type": "readme_full"}}
{"id": "salesforce_LayoutDETR_README.md", "paper_id": "salesforce_LayoutDETR_README", "text": "# LayoutDETR\n\n### [LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer](https://arxiv.org/pdf/2212.09877.pdf)\n[Ning Yu](https://ningyu1991.github.io/), [Chia-Chih Chen](https://scholar.google.com/citations?user=0Hr1SOUAAAAJ&hl=en), [Zeyuan Chen](https://www.linkedin.com/in/zeyuan-chen-0253b6141/), [Rui Meng](http://memray.me/)<br>[Gang Wu](https://www.linkedin.com/in/whoisgang/), [Paul Josel](https://www.linkedin.com/in/paul-josel/), [Juan Carlos Niebles](http://www.niebles.net/), [Caiming Xiong](http://cmxiong.com/), [Ran Xu](https://www.linkedin.com/in/ran-x-a2765924/)<br>\n\nSalesforce Research\n\narXiv 2023\n\n### [paper](https://arxiv.org/pdf/2212.09877.pdf) | [project page](https://ningyu1991.github.io/projects/LayoutDETR.html)\n\n<pre><img src='assets/teaser.png' width=200>\t<img src='assets/framework_architecture.png' width=400></pre>\n<img src='assets/samples_ads_cgl.jpg' width=700></pre>\n\n## Abstract\nGraphic layout designs play an essential role in visual communication. Yet handcrafting layout designs is skill-demanding, time-consuming, and non-scalable to batch production. Generative models emerge to make design automation scalable but it remains non-trivial to produce designs that comply with designers' multimodal desires, i.e., constrained by background images and driven by foreground content. We propose *LayoutDETR* that inherits the high quality and realism from generative modeling, while reformulating content-aware requirements as a detection problem: we learn to detect in a background image the reasonable locations, scales, and spatial relations for multimodal foreground elements in a layout. Our solution sets a new state-of-the-art performance for layout generation on public benchmarks and on our newly-curated ad banner dataset. We integrate our solution into a graphical system that facilitates user studies, and show that users prefer our designs over baselines by significant margins.\n\n## Prerequisites\n- Linux\n- NVIDIA GPU + CUDA 11.3\n- To install conda virtual environment, run\n\t```\n\tconda env create -f environment.yaml\n\tconda activate layoutdetr\n\t```\n- For training, download [Up-DETR pretrained weights](https://drive.google.com/file/d/1JhL1uwNJCaxMrIUx7UzQ3CMCHqmZpCnn/view?usp=sharing) to `pretrained/`.\n- For inference and layout generation in the wild, build Chrome-based text rendering environment by running\n\t```\n\tapt-get update\n\tcp chromedriver /usr/bin/chromedriver\n\tln -fs /usr/share/zoneinfo/America/Los_Angelos /etc/localtime\n\tDEBIAN_FRONTEND=noninteractive apt --assume-yes install ./google-chrome-stable_current_amd64.deb\n\t```\n\n## Data preprocessing\n[Our ad banner dataset](https://drive.google.com/file/d/1T09t4dX7zQ7J-8KxtJv1RkyjRNdilD1m/view?usp=sharing\n) (14.7GB, 7,672 samples). Part of the source images are filtered from [Pitt Image Ads Dataset](https://people.cs.pitt.edu/~kovashka/ads/readme_images.txt) and the others are crawled from Google image search engine with a variety of retailer brands as keywords. Download our dataset and unzip to `data/` which contains three subdirectories:\n- `png_json_gt/` subdirectory contains:\n\t- `*.png` files representing well-designed images with foreground elements superimposed on the background.\n\t- Corresponding `*.json` files with the same file names as of `*.png`, representing the layout ground truth of foreground elements of each well-designed image. Each `*.json` file contains:\n\t\t- `xyxy_word_fit` key: A set of bounding box annotations in the form of `[cy, cx, height, width]`, detected by our [Salesforce Einstein OCR](https://help.salesforce.com/s/articleView?id=release-notes.rn_einstein_vision_ocr_pdf_support.htm&release=230&type=5).\n\t\t- `str` key: Their text contents if any, also recognized by our [Salesforce Einstein OCR](https://help.salesforce.com/s/articleView?id=release-notes.rn_einstein_vision_ocr_pdf_support.htm&release=230&type=5).\n\t\t- `label` key: Their element categories annotated manually through [Amazon Mechanical Turk](https://www.mturk.com/). The interesting categories include {`header`, `pre-header`, `post-header`, `body text`, `disclaimer / footnote`, `button`, `callout`, `logo`}.\n- `1x_inpainted_background_png/` subdirectory correspondingly contains a set of `*_inpainted.png` files representing the background-only images of the well-designed images. The subregions that were superimposed by foreground elements have been inpainted by the [LaMa technique](https://github.com/saic-mdal/lama). These background images should be used for inference or evaluation only, **not for training**.\n- `3x_inpainted_background_png/` subdirectory also correspondingly contains a set of `*_inpainted.png` files representing the background-only images of the well-designed images. There are 2x extra random subregions also inpainted, which aim at avoiding generator being overfitted to inpainted subregions if we inpaint only ground truth layouts. The augmented inpainting subregions serve as false postive which are inpainted but are not ground truth layouts. We use these background images for training.\n\nTo preprocess the dataset that are efficient for training, run\n```\npython dataset_tool.py \\\n--source=data/ads_banner_dataset/png_json_gt \\\n--dest=data/ads_banner_dataset/zip_3x_inpainted \\\n--inpaint-aug\n```\nwhere\n- `--source` indicates the source data direcotry path where you downloaded the raw dataset.\n- `--dest` indicates the preprocessed data direcotry path containing two files: `train.zip` and `val.zip` which are 9:1 splitted from the source data.\n- `inpaint-aug` indicates using `3x_inpainted_background_png/` with extra inpainting on background instead of using `1x_inpainted_background_png/`. Use this argument when preprocessing training data.\n\n## Training\n```\npython train.py --gpus=8 --batch=16 \\\n--data=data/ads_banner_dataset/zip_3x_inpainted/train.zip \\\n--outdir=training-runs \\\n--metrics=layout_fid50k_train,layout_fid50k_val,fid50k_train,fid50k_val,overlap50k_alignment50k_layoutwise_iou50k_layoutwise_docsim50k_train,overlap50k_alignment50k_layoutwise_iou50k_layoutwise_docsim50k_val\n```\nwhere\n- `--batch` indicates the **total batch size** on all the GPUs.\n- `--data` indicates the preprocessed training data .zip file path.\n- `--outdir` indicates the output direcotry path of model checkpoints, result snapshots, config record file, log file, etc.\n- `--metrics` indicates the evaluation metrics measured for each model checkpoint during training, which can include layout FID, image FID, overlap penalty, misalignment penalty, layout-wise IoU, and layout-wise DocSim, etc. See more metric options in `metrics/metric_main.py`.\n- See the definitions and default settings of the other arguments in `train.py`.\n\n## Evaluation\nDownload the **well-trained LayoutDETR model** on our ad banner dataset from [here](https://drive.google.com/file/d/1iaKATX2Id9JnqDunDytVIK5l9HO0a0w-/view?usp=drive_link) (2.7GB) to `checkpoints/`.\n```\npython evaluate.py --gpus=8 --batch=16 \\\n--data=data/ads_banner_dataset/zip_1x_inpainted/val.zip \\\n--outdir=evaluation \\\n--ckpt=checkpoints/layoutdetr_ad_banner.pkl \\\n--metrics=layout_fid50k_val,fid50k_val,overlap50k_alignment50k_layoutwise_iou50k_layoutwise_docsim50k_val,rendering_val\n```\nwhere\n- `--ckpt` indicates the path of the well-trained generator .pkl file.\n- `--metrics=rendering_val` indicates to render texts on background images given generated layouts.\n\n## Layout generation in the wild\n```\npython generate.py \\\n--ckpt=checkpoints/layoutdetr_ad_banner.pkl \\\n--bg='examples/Lumber 2 [header]EVERYTHING 10% OFF[body text]Friends & Family Savings Event[button]SHOP NOW[disclaimer]CODE FRIEND10.jpg' \\\n--bg-preprocessing=256 \\\n--strings='EVERYTHING 10% OFF|Friends & Family Savings Event|SHOP NOW|CODE FRIEND10' \\\n--string-labels='header|body text|button|disclaimer / footnote' \\\n--outfile='examples/output/Lumber 2' \\\n--out-postprocessing=horizontal_center_aligned\n```\nwhere\n- `--ckpt` indicates the path of the well-trained generator .pkl file.\n- `--bg` indicates the provided background image file path.\n- `--bg-preprocessing` indicates the preprocessing operation to the background image. The default is `none`, meaning no preprocessing.\n- `--strings` indicates the ads text strings, the bboxes of which will be generated on top of the background image. Multiple (<10) strings are separated by `|`.\n- `--string-labels` indicates the ads text string labels, selected from {`header`, `pre-header`, `post-header`, `body text`, `disclaimer / footnote`, `button`, `callout`, `logo`}. Multiple (<10) strings are separated by `|`.\n- `--outfile` indicates the output file path and name (without extension).\n- `--out-postprocessing` indicates the postprocessing operation to the output bbox parameters so as to guarantee alignment and remove overlapping. The operation can be selected from {`none`, `horizontal_center_aligned`, `horizontal_left_aligned`}. The default is `none`, meaning no postprocessing.\n- The values of generated bbox parameters [cy, cx, h, w] can be read from the variable `bbox_fake` (in the shape of BxNx4, where B=1 and N=#strings in one ads) in `generate.py`.\n\n## Citation\n  ```\n  @inproceedings{yu2024layoutdetr,\n\t  title={LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer},\n\t  author={Yu, Ning and Chen, Chia-Chih and Chen, Zeyuan and Meng, Rui and Wu, Gang and Josel, Paul and Niebles, Juan Carlos and Xiong, Caiming and Xu, Ran},\n\t  booktitle={European Conference on Computer Vision (ECCV)},\n\t  year={2024}\n  }\n  ```\n\n## Acknowledgement\n- We thank Abigail Kutruff, [Brian Brechbuhl](https://www.linkedin.com/in/brianbrechbuhl), [Elham Etemad](https://ca.linkedin.com/in/elhametemad), and [Amrutha Krishnan](https://www.linkedin.com/in/amruthakrishnan) from Salesforce for constructive advice.\n- We express gratitudes to the [StyleGAN3](https://github.com/NVlabs/stylegan3), [LayoutGAN++](https://github.com/ktrk115/const_layout), [DETR](https://github.com/facebookresearch/detr), [Up-DETR](https://github.com/dddzg/up-detr), and [BLIP](https://github.com/salesforce/BLIP), as our code was modified from their repositories.\n- We also acknowledge the data contribution of [Pitt Image Ads Dataset](https://people.cs.pitt.edu/~kovashka/ads/) and technical contribution of [LaMa](https://github.com/saic-mdal/lama).\n", "metadata": {"source": "github_readmes\\salesforce_LayoutDETR_README.md", "filename": "salesforce_LayoutDETR_README.md", "type": "readme_full"}}
{"id": "schwallergroup_AdsMT_README.md", "paper_id": "schwallergroup_AdsMT_README", "text": "# AdsMT: Multimodal Transformer for Predicting Global Minimum Adsorption Energy\n\n<!-- [![arXiv](https://img.shields.io/badge/arXiv-2312.13136-b31b1b.svg)](https://arxiv.org/abs/2312.13136) -->\n[![DOI](https://img.shields.io/badge/DOI-10.1038/s41467--025--58499--7-blue.svg)](https://www.nature.com/articles/s41467-025-58499-7)\n\nAdsMT is a novel multi-modal transformer to rapidly predict the global minimum adsorption energy (GMAE) of diverse catalyst/adsorbate combinations based on surface graphs and adsorbate feature vectors without any binding information.\n\n<!-- The fast assessment of the global minimum adsorption energy (GMAE) between catalyst surfaces and adsorbates is crucial for large-scale catalyst screening. However, multiple adsorption sites and numerous possible adsorption configurations for each surface/adsorbate combination make it prohibitively expensive to calculate the GMAE through density functional theory (DFT). Thus, we designed a novel multi-modal transformer called AdsMT to rapidly predict the GMAE based on surface graphs and adsorbate feature vectors without any binding information. -->\n<!-- Three diverse benchmark datasets were constructed for this challenging GMAE prediction task. Our AdsMT framework demonstrates excellent performance by adopting the tailored graph encoder and transfer learning, achieving mean absolute errors of 0.09, 0.14, and 0.39 eV, respectively. Beyond GMAE prediction, AdsMT's cross-attention scores showcase the interpretable potential to identify the most energetically favorable adsorption sites. Additionally, uncertainty quantification was integrated into AdsMT to further enhance its trustworthiness in experimental catalyst discovery. -->\n\n\n## 🚀 Environment Setup\n- System requirements: This package requires a standard Linux computer with GPU (supports CUDA >= 11) and enough RAM (> 2 GB). The codes have been tested on NVIDIA RTX 3090, A6000 and A100 GPUs. If you want to run the code on a GPU that does not support CUDA>=11, you need to modify the versions of PyTorch and CUDA in the [env.yml](env.yml) file.\n- We'll use `conda` to install dependencies and set up the environment for a Nvidia GPU machine.\nWe recommend using the [Miniconda installer](https://docs.conda.io/projects/miniconda/en/latest/miniconda-other-installer-links.html).\n- After installing `conda`, install [`mamba`](https://mamba.readthedocs.io/en/latest/) to the base environment. `mamba` is a faster, drop-in replacement for `conda`:\n    ```bash\n    conda install mamba -n base -c conda-forge\n    ```\n- Then create a conda environment and install the dependencies:\n    ```bash\n    mamba env create -f env.yml\n    ```\n    Activate the conda environment with `conda activate adsmt`. It will take about 10 minutes to configure the environment for running code.\n\n## 📌 Datasets\nDataset links: [Zenodo](https://doi.org/10.5281/zenodo.12104162) and [Figshare](https://doi.org/10.6084/m9.figshare.25966573)\n\nWe built three GMAE benchmark datasets named `OCD-GMAE`, `Alloy-GMAE` and `FG-GMAE` from [OC20-Dense](https://doi.org/10.1038/s41524-023-01121-5), [Catalysis Hub](https://doi.org/10.1038/s41597-019-0080-z), and [FG-dataset](https://doi.org/10.1038/s43588-023-00437-y) datasets through strict data cleaning, and each data point represents a unique combination of catalyst surface and adsorbate.\n\n| Dataset | Combination Num. | Surface Num. | Adsorbate Num. | Range of GMAE (eV) |\n|:--------:|:---------:|:----------:|:-----------:|:------:|\n| Alloy-GMAE | 11,260 | 1,916 (37) | 12 (5) | -4.3 $\\sim$ 9.1  |\n| FG-GMAE | 3,308 | 14 (14)| 202 (5) | -4.0 $\\sim$ 0.8  |\n| OCD-GMAE | 973 | 967 (54) | 74 (4) | -8.0 $\\sim$ 6.4  |\n\nNote: The values in brackets represent the numbers of element types.\n\n\nWe can run [`scripts/download_datasets.sh`](scripts/download_datasets.sh) to download all datasets:\n```bash\nbash scripts/download_datasets.sh\n```\n\n## 🔥 Model Training\n\n### 1. Training from scratch\nTo train a AdsMT model with different graph encoder on a dataset by [`scripts/train.sh`](scripts/train.sh) and the following command:\n```bash\nbash scripts/train.sh [DATASET] [GRAPH_ENCODER]\n```\nThis code repo includes 7 different graph encoders:\n[SchNet](https://arxiv.org/abs/1706.08566) (schnet), \n[CGCNN](https://doi.org/10.1103/PhysRevLett.120.145301) (cgcnn), \n[DimeNet++](https://arxiv.org/abs/2011.14115) (dpp),\n[GemNet-OC](https://arxiv.org/abs/2204.02782) (gemnet-oc), \n[TorchMD-NET](https://arxiv.org/abs/2202.02541) (et), \n[eSCN](https://arxiv.org/abs/2302.03655) (escn), \nAdsGT (adsgt, this work). The log file including experiment results will be found in `exp_results/[DATASET]/[GRAPH_ENCODER].log`. It will take 3-24 hours for one task, depending on the dataset and graph encoder.\n\n### 2. Pretraining on the OC20-LMAE dataset\nWe provide scripts for model pretraining on the OC20-LMAE dataset. For example, a AdsMT model with different graph encoders will be pretrained by running:\n```bash\nbash scripts/pretrain_base.sh [GRAPH_ENCODER]\n```\nThe checkpoint file of pretrained model can be found at `checkpoint_dir` in the log file.\n\n### 3. Finetuning on the GMAE datasets\nTo finetune a AdsMT model on a GMAE dataset, you need to change the `ckpt_path` parameter in the model's configuration file (`configs/[DATASET]/finetune/[GRAPH_ENCODER].yml`) to the checkpoint path of your pre-trained model, then run the following command:\n```bash\nbash scripts/finetune.sh [DATASET] [GRAPH_ENCODER]\n```\n\n### 4. Cross-attention scores for adsorption site identification\nThe [`scripts/attn4sites.sh`](scripts/attn4sites.sh) is used to calculate the cross-attention scores of a trained AdsMT model on a GMAE dataset by running:\n```bash\nbash scripts/attn4sites.sh [CONFIG_PATH] [CHECKPOINT_PATH]\n```\nThe output file will be stored at the `results_dir` in the log file.\n\nWe provide a notebook [`visualize/vis_3D.ipynb`](visualize/vis_3D.ipynb) to visualize and compare cross-attention score-colored surfaces with DFT-optimized adsorption configurations under GMAE.\n\n## 🌈 Acknowledgements\nThis work was supported as part of NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation.\n\nThis code repo is based on several existing repositories:\n- [Open Catalyst Project](https://github.com/Open-Catalyst-Project/ocp)\n- [PyTorch Geometric](https://github.com/pyg-team/pytorch_geometric)\n- [PyTorch](https://github.com/pytorch/pytorch)\n\n## 📝 Citation\nIf you find our work useful, please consider citing it:\n```bibtex\n@article{adsmt,\nauthor={Chen, Junwu and Huang, Xu and Hua, Cheng and He, Yulian and Schwaller, Philippe},\ntitle={A multi-modal transformer for predicting global minimum adsorption energy},\njournal={Nature Communications},\nyear={2025},\nvolume={16},\nnumber={1},\npages={3232},\nissn={2041-1723},\ndoi={10.1038/s41467-025-58499-7},\nurl={https://doi.org/10.1038/s41467-025-58499-7}\n}\n```\n\n## 📫 Contact\nIf you have any question, welcome to contact me at:\n\nJunwu Chen: junwu.chen@epfl.ch", "metadata": {"source": "github_readmes\\schwallergroup_AdsMT_README.md", "filename": "schwallergroup_AdsMT_README.md", "type": "readme_full"}}
{"id": "shamanez_Self-Supervised-Embedding-Fusion-Transformer_README.md", "paper_id": "shamanez_Self-Supervised-Embedding-Fusion-Transformer_README", "text": "# [Multimodal Emotion Recognition with Transformer-Based Self Supervised Feature Fusion](https://ieeexplore.ieee.org/document/9206016)\n\n\n![Model Overviw](https://github.com/shamanez/Self-Supervised-Embedding-Fusion-Transformer/blob/master/figure.png)\n\n# ****Please replace the Table 6 in the paper****\n\n[Please replace the Table 6 of the paper with this table.](https://github.com/shamanez/Self-Supervised-Embedding-Fusion-Transformer/blob/master/MOSI.pdf)\n\n# Basic strucutre of the code\n\n## Inspiration from fairseq\n\n1. This code strcuture is built on top of Faiseq interface\n2. Fairseq is an open source project by FacebookAI team that combined different SOTA architectures for sequencial data processing\n3. This also consist of SOTA optimizing mechanisms such as ealry stopage, warup learnign rates, learning rate shedulers\n4. We are trying to develop our own architecture in compatible with fairseq interface. \n5. For more understanding please read the [paper](https://arxiv.org/abs/1904.01038) published about Fairseq interaface.\n\n## Merging of our own architecture with Fairseq interface\n\n1. This can be bit tricky in the beggining. First  it is important to udnestand that Fairseq has built in a way that all architectures can be access through the terminal commands (args).\n\n2. Since our architecture has lot of properties in tranformer architecture, we followed the [a tutorial](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.custom_classification.md) that describe to use Roberta for the custom classification task.\n\n3. We build over archtiecture by inserting new stuff to following directories in Fairseq interfeace.\n   - fairseq/data\n   - fairseq/models\n   - fairseq/modules\n   - fairseq/tasks\n   - fairseq/criterions\n\n\n# Main scripts of the code\n\n## Our main scripts are categorized in to for parts\n\n1. Custom dataloader for load raw audio, faceframes and text is in the **fairseq/data/raw_audio_text_video_dataset.py**\n\n2. The task of the emotion prediction similar to other tasks such as translation is in the **fairseq/tasks/emotion_prediction.py**\n\n3. The custom architecture of our model similar to roberta,wav2vec is in the **fairseq/models/mulT_emo.py**\n\n4. To obtain Inter-Modal attention we modify the self attentional architecture a bit. They can be found in **fairseq/modules/transformer_multi_encoder.py** and  **fairseq/modules/transformer_layer.py**\n\n5. Finally the cutom loss function scripts  cab be found it **fairseq/criterions/emotion_prediction_cri.py**\n\n\n\n# Prerequest models \n\n### Our model uses pretrained SSL methods to extract features. It is important to download those checkpoints prior to the trainig procedure. Please you the following links to downlaod the pretrained SSL models.\n\n1. For audio fetures - [wav2vec](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec) \n2. For facial features - [Fabnet](http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/fabnet.html)\n3. For sentence (text) features - [Roberta](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.md)\n\n\n\n## Training Command\n\n\npython train.py  --data ./T_data-old/mosei_sent    --restore-file None   --task emotion_prediction     --reset-optimizer --reset-dataloader --reset-meters     --init-token 0 --separator-token 2     --arch robertEMO_large     --criterion emotion_prediction_cri     --num-classes 1     --dropout 0.1 --attention-dropout 0.1     --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06     --clip-norm 0.0  --lr 1e-03  --max-epoch 32     --best-checkpoint-metric loss     --encoder-layers 2  --encoder-attention-heads 4 --max-sample-size 150000  --max-tokens 150000000 --batch-size 4  --encoder-layers-cross 2  --max-positions-t 512  --max-positions-a 936 --max-positions-v 301  --no-epoch-checkpoints   --update-freq 2 --find-unused-parameters --ddp-backend=no_c10d --lr-scheduler reduce_lr_on_plateau --regression-target-mos\n\n\n## Validation Command\n\n\nCUDA_VISIBLE_DEVICES=1 python validate.py  --data ./T_data/emocap   --path './checkpoints/checkpoint_best.pt' --task emotion_prediction --valid-subset test --batch-size 4\n", "metadata": {"source": "github_readmes\\shamanez_Self-Supervised-Embedding-Fusion-Transformer_README.md", "filename": "shamanez_Self-Supervised-Embedding-Fusion-Transformer_README.md", "type": "readme_full"}}
{"id": "shijun18_H-DenseFormer_README.md", "paper_id": "shijun18_H-DenseFormer_README", "text": "# MultiModal-Tumor-Seg\nA repo for H-DenseFormer: A Hybrid Densely Connected Transformer for Tumor Segmentation using Multimodal Medical Image.\n", "metadata": {"source": "github_readmes\\shijun18_H-DenseFormer_README.md", "filename": "shijun18_H-DenseFormer_README.md", "type": "readme_full"}}
{"id": "showlab_Show-o_README.md", "paper_id": "showlab_Show-o_README", "text": "<div align=\"center\">\n<br>\n<img src=\"docs/showo_title.png\" width=\"200\">\n<h3>One Single Transformer to Unify Multimodal Understanding and Generation</h3>\n\n[Jinheng Xie](https://sierkinhane.github.io/)<sup>1&#42;</sup>&nbsp;\n[Weijia Mao](https://scholar.google.com/citations?hl=zh-CN&user=S7bGBmkyNtEC&view_op=list_works&sortby=pubdate)<sup>1&#42;</sup>&nbsp;\n[Zechen Bai](https://www.baizechen.site/)<sup>1&#42;</sup>&nbsp;\n[David Junhao Zhang](https://junhaozhang98.github.io/)<sup>1&#42;</sup>&nbsp;\n<br>\nWeihao Wang<sup>2</sup>&nbsp;\n[Kevin Qinghong Lin](https://qinghonglin.github.io/)<sup>1</sup>&nbsp;\n[Yuchao Gu](https://ycgu.site/)<sup>1</sup>\nZhijie Chen<sup>2</sup>&nbsp;\n[Zhenheng Yang](https://scholar.google.com/citations?user=Ds5wwRoAAAAJ&hl=en)<sup>2</sup>&nbsp;\n[Mike Zheng Shou](https://sites.google.com/view/showlab)<sup>1</sup> \n\n<sup>1</sup> [Show Lab](https://sites.google.com/view/showlab/home?authuser=0), National University of Singapore&nbsp; <sup>2</sup> Bytedance&nbsp;\n \n[![ArXiv](https://img.shields.io/badge/ICLR-<OpenReview>-<COLOR>.svg)](https://openreview.net/pdf?id=o6Ynz6OIQ6) [![ArXiv](https://img.shields.io/badge/Arxiv-<2408.12528>-<COLOR>.svg)](https://arxiv.org/pdf/2408.12528) [![Demo](https://img.shields.io/badge/Demo-HuggingFace-<COLOR>.svg)](https://huggingface.co/spaces/showlab/Show-o) [![slack badge](https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp)](https://discord.gg/p6k7XupM) [![WeChat badge](https://img.shields.io/badge/微信-加入-green?logo=wechat&amp)](https://github.com/showlab/Show-o/blob/main/docs/wechat_qa_3.jpg) \n</div>\n\n[//]: # ([![Hits]&#40;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fshowlab%2FShow-o&count_bg=%234DC621&title_bg=%23811AD2&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false&#41;]&#40;https://hits.seeyoufarm.com&#41;)\n<div align=\"center\">\n<br>\n<img src=\"show-o2/docs/showo2.png\" width=\"200\">\n\n<h3>Improved Native Unified Multimodal Models</h3>\n\n[Jinheng Xie](https://sierkinhane.github.io/)<sup>1</sup>&nbsp;\n[Zhenheng Yang](https://scholar.google.com/citations?user=Ds5wwRoAAAAJ&hl=en)<sup>2</sup>&nbsp;\n[Mike Zheng Shou](https://sites.google.com/view/showlab)<sup>1</sup> \n\n<sup>1</sup> [Show Lab](https://sites.google.com/view/showlab/home?authuser=0), National University of Singapore&nbsp; <sup>2</sup> Bytedance&nbsp;\n \n[![ArXiv](https://img.shields.io/badge/Arxiv-<2506.15564>-<COLOR>.svg)](https://arxiv.org/abs/2506.15564) [![ArXiv](https://img.shields.io/badge/Code-<GitHub_Repository>-<COLOR>.svg)](https://github.com/showlab/Show-o/tree/main/show-o2) [![WeChat badge](https://img.shields.io/badge/微信-加入-green?logo=wechat&amp)](https://github.com/showlab/Show-o/blob/main/docs/wechat_qa_3.jpg) \n</div>\n\n\n## News\n* **[2025-09-18]** [Arxiv update](https://arxiv.org/pdf/2506.15564) to include video understanding, OneIG, and more ablation study results.\n* **[2025-09-18]** **Show-o2 has been accepted to NeurIPS 2025.**\n* **[2025-09-05]** Release the 1.5B and 7B models with video understanding capability.\n* **[2025-07-05]** **Fix some issues related to visualization of generated images during training.**\n* **[2025-07-05]** We release the training and inference code for simple mixed-modality generation.\n* **[2025-06-27]** We release the training code for multimodal understanding and generation.\n* **[2025-06-25]** We thank team [OneIG-Bench](https://github.com/OneIG-Bench/OneIG-Benchmark) for evaluating Show-o2 models on their new benchmark, in which our models have achieved leading performance in terms of Alignment and Reasoning metrics. The leaderboard is maintained [here](https://oneig-bench.github.io/).\n\n<img src=\"show-o2/docs/OnelG-Bench.jpg\" width=\"1000\">\n\n* **[2025-06-20]** We are including more concurrent works in our [comparative analysis tables](https://github.com/showlab/Show-o/blob/main/show-o2/docs/comparative_analysis.png). Feel free to reach out to us if we miss your works.\n\n[//]: # (<img src=\"show-o2/docs/comparative_analysis.png\" width=\"1000\">)\n* **[2025-06-19]** We release the [**Show-o2**](https://github.com/showlab/Show-o/tree/main/show-o2) models **with 1.5B and 7B LLM parameters** for multimodal understanding and generation.\nWe perform the unified learning of multimodal understanding and generation on the text token and **3D Causal VAE space**, which is scalable for **text, image, and video modalities**. A dual-path of spatial (-temporal) fusion is proposed to accommodate the distinct feature dependency  of multimodal understanding and generation. We employ specific heads with **autoregressive modeling and flow matching** for the overall unified learning of **multimodal understanding, image/video and mixed-modality generation.**\n<img src=\"show-o2/docs/overview.png\" width=\"1000\">\n<br/>\n\n<img src=\"show-o2/docs/demo3.png\" width=\"1000\">\n\n<table style=\"width:100%;\">\n  <tr>\n    <td style=\"width:50%;\">\n      <img src=\"docs/videos/waves.gif\" style=\"width:100%;\" alt=\"GIF 1\" />\n    </td>\n    <td style=\"width:50%;\">\n      <img src=\"docs/videos/sky.gif\" style=\"width:100%;\" alt=\"GIF 2\" />\n    </td>\n  </tr>\n</table>\n\n<table style=\"width:100%;\">\n  <tr>\n    <td style=\"width:25%;\">\n      <img src=\"show-o2/docs/videos/i2v_3.gif\" style=\"width:100%;\" alt=\"GIF 1\" />\n    </td>\n    <td style=\"width:25%;\">\n      <img src=\"show-o2/docs/videos/i2v_4.gif\" style=\"width:100%;\" alt=\"GIF 2\" />\n    </td>\n    <td style=\"width:25%;\">\n      <img src=\"show-o2/docs/videos/i2v_1.gif\" style=\"width:100%;\" alt=\"GIF 3\" />\n    </td>\n    <td style=\"width:25%;\">\n      <img src=\"show-o2/docs/videos/i2v_2.gif\" style=\"width:100%;\" alt=\"GIF 4\" />\n    </td>\n  </tr>\n</table>\n\n* **[2025-01-23]** **Show-o has been accepted to ICLR 2025.**\n* **[2024-10-15]** Update Arxiv paper to include new features and experimental results.\n  * Support image generation in a resolution of 512x512.\n  <p align=\"center\"> <img src=\"docs/show-o-512x512-t2i.png\" width=\"666\"></p>\n  \n  * Improve the multimodal understanding capabilities of purely discrete Show-o.\n  <p align=\"center\"> <img src=\"docs/show-o-512x512-mmu.png\" width=\"666\"></p>\n  \n  * Improve the performance on the GenEval benchmark.\n  <p align=\"center\"> <img src=\"docs/show-o-geneval.png\" width=\"666\"></p>\n  \n  * Explore the impact of dataset scale and image resolution on multimodal understanding capabilities of discrete image tokens. For more information, please refer to the paper.\n  <p align=\"center\"> <img src=\"docs/show-o-ablation.png\" width=\"666\"></p>\n  \n  * We release [the weight of Show-o](https://huggingface.co/showlab/show-o-512x512-wo-llava-tuning) before fine-tuning on LLaVA instructional tuning datasets. You can fine-tune it following the configurations in `./configs`.\n  \n\n* **[2024-09-12]** Arxiv paper updated to include preliminaries about discrete diffusion.\n* **[2024-09-03]** We deploy an online demo on [Hugging Face Space](https://huggingface.co/spaces/showlab/Show-o). 🤗 Have fun!\n* **[2024-09-02]** **We release the training code for pre-training and instruction tuning!** 🔥🔥\n* **[2024-09-01]** Add [FlexAttention implementation](https://github.com/showlab/Show-o/blob/main/training/omni_attention.py) for accleration. Thanks to [@Horace](https://github.com/Chillee) for providing examples.\n* **[2024-08-28]** We maintain a repo of [Awesome Unified Multimodal Models](https://github.com/showlab/Awesome-Unified-Multimodal-Models). If you are interested in unified models, star and watch it to get latest updates!\n* **[2024-08-27]** Add integration to Hugging Face! Thanks to @[NielsRogge](https://github.com/NielsRogge).\n* **[2024-08-26]** We build two community platforms to facilitate discussion, request and collaboration! Reach us with [Discord](https://discord.gg/p6k7XupM) and [WeChat](https://github.com/showlab/Show-o/blob/main/docs/wechat_qa_3.jpg)!\n* **[2024-08-23]** We release the inference code of Show-o (**1.3B**) for multimodal understanding and generation including image captioning, visual question answering (VQA), text-to-image generation, text-guided inpainting and extrapolation.\n\n## What is the new about Show-o?\nBelow is a characteristics comparison among understanding only, generation only, and unified (understanding \\& generation) models. `Vision` and `Language` indicate the representations from specific input modalities. **In this context, `Diffusion` represents both continuous and discrete diffusion.**\n<p align=\"center\">\n<img src=\"docs/characteristic_comparison.png\" width=\"666\">\n</p>\n\nBelow is an overview of **Show-o**. The input data, regardless of its modalities, is tokenized and then prompted into a formatted input sequence. Show-o processes text tokens autoregressively with causal attention and image tokens in (discrete) denoising diffusion modeling via full attention, and then generates the desired output. Specifically, Show-o is capable of handling image captioning, visual question answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed modality generation.\n\n<img src=\"docs/showo.png\" width=\"1000\">\n\n<br/>\n\n## TODO\n- [X] Release the inference code.\n- [X] Release the training code.\n- [X] Support image generation in a resolution of 512x512.\n- [ ] Scale up the model size (based on LLaMA3) and increase the number of training data.\n\n## Hugging Face models and annotations\nThe Show-o2 checkpoints can be found on Hugging Face:\n* [showlab/show-o2-1.5B](https://huggingface.co/showlab/show-o2-1.5B)\n* [showlab/show-o2-1.5B-HQ](https://huggingface.co/showlab/show-o2-1.5B-HQ) (text-to-image generation in resolutions of 512x512 and 1024x1024 with better text rendering)\n* [showlab/show-o2-7B](https://huggingface.co/showlab/show-o2-7B)\n* [showlab/show-o2-1.5B](https://huggingface.co/showlab/show-o2-1.5B-w-video-und) (further unified fine-tuning on video understanding data)\n* [showlab/show-o2-7B](https://huggingface.co/showlab/show-o2-7B-w-video-und) (further unified fine-tuning on video understanding data)\n\nThe Show-o checkpoints can be found on [Hugging Face](https://huggingface.co/showlab):\n* [showlab/show-o-512x512](https://huggingface.co/showlab/show-o-512x512)\n* [showlab/show-o-w-clip-vit-512x512](https://huggingface.co/showlab/show-o-w-clip-vit-512x512)\n* [showlab/show-o-512x512-wo-llava-tuning](https://huggingface.co/showlab/show-o-512x512-wo-llava-tuning)\n* [showlab/show-o](https://huggingface.co/showlab/show-o)\n* [showlab/show-o-w-clip-vit](https://huggingface.co/showlab/show-o-w-clip-vit)\n* [showlab/magvitv2](https://huggingface.co/showlab/magvitv2)\n* [Journeydb-Annotation](https://huggingface.co/datasets/Sierkinhane/JourneyDB-Annotations )\n\n## Getting Started\nFirst, set up the environment:\n```\npip3 install -r requirements.txt\n```\nLogin your wandb account on your machine or server.\n```\nwandb login <your wandb keys>\n```\nInference demo for **Multimodal Understanding** and you can view the results on wandb.\n```\noption (c)\n\npython3 inference_mmu.py config=configs/showo_demo_w_clip_vit_512x512.yaml \\\nmax_new_tokens=100 \\\nmmu_image_root=./mmu_validation question='Please describe this image in detail. *** Do you think the image is unusual or not?'\n\nor option (a)\n\npython3 inference_mmu.py config=configs/showo_demo_512x512.yaml \\\nmax_new_tokens=100 \\\nmmu_image_root=./mmu_validation question='Please describe this image in detail. *** Do you think the image is unusual or not?'\n```\n<img src=\"docs/github_mmu.png\" width=\"1000\">\n\nInference demo for **Text-to-Image Generation** and you can view the results (in a resolution of 512x512) on wandb.\n```\npython3 inference_t2i.py config=configs/showo_demo_512x512.yaml \\\nbatch_size=1 validation_prompts_file=validation_prompts/showoprompts.txt \\\nguidance_scale=5 generation_timesteps=50 \\\nmode='t2i'\n```\n<img src=\"docs/github_t2i.png\" width=\"1000\">\n\nInference demo for **Text-guided Inpainting** and you can view the results (in a resolution of 256x256) on wandb.\n```\npython3 inference_t2i.py config=configs/showo_demo.yaml \\\nbatch_size=1 \\\nguidance_scale=1.75 generation_timesteps=16 \\\nmode='inpainting' prompt='A blue sports car with sleek curves and tinted windows, parked on a bustling city street.' \\\nimage_path=./inpainting_validation/bus.jpg inpainting_mask_path=./inpainting_validation/bus_mask.webp\n```\n<img src=\"docs/github_inpainting.png\" width=\"1000\">\n\nInference demo for **Text-guided Extrapolation** and you can view the results (in a resolution of 256x256) on wandb.\n```\npython3 inference_t2i.py config=configs/showo_demo.yaml \\\nbatch_size=1 \\\nguidance_scale=1.75 generation_timesteps=16 \\\nmode='extrapolation' extra_direction='left *** left *** left *** right *** right *** right' offset=0 prompt='a serene natural landscape featuring a clear, blue lake surrounded by lush green trees. *** a serene natural landscape featuring a clear, blue lake surrounded by lush green trees. *** a serene natural landscape featuring a clear, blue lake surrounded by lush green trees. *** a serene natural landscape featuring a clear, blue lake surrounded by lush green trees. *** a serene natural landscape featuring a clear, blue lake surrounded by lush green trees. *** a serene natural landscape featuring a clear, blue lake surrounded by lush green trees.' \\\nimage_path=./inpainting_validation/alpine_lake.jpg\n```\n<img src=\"docs/github_extrapolation.png\" width=\"1000\">\n\n## Training pipeline\n**Prepare your training data and change the data path in `configs/xx.yaml`.**\n\nNote that, our training process is based on `accelerate`. Please ensure to config your `accelerate` for distributed training. We provide config examples below for (distributed) training on a single GPU or multiple GPUs.\n```\n├── accelerate_configs/ \n|   ├── multi_nodes (6x8 GPUs)\n|   |   ├—— ...\n|   ├── 1_gpu.yaml\n|   └── 8_gpu_deepspeed_zero2.yaml\n```\nStage 1 - Pre-training on ImageNet-1K dataset. Change the data path to ImageNet-1K in `configs/showo_pretraining_stage1.yaml`. **Note that, we use the internal packages to process the RefinedWeb dataset, and you must manually comment the code part related to language modeling in `training/train.py` or write a new dataloder**.\n```\naccelerate launch --config_file path/to/your/accelerate_config --main_process_port=8888 training/train.py config=configs/showo_pretraining_stage1.yaml\n```\nOnce trained, the `checkpoint` folder is structured as follows:\n```\n├── show-o-training-stage1/ \n|   ├── ...\n|   ├── checkpoint-500000\n|   └── config.yaml\n```\n**A bit cumbersome.** Just create a new output folder (edited in the yaml config) for stage 2, copy the latest `checkpoint` of stage 1 to this folder, and rename it to `checkpoint-0`. It will be automatically resumed for next stage training. **Apply same procedures for the `resume` training in the following stages.**\n```\n├── show-o-training-stage2/ \n|   └── checkpoint-0\n```\nStage 2 - Pre-training on Image-Text dataset. The default dataloader is based on `WebDataset`. Change the data path in `configs/showo_pretraining_stage2.yaml`.\n```\naccelerate launch --config_file path/to/your/accelerate_config --main_process_port=8888 training/train.py config=configs/showo_pretraining_stage2.yaml\n```\nStage 3 - Pre-training on High-quality Image-Text dataset. Change the data path in `configs/showo_pretraining_stage3.yaml`\n\nCopy the pre-trained weights to the `output_dir` (specified in the config)\n```\n├── show-o-training-stage3/ \n|   └── checkpoint-0\n```\n```\naccelerate launch --config_file path/to/your/accelerate_config --main_process_port=8888 training/train.py config=configs/showo_pretraining_stage3.yaml\n```\n[Option a] Stage 3 - Instruction tuning on LLaVA dataset (llava-pretrain). Change the data path in `llava/llava_data_vq_unified.py`.\n```\naccelerate launch --config_file path/to/your/accelerate_config --main_process_port=8888 training/train.py config=configs/showo_instruction_tuning_1.yaml\n```\n[Option a] Stage 3 - Instruction tuning on LLaVA dataset (llava-tuning).  Change the data path in `llava/llava_data_vq_unified.py`.\n```\naccelerate launch --config_file path/to/your/accelerate_config --main_process_port=8888 training/train.py config=configs/showo_instruction_tuning_2.yaml\n```\n[Option c] Stage 3 - Instruction tuning on LLaVA dataset (llava-pretrain) with CLIP-ViT. Change the data path in `llava/llava_pretrain_data.py`.\n```\naccelerate launch --config_file path/to/your/accelerate_config --main_process_port=8888 training/train_w_clip_vit.py config=configs/showo_instruction_tuning_1_w_clip_vit.yaml\n```\n[Option c] Stage 3 - Instruction tuning on LLaVA dataset (llava-tuning) with CLIP-ViT. Change the data path in `llava/llava_instuct_data.py`.\n```\naccelerate launch --config_file path/to/your/accelerate_config --main_process_port=8888 training/train_w_clip_vit.py config=configs/showo_instruction_tuning_2_w_clip_vit.yaml\n```\n\n### Request new features? Willing to contribute?\nWe welcome your bravo new ideas and contributions! If you would like to see any new features in Show-o, or you want to contribute to this project, please fill in [this form](https://docs.google.com/forms/d/e/1FAIpQLSdBlfEWgC2sNBsczyxtzIDE9lJ726ALzyRVn19nc8hJ-ymi2Q/viewform?usp=sf_link)!\n\n**Pending Requested Features**\n- [ ] Mixed-modal generation\n- [ ] Support training on more datasets\n- [ ] Visual tokenizer training\n\nFind more at [Contributing and Roadmap](CONTRIBUTING_ROADMAP.md).\n\n<p align=\"center\">\n<img src=\"docs/show-o-want-u.png\" width=\"512\">\n</p>\n\n### Join Discussion\nWelcome to discuss with us and continuously improve the user experience of Show-o.\nReach us with this [Discord channel](https://discord.gg/p6k7XupM) or the WeChat QR code below!\n<p align=\"center\">\n<img src=\"docs/wechat_qa_3.jpg\" width=\"256\">\n</p> \n\n\n### Citation\nTo cite the paper and model, please use the below:\n```\n@article{xie2024showo,\n  title={Show-o: One Single Transformer to Unify Multimodal Understanding and Generation},\n  author={Xie, Jinheng and Mao, Weijia and Bai, Zechen and Zhang, David Junhao and Wang, Weihao and Lin, Kevin Qinghong and Gu, Yuchao and Chen, Zhijie and Yang, Zhenheng and Shou, Mike Zheng},\n  journal={arXiv preprint arXiv:2408.12528},\n  year={2024}\n}\n\n@article{xie2025showo2,\n  title={Show-o2: Improved Native Unified Multimodal Models},\n  author={Xie, Jinheng and Yang, Zhenheng and Shou, Mike Zheng},\n  journal={arXiv preprint arXiv:2506.15564},\n  year={2025}\n}\n```\n### Acknowledgments\nThis work is heavily based on [open-muse](https://github.com/huggingface/open-muse), [Phi-1.5](https://huggingface.co/microsoft/phi-1_5), [muse-maskgit-pytorch](https://github.com/lucidrains/muse-maskgit-pytorch), [maskgit](https://github.com/google-research/maskgit), [taming-transformers](https://github.com/CompVis/taming-transformers), [transformers](https://github.com/huggingface/transformers), [accelerate](https://github.com/huggingface/accelerate), [diffusers](https://github.com/huggingface/diffusers), and [webdataset](https://github.com/webdataset/webdataset). Thanks to all the authors for their great work.\n", "metadata": {"source": "github_readmes\\showlab_Show-o_README.md", "filename": "showlab_Show-o_README.md", "type": "readme_full"}}
{"id": "SMARTlab-Purdue_PrefMMT_README.md", "paper_id": "SMARTlab-Purdue_PrefMMT_README", "text": "# PrefMMT\n\nThis repository contains the source code for our paper: **\"PrefMMT: Modeling Human Preferences in Preference-based Reinforcement Learning with Multimodal Transformers\"**, submitted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025). For more information, visit our [project website](https://sites.google.com/view/prefmmt).\n\n## Abstract\n\nPreference-based Reinforcement Learning (PbRL) is a promising approach for aligning robot behaviors with human preferences, but its effectiveness relies on accurately modeling those preferences through reward models. Traditional methods often assume preferences are Markovian, neglecting the temporal dependencies within robot behavior trajectories that influence human evaluations. While recent approaches use sequence modeling to learn non-Markovian rewards, they overlook the multimodal nature of robot trajectories, consisting of both state and action elements. This oversight limits their ability to capture the intricate interplay between these modalities, which is critical in shaping human preferences.\n\nIn this work, we introduce **PrefMMT**, a multimodal transformer network designed to disentangle and model the state and action modalities separately. PrefMMT hierarchically leverages intra-modal temporal dependencies and inter-modal state-action interactions to capture complex preference patterns. Our experiments show that PrefMMT consistently outperforms state-of-the-art baselines on locomotion tasks from the D4RL benchmark and manipulation tasks from the Meta-World benchmark.\n\n## Comparison: PrefMMT vs. Other Preference Modeling Methods\n\n<div align=\"center\">\n  <img src=\"/figures/Comparison.jpg\" alt=\"Comparison of PrefMMT with other methods\" width=\"800\"/>\n</div>\n\nThe diagram above highlights the key distinctions between **PrefMMT** and other existing preference modeling methods. While traditional approaches often make Markovian assumptions and fail to capture the multimodal interactions between state and action, **PrefMMT** addresses this gap by leveraging a multimodal transformer architecture. This allows for a more accurate and dynamic understanding of human preferences by modeling both intra-modal and inter-modal dependencies.\n\n## Architecture Overview\n\n<div align=\"center\">\n  <img src=\"/figures/PrefMMTAR.PNG\" alt=\"PrefMMT Architecture\" width=\"400\"/>\n</div>\n\n## Installation\n\n### Requirements\n\n1. Install dependencies:\n\n    ```bash\n    pip install --upgrade pip\n    conda install -y -c conda-forge cudatoolkit=11.1 cudnn=8.2.1\n    pip install -r requirements.txt\n    cd d4rl\n    pip install -e .\n    cd ..\n    ```\n\n2. Install JAX and JAXlib:\n\n    - `jax 0.4.9`\n    - `jaxlib 0.4.9` (Install from [JAX CUDA releases](https://storage.googleapis.com/jax-releases/jax_cuda_releases.html))\n\n\n## Run the code\n\n\n### Run Training Reward Model\n\n```python\nCUDA_VISIBLE_DEVICES=0 python -m JaxPref.main --use_human_label True --comment {experiment_name} --transformer.embd_dim 256 --transformer.n_layer 3 --transformer.n_head 4 --env {D4RL env name} --logging.output_dir './logs/pref_reward' --batch_size 256 --num_query {number of query} --query_len 100 --n_epochs 10000 --skip_flag 0 --seed {seed} --model_type PrefMMT\n```\n\n### Run IQL with learned Reward Model\n\n```python\nCUDA_VISIBLE_DEVICES=0 python train_offline.py --seq_len {sequence length in reward prediction} --comment {experiment_name} --eval_interval {5000: mujoco / 100000: antmaze / 5000: metaworld} --env_name {d4rl env name} --config {configs/(mujoco|antmaze|metaworld)_config.py} --eval_episodes {100 for ant , 10 o.w.} --use_reward_model True --model_type PrefMMT --ckpt_dir {reward_model_path} --seed {seed}\n```\n(The code was tested in Ubuntu 20.04 with Python 3.9.)\n\n\n## Acknowledgments\n\nOur code is based on the implementation of [PT](https://github.com/csmile-1006/PreferenceTransformer), [Flaxmodels](https://github.com/matthias-wright/flaxmodels) and [IQL](https://github.com/ikostrikov/implicit_q_learning). \n", "metadata": {"source": "github_readmes\\SMARTlab-Purdue_PrefMMT_README.md", "filename": "SMARTlab-Purdue_PrefMMT_README.md", "type": "readme_full"}}
{"id": "snap-research_MMVID_README.md", "paper_id": "snap-research_MMVID_README", "text": "## MMVID<br><sub>Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning (CVPR 2022)</sub>\n\n### [Project](https://snap-research.github.io/MMVID/) | [arXiv](https://arxiv.org/abs/2203.02573) | [PDF](https://arxiv.org/pdf/2203.02573.pdf) | [Dataset](#multimodal-voxceleb-dataset)\n\n<div align=\"center\">\n  Generated Videos on Multimodal VoxCeleb\n</div>\n\n<div class=\"gif\">\n<p align=\"center\">\n<img src='images/demo.gif' align=\"center\" width=400>\n</p>\n</div>\n\nThis repo contains the code for training and testing, models, and data for MMVID.\n\n> [**Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning**](https://snap-research.github.io/MMVID/)<br>\n> [Ligong Han](https://phymhan.github.io/), [Jian Ren](https://alanspike.github.io/), [Hsin-Ying Lee](http://hsinyinglee.com/), [Francesco Barbieri](https://fvancesco.github.io/), [Kyle Olszewski](https://kyleolsz.github.io/), [Shervin Minaee](https://sites.google.com/site/shervinminaee/home), [Dimitris Metaxas](https://people.cs.rutgers.edu/~dnm/), [Sergey Tulyakov](http://www.stulyakov.com/)<br>\n> Snap Inc., Rutgers University<br>\n> CVPR 2022\n\n\n## MMVID Code \n\n## CLIP model\nDownload OpenAI's pretrained CLIP model and place it under `./` (or any other directory that is consistent with arg `--openai_clip_model_path`),\n\n```bash\nwget https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\n```\n\n## VQGAN\n\nCode for finetuning VQGAN models is provided in [this repo](https://github.com/phymhan/taming-transformers).\n\n## Multimodal VoxCeleb\n\nFor testing, please download [pretrained models](#pretrained-models) and change the path for `--dalle_path` in the scripts.\n\nFor quantitative evaluation, append `--eval_mode eval` to each testing command. Output log directory can be changed by appending `--name_suffix _fvd` to add suffix (example [here](scripts/mmvoxceleb/text_to_video/evaluation.sh)).\n\n<details>\n  <summary>Text-to-Video</summary>\n  \n  #### Training:\n    bash scripts/mmvoxceleb/text_to_video/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/text_to_video/test.sh\n  #### For Quantitative Evaluation (FVD and PRD):\n    bash scripts/mmvoxceleb/text_to_video/evaluation.sh\n</details>\n\n<details>\n  <summary>Text Augmentation</summary>\n\n  Text augmentation for better training. To enable using a pretrained RoBERTa model, append `--fixed_language_model roberta-large` to the training/testing command. Note that this feature is only *experimental* and is not very robust.\n\n  To enable text dropout, append `--drop_sentence` to the training command. Text dropout is also compatible with using a RoBERTa. We observed that text dropout genrally improves diversity in the generated videos.\n\n  #### Training:\n    bash scripts/mmvoxceleb/text_augement/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/text_augement/test.sh\n</details>\n\n<details>\n  <summary>Text and Mask</summary>\n  \n  #### Training:\n    bash scripts/mmvoxceleb/text_and_mask/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/text_and_mask/test.sh\n  <!-- #### For Quantitative Evaluation (FVD and PRD):\n    To Add -->\n</details>\n\n<details>\n  <summary>Text and Drawing</summary>\n  \n  #### Training:\n    bash scripts/mmvoxceleb/text_and_drawing/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/text_and_drawing/test.sh\n  <!-- #### For Quantitative Evaluation (FVD and PRD):\n    To Add -->\n</details>\n\n<details>\n  <summary>Drawing and Mask</summary>\n  \n  #### Training:\n    bash scripts/mmvoxceleb/drawing_and_mask/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/drawing_and_mask/test.sh\n  <!-- #### For Quantitative Evaluation (FVD and PRD):\n    To Add -->\n</details>\n\n<details>\n  <summary>Image and Mask</summary>\n  \n  #### Training:\n    bash scripts/mmvoxceleb/image_and_mask/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/image_and_mask/test.sh\n  <!-- #### For Quantitative Evaluation (FVD and PRD):\n    To Add -->\n</details>\n\n<details>\n  <summary>Text and Partial Image</summary>\n  \n  #### Training:\n    bash scripts/mmvoxceleb/image_and_mask/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/image_and_mask/test.sh\n  <!-- #### For Quantitative Evaluation (FVD and PRD):\n    To Add -->\n</details>\n\n<details>\n  <summary>Image and Video</summary>\n  \n  #### Training:\n    bash scripts/mmvoxceleb/image_and_mask/train.sh\n  #### Testing:\n    bash scripts/mmvoxceleb/image_and_mask/test.sh\n  <!-- #### For Quantitative Evaluation (FVD and PRD):\n    To Add -->\n</details>\n\n\n## Pretrained Models\n\n\nPretrained models are provided [here](https://drive.google.com/drive/folders/1q_YdEBylrAWeuSleq6Jp58epE3KM-oXK?usp=sharing).\n\n### Multimodal VoxCeleb\n|     | Weight | FVD |\n| --- | :---: | :---: |\n| VQGAN (vae) | [ckpt](https://drive.google.com/file/d/1zaud_h46OUJWMKQtkpwaRvHw5I4_wdpg/view?usp=sharing) | - |\n| VQGAN (cvae for image conditiong) | [ckpt](https://drive.google.com/file/d/1XO_QKsI6H6c0ombHjnpMTwkW0M7f7nJv/view?usp=sharing) | - |\n| Text-to-Video | [pt](https://drive.google.com/file/d/1kBjpLn8Z11w6RqgsNFt1yWUrENb8S1dB/view?usp=sharing) | 59.46 |\n| Text-to-Video (ARTV) | [pt](https://drive.google.com/file/d/1enkF3aquQvi7qgGgk-45iQLjgMNs29Cl/view?usp=sharing) | 70.95 |\n| Text and Mask | [pt](https://drive.google.com/file/d/1EHLcQ4aZ3ZuUOgPvFcNKFzDdZKGTm5rb/view?usp=sharing) | - |\n| Text and Drawing | [pt](https://drive.google.com/file/d/1-kcnX-NY4pX0SEV4It7404yWtG4fCrdr/view?usp=sharing) | - |\n| Drawing and Mask | [pt](https://drive.google.com/file/d/13lMHqVVHUfpVqM4edyc3dKeBSFfUKBuq/view?usp=sharing) | - |\n| Image and Mask | [pt](https://drive.google.com/file/d/1vcq8la7kpJFqdswfX_KuincRNI6o0h3C/view?usp=sharing) | - |\n| Text and Partial Image | [pt](https://drive.google.com/file/d/1wSBm9erN9VP58m3jRQnB_kBCrXW-RGSg/view?usp=sharing) | - |\n| Image and Video | [pt](https://drive.google.com/file/d/1LGYA9i5KRA1L-5DlM9Bubbo9PiH2RqfG/view?usp=sharing) | - |\n| Text-Augmentation | [pt](https://drive.google.com/file/d/1q-r2PO8qSGunG9w9CjFRbbI9pLO1g_s-/view?usp=sharing) | - |\n\n\n## Multimodal VoxCeleb Dataset\n\n[**Multimodal VoxCeleb Dataset**](mm_vox_celeb/README.md) has a total of 19,522 videos with 3,437 various interview situations (453 people). Please see details about how to prepare the dataset in `mm_vox_celeb/README.md`. Preprocessed data is also available [here](https://drive.google.com/drive/folders/18ebgGGTw0610_SRxiu5M3mdJCZqa-O74?usp=sharing).\n\n\n## Acknowledgement\nThis code is heavily based on [DALLE-PyTorch](https://github.com/lucidrains/DALLE-pytorch) and uses [CLIP](https://github.com/openai/CLIP), [Taming Transformer](https://github.com/CompVis/taming-transformers), [Precision Recall Distribution](https://github.com/msmsajjadi/precision-recall-distributions), [Frechet Video Distance](https://github.com/google-research/google-research/tree/master/frechet_video_distance), [Facenet-PyTorch](https://github.com/timesler/facenet-pytorch), [Face Parsing](https://github.com/zllrunning/face-parsing.PyTorch), and [Unpaired Portrait Drawing](https://github.com/yiranran/Unpaired-Portrait-Drawing).\n\nThe authors thank everyone who makes their code and models available.\n\n\n\n\n## Citation\n\nIf our code, data, or models help your work, please cite our [paper](https://arxiv.org/abs/2203.02573):\n```BibTeX\n@inproceedings{han2022show,\ntitle={Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning},\nauthor={Han, Ligong and Ren, Jian and Lee, Hsin-Ying and Barbieri, Francesco and Olszewski, Kyle and Minaee, Shervin and Metaxas, Dimitris and Tulyakov, Sergey},\nbooktitle={CVPR},\nyear={2022}\n}\n```\n", "metadata": {"source": "github_readmes\\snap-research_MMVID_README.md", "filename": "snap-research_MMVID_README.md", "type": "readme_full"}}
{"id": "SQD1_RESTORE-DiT_README.md", "paper_id": "SQD1_RESTORE-DiT_README", "text": "# [RSE 2025] RESTORE-DiT: Reliable satellite image time series reconstruction by multimodal sequential diffusion transformer\n\nRESTORE-DiT is **a novel Diffusion-based framework** for Satellite Image Time Series (SITS) reconstruction. Our work **firstly** promotes the sequence-level optical-SAR fusion through a **diffusion** framework. [Paper](https://www.sciencedirect.com/science/article/pii/S0034425725002767).\n\nInspired by the great success of diffusion models in image and video generation, we approach the time series reconstruction problem from the perspective of **conditional generation**. Conditioned on **SAR image time series** and **date information**, RESTORE-DiT achieves superior reconstruction performance for **highly-dynamic** land surface (e.g. vegetations) under **persist cloud cover** (as shown below).\n\n![Figure 6](https://github.com/user-attachments/assets/7a4e4363-8f6b-44e2-b8f7-0e8f129d4736)\n\n\n## :speech_balloon: Method overview\n\n![Figure 4](https://github.com/user-attachments/assets/bec7e831-037b-49ac-9c5d-702bdd5bf229)\nFig. 1. Structure of RESTORE-DiT framework. The noisy cloudy optical time series is iteratively denoised by Denoising Transformer under the condition of SAR and date.\n\n\n## :speech_balloon: To do list\n- [x] Code and configuration for training and test dataset at France site.\n- [x] Code for Denoising Transformer.\n- [x] Training codes of RESTORE-DiT.\n- [x] Evaluation codes of RESTORE-DiT.\n\n## :speech_balloon: Requirments\n```bash\npip install -r requirements.txt\n```\n\n\n\n\n## :speech_balloon: Data preparation\n\n1. **Dataset download**\n\n    Download the original PASTIS-R dataset [here](https://zenodo.org/records/5735646).\n\n2. **Generate cloud masks**\n\n   The model is trained on cloud-free image time series with simulated masks. Cloud/shadow detection is necessary to remove cloudy images in each time series. [CloudSEN12](https://github.com/cloudsen12) is used for cloud/shadow detection.\n\n   Modify the data folder of PASTIS-R in `CloudDetection.py` and generate the real cloud masks of PASTIS-R dataset. You may need to install necessary packages like segmentation_models_pytorch and geopandas to run `CloudDetection.py`.\n\n3. **Record cloudy frames**\n\n   Based on the obtained cloud masks [T,1,H,W], Indexes of cloudy frames for each sample are recorded in a json file, which will be used for pre-processing in [PASTISDataset.py](https://github.com/SQD1/RESTORE-DiT/blob/main/lib/datasets/PASTISDataset.py). We provide the [json](https://github.com/SQD1/RESTORE-DiT/blob/main/lib/datasets/bad_frames.json) file for the PASTIS-R dataset. You can simply place it to the root of PASTIS-R dataset for training.\n\n## :speech_balloon: Training\n\n    python run_train_PASTIS.py ./configs/config_PASTIS_train.yaml --save_dir ./results/\n\n    \nThis command will create a `./results/START_TIME` path, which saves the training configs and models. The START_TIME is the folder named based on the time you start training, which could be shown as \"2025-06-17_18-00\".\n\n## :speech_balloon: Evaluation\n\n    python run_eval.py config_yaml_path SDT --test-data.test-config ./configs/config_PASTIS_test_simulation.yaml --checkpoint pth_model_path --inference_steps 1\n\nTo use the command above for evaluation on the test set of PASTIS-R, YOU NEED TO:\n1. Replace the `config_yaml_path` to your specific config.yaml path in `results` folder, which could be like `./results/2025-06-17_18-00/config.yaml`. \n2. Replace the `pth_model_path` to your specific saved model path in `results` folder, which could be like `./results/2025-06-17_18-00/checkpoints/Model_best.pth`.\n\n\n## :speech_balloon: Citation \n\nIf you find our method useful in your research, please cite with:\n\n```\n@ARTICLE{RESTORE-DiT,\n  author={Shu, Qidi and Zhu, Xiaolin and Xu, Shuai and Wang, Yan and Liu, Denghong},\n  journal={Remote Sensing of Environment}, \n  title={RESTORE-DiT: Reliable satellite image time series reconstruction by multimodal sequential diffusion transformer}, \n  year={2025},\n  volume={328},\n  number={114872},\n}\n```\n\n\n## Acknowledgements\n\nThanks for these excellent works: [U-TILISE](https://github.com/prs-eth/U-TILISE), [VDT](https://github.com/RERV/VDT), [DiT](https://github.com/facebookresearch/DiT), [DiffCR](https://github.com/XavierJiezou/DiffCR), [PASTIS-R](https://github.com/VSainteuf/pastis-benchmark).\n\n", "metadata": {"source": "github_readmes\\SQD1_RESTORE-DiT_README.md", "filename": "SQD1_RESTORE-DiT_README.md", "type": "readme_full"}}
{"id": "sunlicai_EMT-DLFR_README.md", "paper_id": "sunlicai_EMT-DLFR_README", "text": "# EMT-DLFR\n> **Efficient Multimodal Transformer with Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis (TAC 2023)**<br>\n> [[arXiv]](https://arxiv.org/abs/2208.07589), [[IEEE]](https://ieeexplore.ieee.org/document/10122560) <br>\n> [Licai Sun](https://scholar.google.com/citations?user=7qo_cTcAAAAJ&hl=en&oi=ao), [Zheng Lian](https://scholar.google.com/citations?user=S34nWz0AAAAJ&hl=en), [Bin Liu](https://scholar.google.com/citations?user=UEB_5QEAAAAJ&hl=en), and [Jianhua Tao](https://scholar.google.com/citations?user=781jbHMAAAAJ&hl=en)<br>\n> University of Chinese Academy of Sciences & Institute of Automation, Chinese Academy of Sciences & Tsinghua University<br>\n\n## News\n**[2024.07]** EMT-DLFR is selected as an [ESI Highly Cited Paper](https://www.webofscience.com/wos/woscc/full-record/WOS:001178971100010).<br>\n**[2023.11]** We upload the [poster](figs/EMT-DLFR_CCAC2023.pdf) of EMT-DLFR for CCAC 2023.<br>\n\n## Overview\n\n![EMT-DLFR Framework](figs/EMT-DLFR_Architecture.png)\n\nIn this paper, we aims to tackle two major challenges in robust multimodal sentiment analysis (MSA): \n- *inefficiency* when modeling cross-modal interactions in unaligned multimodal data.\n- *vulnerability* to random modality feature missing which typically occurs in realistic settings.\n\nTo this end, we propose Efficient Multimodal Transformer with Dual-Level Feature Restoration (EMT-DLFR):\n- Efficient Multimodal Transformer (EMT). EMT introduces the *global multimodal context* and \nemploys it to interact with *local unimodal features* to enable efficient *global-local* \ncross-modal interaction, which not only avoids the quadratic scaling cost of previous \n*local-local* interaction methods (e.g., [MulT](https://github.com/yaohungt/Multimodal-Transformer), [TFR-Net](https://github.com/thuiar/TFR-Net), and [PMR](https://openaccess.thecvf.com/content/CVPR2021/html/Lv_Progressive_Modality_Reinforcement_for_Human_Multimodal_Emotion_Recognition_From_Unaligned_CVPR_2021_paper.html)) \nbut also leads to performance gains.\n- Dual-Level Feature Restoration (DLFR). Unlike the standalone *implicit low-level \nfeature reconstruction* in [TFR-Net](https://github.com/thuiar/TFR-Net), DLFR combines \nboth *implicit low-level feature reconstruction* and *explicit high-level feature attraction* \nto more effectively guide EMT to achieve robust representation learning from incomplete multimodal data.\n\n## Prerequisites\n* `Python 3.8`\n* `PyTorch 1.7.1`\n* `transformers==4.29.0`\n* `einops==0.6.0`\n* `scikit-learn=1.1.3`\n* `pandas==1.5.1`\n* `numpy=1.23.4`\n\nPlease refer to [environment.yml](environment.yml) for more details.\n\n\n## Data Preparation\n- We conduct experiments on three popular MSA datasets, including [CMU-MOSI](https://ieeexplore.ieee.org/abstract/document/7742221/), [CMU-MOSEI](https://aclanthology.org/P18-1208/), and [CH-SIMS](https://aclanthology.org/2020.acl-main.343/).\n- Download pre-pocessed dataset features and pre-trained bert checkpoints from [Baidu Cloud Drive](https://pan.baidu.com/s/1oksuDEkkd3vGg2oBMBxiVw) (code: `ctgs`) or [Google Cloud Drive](https://drive.google.com/drive/folders/1E5kojBirtd5VbfHsFp6FYWkQunk73Nsv?usp=sharing) (Credits: [Self-MM](https://github.com/thuiar/Self-MM)).\nAssume your data structure to be like this (using `sha1sum` command to verify SHA-1 hash value in the parenthesis):\n```\n|MMSA\n    |MOSI\n        |Processed\n            |unaligned_50.pkl (5da0b8440fc5a7c3a457859af27458beb993e088)\n    |MOSEI\n        |Processed\n            |unaligned_50.pkl (db3e2cff4d706a88ee156981c2100975513d4610)\n    |SIMS\n        |Processed\n            |unaligned_39.pkl (a00c73e92f66896403c09dbad63e242d5af756f8)\n```\n\n- Go to [config/get_data_root.py](config/get_data_root.py) and change the data root to your own path.\n\n- For SIMS, you need to use the following script to generate the normalized version (only normalize audio and visual features).\n\n```\npython preprocess/normalize_sims.py\n```\n\n## Run\n- CMU-MOSI\n```\nsh scripts/mosi/run_once.sh 0\nor\nsh scripts/mosi/run.sh 0\n```\n'run_once.sh': for running across all missing rates (i.e., 0.0, 0.1, ..., 1.0).\n\n'run.sh': only run a single specified missing rate (e.g., 0.5).\n\n- CMU-MOSEI\n```\nsh scripts/mosei/run_once.sh 0\n```\n- CMU-MOSEI\n```\nsh scripts/sims/run_once.sh 0\n```\n\n## Results\n\n- CMU-MOSI & CMU-MOSEI\n\n![mosi_mosei_incomplete_table](figs/mosi_mosei_incomplete_table.png)\n\n![mosi_mosei_incomplete_fig](figs/mosi_mosei_incomplete_fig.png)\n\n\n- CH-SIMS\n<p align=\"center\">\n<img src=\"figs/sims_incomplete_table.png\" width=\"50%\" alt=\"\" />\n\n![sims_incomplete_fig](figs/sims_incomplete_fig.png)\n\n- Note: since these datasets are relatively small and there exists the randomness \ncaused by different software & hardware settings, the reproduced results might \nslightly better or worse than those reports in the paper. You can run multiple times \nand tweak the hyperparameters to obtain more better results.\n\n## Further Exploration\nOur conversational version for incomplete multimodal learning is open-sourced at [GCNet](https://github.com/zeroQiaoba/GCNet).\n\n## Acknowledgements\nThis project is built upon [TFR-Net](https://github.com/thuiar/TFR-Net) and [MMSA](https://github.com/thuiar/MMSA). Thanks for the nice codebases.\n\n## Citation\n\nIf you think this project is helpful, please feel free to leave a star and cite our paper:\n\n\n```  \n@article{sun2023efficient,\n  title={Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis},\n  author={Sun, Licai and Lian, Zheng and Liu, Bin and Tao, Jianhua},\n  journal={IEEE Transactions on Affective Computing},\n  year={2023},\n  publisher={IEEE}\n}\n```\n\n## Contact \nIf you have any questions, feel free to contact us.\n\nLicai Sun: licai.sun@oulu.fi, or Zheng Lian: lianzheng2016@ia.ac.cn\n", "metadata": {"source": "github_readmes\\sunlicai_EMT-DLFR_README.md", "filename": "sunlicai_EMT-DLFR_README.md", "type": "readme_full"}}
{"id": "ThreePoundUniverse_TSMMF-ESWA_README.md", "paper_id": "ThreePoundUniverse_TSMMF-ESWA_README", "text": "# TSMMF-ESWA\nThis is the official implementation of ESWA - A bidirectional cross-modal transformer representation learning model for EEG-fNIRS multimodal affective BCI\n\n# Abstract\nBy recognizing or regulating human emotions, the affective brain-computer interfaces (BCIs) could improve human-computer interactions. However, human emotion involves complex temporal-spatial brain networks. Therefore, unimodal brain imaging methods have difficulty to decode complex human emotions. Multimodal brain imaging methods, which capture temporal-spatial multi-dimensional brain signals, have been successfully employed in non-affective BCIs, showing extreme potential to improve the affective BCIs. In order to explore a multimodal fusion model with interpretability and improve emotion recognition performance for multimodal affective BCIs. In this study, we propose a Temporal-Spatial Multimodal Fusion (TSMMF) model, which leverages the bidirectional Cross-Modal Transformer (BCMT) to fuse electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) multimodal brain signals. Firstly, intra-modal feature extractors and the Self-Attention Transformer were employed to construct joint EEG-fNIRS multimodal representations, reducing inter-modal differences. Secondly, the BCMT was adopted to achieve temporal-spatial multimodal fusion, followed by attention fusion to adaptively adjust the weights of the temporal-spatial multimodal features. Thirdly, modality-specific branches were introduced to preserve the unique features of each modality, then the outputs of all branches were weighted sum for emotion recognition. Furthermore, the model learned the weights of emotion-related brain regions for different modalities. Results showed that: (1) We proposed the first affective BCI based on multimodal brain imaging methods and the emotion recognition outperformed the state-of-the-art methods. (2) An accuracy of 76.15\\% was achieved for cross-subject emotion decoding, representing improvements of 6.06\\% and 12.44\\% compared to EEG and fNIRS unimodal approaches, respectively. (3) The spatial interpretability indicated that: compared to modality-specific branches focusing on common brain regions, whereas the multimodal fusion branch emphasizes differential brain regions related to different emotions. Collectively, our method, inspired by neuroscience, could enhance the development of BCI and multimodal brain signals decoding.\n\n![image](https://github.com/user-attachments/assets/9ca816f6-3e56-41c2-99a6-d485cf1c65eb)\n\n# data\nThe data is waiting to be open sourced. If you need it, please contact tjzhangshuai@tju.edu.cn. We will provide sample data of 3 subjects.\n\n# contact\nIf you have any bugs or questions please contact tjzhangshuai@tju.edu.cn\n", "metadata": {"source": "github_readmes\\ThreePoundUniverse_TSMMF-ESWA_README.md", "filename": "ThreePoundUniverse_TSMMF-ESWA_README.md", "type": "readme_full"}}
{"id": "thuiar_TFR-Net_README.md", "paper_id": "thuiar_TFR-Net_README", "text": "![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg)\n\n> This repository contains the official implementation code of the paper Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis, accepted at ACMMM 2021.\n\n**Note:** We strongly recommend that you browse the overall structure of our code at first. If you have any question, feel free to contact us.\n\n## Support Models\n\nIn this framework, we support the following methods:\n\n|     Type    |   Model Name      |     From                |\n|:-----------:|:----------------:|:------------------------:|\n| Baselines |[TFN](models/singleTask/TFN.py)|[Tensor-Fusion-Network](https://github.com/A2Zadeh/TensorFusionNetwork)|\n| Baselines |[MulT](models/singleTask/MulT.py)(without CTC) |[Multimodal-Transformer](https://github.com/yaohungt/Multimodal-Transformer)|\n| Baselines |[MISA](models/singleTask/MISA.py) |[MISA](https://github.com/declare-lab/MISA)|\n| Missing-Task  |[TFR-Net](models/missingTask/TFR_NET)|      [TFR-Net](https://github.com/Columbine21/TFR-Net)  |\n\n## Usage\n\n\n- Clone this repo and install requirements.\n```\ngit clone https://github.com/Columbine21/TFR-Net.git\ncd TFR-Net\n```\n\n### Data Preprocessing\n\n1. Download datasets from the following links.\n\n- MOSI\n> download from [CMU-MultimodalSDK](http://immortal.multicomp.cs.cmu.edu/raw_datasets/processed_data/)\n\n- SIMS\n> download from [Baidu Yun Disk](https://pan.baidu.com/share/init?surl=XmobKHUqnXciAm7hfnj2gg) [code: `mfet`] or [Google Drive](https://drive.google.com/drive/folders/1A2S4pqCHryGmiqnNSPLv7rEg63WvjCSk)  \n> **Notes:** Please download new features `unaligned_39.pkl` from [Baidu Yun Disk](https://pan.baidu.com/share/init?surl=XmobKHUqnXciAm7hfnj2gg) [code: `mfet`] or [Google Drive](https://drive.google.com/drive/folders/1A2S4pqCHryGmiqnNSPLv7rEg63WvjCSk), which is compatible with our new code structure. The `md5 code` is `a5b2ed3844200c7fb3b8ddc750b77feb`.\n\n1. Download [Bert-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) from [Google-Bert](https://github.com/google-research/bert).  \n\n2. Convert Tensorflow into pytorch using [transformers-cli](https://huggingface.co/transformers/converting_tensorflow_models.html)  \n\n3. Install python dependencies\n\n4. Organize features and save them as pickle files with the following structure.\n\n> **Notes:** `unaligned_39.pkl` is compatible with the following structure\n\n###### Dataset Feature Structure\n\n```python\n{\n    \"train\": {\n        \"raw_text\": [],\n        \"audio\": [],\n        \"vision\": [],\n        \"id\": [], # [video_id$_$clip_id, ..., ...]\n        \"text\": [],\n        \"text_bert\": [],\n        \"audio_lengths\": [],\n        \"vision_lengths\": [],\n        \"annotations\": [],\n        \"classification_labels\": [], # Negative(< 0), Neutral(0), Positive(> 0)\n        \"regression_labels\": []\n    },\n    \"valid\": {***}, # same as the \"train\" \n    \"test\": {***}, # same as the \"train\"\n}\n```\n\n5. Modify `config/config_regression.py` to update dataset pathes.\n\n\n### Run\n\n```\nsh test.sh\n```\n\n## Paper\n\n- [CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality](https://www.aclweb.org/anthology/2020.acl-main.343/)\n- [Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis](https://dl.acm.org/doi/pdf/10.1145/3474085.3475585?casa_token=-wxKWlUW7LkAAAAA:ebkynOJtEO-2T49_kkPj5gc-AvHKAfPKkzbR9Vu1Z8pLS6ht3rWORg04JjV4ACbUhuZVbDmjIgcdqQ)\n\nPlease cite our paper if you find our work useful for your research:\n\n```\n@inproceedings{yu2020ch,\n  title={CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality},\n  author={Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng},\n  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  pages={3718--3727},\n  year={2020}\n}\n```\n\n```\n@inproceedings{yuan2021transformer,\n  title={Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis},\n  author={Yuan, Ziqi and Li, Wei and Xu, Hua and Yu, Wenmeng},\n  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},\n  pages={4400--4407},\n  year={2021}\n}\n```", "metadata": {"source": "github_readmes\\thuiar_TFR-Net_README.md", "filename": "thuiar_TFR-Net_README.md", "type": "readme_full"}}
{"id": "TIAN-viola_DynRT_README.md", "paper_id": "TIAN-viola_DynRT_README", "text": "# DynRT\nThis repository contains (PyTorch) code and pre-trained models for Dynamic Routing Transformer Network (DynRT), described by the paper *Dynamic Routing Transformer Network for Multimodal Sarcasm Detection* accepted by ACL 2023.\n## Quick links\n- [DynRT](#dynrt)\n  - [Quick links](#quick-links)\n  - [Overview](#overview)\n  - [Setup](#setup)\n    - [Install dependencies](#install-dependencies)\n    - [Datasets](#datasets)\n    - [Pretrained model](#pretrained-model)\n  - [Model](#model)\n    - [Train/evaluate the model](#trainevaluate-the-model)\n  - [CheckList](#checklist)\n  - [Citation](#citation)\n  - [Acknowledgement](#acknowledgement)\n\n## Overview\n![](./figs/overview.png)\nIn this work, we present a novel method for multimodal sarcasm detection. \nPlease find more details of this work in our paper.\n## Setup\n\n### Install dependencies\nPlease install all the dependency packages using the following command:\n```\npip install -r requirements.txt\n```\n### Datasets\nOur experiments are based on Multimodal Sarcasm Detection (MSD) dataset. Please refer to our paper for more details about this datasets. The file size of image data is too large to submit as supplementary materials. \n\nPlease download the image data from [data-of-multimodal-sarcasm-detection](https://github.com/headacheboy/data-of-multimodal-sarcasm-detection) and put all the images in the folder `dataset_image`. \n\nThe text data and corresponding labels before preprocessing are in the folder `input/prepared`, which are the same with \n[data-of-multimodal-sarcasm-detection/text](https://github.com/headacheboy/data-of-multimodal-sarcasm-detection/tree/master/text). We follow [data-of-multimodal-sarcasm-detection/LoadData.py](https://github.com/headacheboy/data-of-multimodal-sarcasm-detection/blob/f42b16510208624d91fa545ca9bb64c6335f971e/codes/loadData.py#L80) to remove easy samples with regular words (e.g. humor, sarcasm, etc.) as all previous studies do. Please run `clean_dataset.py` to get preprocessed dataset in the folder `input/prepared_clean`. To save storage space, all the text data and corresponding labels are saved as binary files. To read these binary files, please use the following function:\n\n```\ndef load_file(filename):\n    with open(filename, 'rb') as filehandle:\n        ret = pickle.load(filehandle)\n        return ret\n```\n\n\nWe preprocess the image and convert the image to a numpy array in order to save training time.  The numpy array file of the image will be saved in the fold `image_tensor/`. Please run the following command:\n```\npython convert_image_to_tensor_save.py\n```\n\n### Pretrained model\nDownload the pre-trained model roberta-base and corresponding files from \n[roberta-base](https://huggingface.co/roberta-base/). Put these files in `roberta-base/` folder.\n\n\n## Model\n\n### Train/evaluate the model\nThe parameter configuration files for training and testing are in the fold `config/`.\nYou can use `train.py` to train a DynRT model. A command template is as follows:\n```bash\nCUDA_VISIBLE_DEVICES=0 python train.py {path of the parameter configuration file} \\\n```\nYou can use `test.py` to evaluate an existing model. Please fill the model path as a value for the key  `test_on_checkpoint` in the config file. You can find our checkpoint file from the [ACL23-DynRT](https://drive.google.com/drive/folders/1sV9r-dlESCOeD2xsnpkd_lmgL_4MlT8U?usp=share_link). A command template is as follows:\n```bash\nCUDA_VISIBLE_DEVICES=0 python test.py {path of the parameter configuration file} \\\n```\n\nThe experimental results will be stored in a subfolder of the folder `exp/{date-time}/`. In this fold, `log.txt` is the log file, JSON file is the parameter configuration file. \n\nThe configuration files are in the folder `config/`. A configuration file template for training is as follows:\n```bash\n{\n    \"info\": {\n        # The name of the model\n        \"name\": \"DynRT\",\n        # The config of the log\n        \"log\": {\n            \"name\": \"\"\n        },\n        # The device id of the GPU\n        \"device\": [\n            0\n        ],\n        # If test on checkpoint model, fill in the model file path, otherwise, fill in the \"none\"\n        \"test_on_checkpoint\": \"none\",\n        # If continue to train on the checkpoint model, fill in the model file path, otherwise, fill in the \"none\"\n        \"train_on_checkpoint\": \"none\"\n    },\n    \"opt\": {\n        \"seed\": 2,\n        # the information of data, including text, image and label\n        \"dataloader\": { \n            \"requires\": {\n                \"tokenizer_roberta\": {\n                    # the path of Roberta\n                    \"path\": \"pretrained_models/roberta-base\"\n                }\n            },\n            \"loaders\": {\n                \"text\": {\n                    # The path of text data\n                    \"data_path\": \"input/prepared_clean/\",\n                    # The max length of text\n                    \"len\": 100,\n                    # The pad index in the pretrained model\n                    \"pad\": 1\n                },\n                \"img\": {\n                    \"data_path\": \"input/prepared_clean/\",\n                    # The path of the image tensor\n                    \"transform_image\": \"image_tensor/\"\n                },\n                \"label\": {\n                    \"data_path\": \"input/prepared_clean/\",\n                    \"test_label\": true\n                }\n            },\n            \"batch_size\": 32,\n            \"pin_memory\": true,\n            \"num_workers\": 0,\n            \"shuffle\": true,\n            \"drop_last\": false\n        },\n        \"mode\": [\n            \"train\",\n            \"valid\",\n            \"test\"\n        ],\n        \"checkpoint_step\": 2,\n        \"lr_decay_list\": [\n            20,\n            25,\n            30\n        ],\n        \"lr_decay_r\": 0.8,\n        \"modelopt\": {\n            \"name\": \"DynRT\",\n            \"input1\": \"text\",\n            \"input2\": \"img\",\n            \"input3\": \"text_mask\",\n            \"layer\": 4,\n            \"tau_max\": 10,\n            # the order of masked sliding windows in our paper\n            \"ORDERS\": [\n                0,\n                1,\n                2,\n                3\n            ],\n            \"IMG_SCALE\": 7,\n            \"dropout\": 0.5,\n            \"hidden_size\": 768,\n            \"ffn_size\": 768,\n            # The head of multi-head attention\n            \"multihead\": 2,\n            # the method for routing, \"hard\" for gumble softmax, \"mean\" for average probability \n            \"routing\": \"hard\",\n            \"BINARIZE\": false,\n            \"len\": 100,\n            \"glimpses\": 1,\n            \"mlp_size\": 768,\n            \"output_size\": 768,\n            # The order of ORDERS\n            \"orders\": 4,\n            \"pooling\": \"avg\",\n            \"classifier\": \"both\",\n            \"roberta_path\": \"roberta-base/\",\n            \"roberta_layer\": 1,\n            \"vitmodel\": \"vit_base_patch32_224\",\n            \"finetune\": false\n        },\n        \"optimizeropt\": {\n            \"name\": \"Adam\",\n            \"lr\": 1e-06,\n            \"weight_decay\": 0.01,\n            \"params\": {\n                \"bertl_text\": {\n                    \"lr\": 3e-7\n                },\n                \"vit\": {\n                    \"lr\": 3e-7,\n                    \"weight_decay\": 0.01\n                },\n                \"trar\": {\n                    \"lr\": 1e-06,\n                    \"weight_decay\": 0.01\n                },\n                \"classifier\": {}\n            }\n        },\n        \"lossopt\": {\n            \"name\": \"CrossEntropyLoss\"\n        },\n        \"total_epoch\": 15,\n        # The value of gradient clipping\n        \"clip\": 10\n    }\n}\n\n```\n\n\n\n## CheckList \n\nWe train our model on GeForce RTX 2080 Ti GPUs.\n\nWe take the average results of  5 runs for reports.\n\n## Citation\n\nIf you find this repo useful for your research, please consider citing our paper.\n\n```\n@inproceedings{dynrt2023,\n  author    = {Tian, Yuan and\n               Xu, Nan and\n               Zhang, Ruike and\n               Mao, Wenji},\n  title     = {Dynamic Routing Transformer Network for Multimodal Sarcasm Detection},\n  booktitle = {Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics},\n  publisher = {Association for Computational Linguistics},\n  pages = {2468--2480},\n  year      = {2023}\n}\n```\n## Acknowledgements\n\nThanks for the dataset from https://github.com/headacheboy/data-of-multimodal-sarcasm-detection\n\nThanks for the RoBERTa model from https://huggingface.co/roberta-base/\n\nThanks for the TRAR from https://github.com/rentainhe/TRAR-VQA\n\n", "metadata": {"source": "github_readmes\\TIAN-viola_DynRT_README.md", "filename": "TIAN-viola_DynRT_README.md", "type": "readme_full"}}
{"id": "tthinking_MATR_README.md", "paper_id": "tthinking_MATR_README", "text": "# MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer (IEEE TIP 2022).\n\nThis is the official implementation of the MATR model proposed in the paper ([MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer](https://ieeexplore.ieee.org/document/9844446)) with Pytorch.\n\n<h1 dir=\"auto\"><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\" href=\"#requirements\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Requirements</h1>\n<ul dir=\"auto\">\n<li>CUDA 11.4</li>\n<li>conda 4.10.1</li>\n<li>Python 3.8.12</li>\n<li>PyTorch 1.9.1</li>\n<li>timm 0.4.12</li>\n<li>tqdm</li>\n<li>glob</li>\n<li>pandas</li>\n</ul>\n\n# Tips:\n<strong>Dealing with RGB input:</strong>\nRefer to [DPCN-Fusion](https://github.com/tthinking/DPCN-Fusion/blob/master).\n\n<strong>Dataset is </strong> [here](http://www.med.harvard.edu/AANLIB/home.html).\n\nThe code for <strong>evaluation metrics</strong> is [here](https://github.com/tthinking/MATR/tree/main/evaluation).\n\n\n# Cite the paper\nIf this work is helpful to you, please cite it as:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"@ARTICLE{Tang_2022_MATR,\n  author={Tang, Wei and He, Fazhi and Liu, Yu and Duan, Yansong},\n  journal={IEEE Transactions on Image Processing}, \n  title={MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer}, \n  year={2022},\n  volume={31},\n  number={},\n  pages={5134-5149},\n  doi={10.1109/TIP.2022.3193288}}\"><pre class=\"notranslate\"><code>@ARTICLE{Tang_2022_MATR,\n  author={Tang, Wei and He, Fazhi and Liu, Yu and Duan, Yansong},\n  journal={IEEE Transactions on Image Processing}, \n  title={MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer}, \n  year={2022},\n  volume={31},\n  number={},\n  pages={5134-5149},\n  doi={10.1109/TIP.2022.3193288}}\n</code></pre></div>\n\nIf you have any questions,  feel free to contact me (<a href=\"mailto:weitang2021@whu.edu.cn\">weitang2021@whu.edu.cn</a>).\n", "metadata": {"source": "github_readmes\\tthinking_MATR_README.md", "filename": "tthinking_MATR_README.md", "type": "readme_full"}}
{"id": "uakarsh_latr_README.md", "paper_id": "uakarsh_latr_README", "text": "# LaTr - PyTorch\n\n![latr architecture](images/latr-architecture.jpg)\n\nImplementation of [LaTr: Layout-aware transformer for scene-text VQA](https://arxiv.org/abs/2112.12494),a novel multimodal architecture for Scene Text Visual Question Answering (STVQA).\n\nLaTr improves robustness towards OCR errors, a common reason for failure cases in STVQA. In addition, by leveraging a vision transformer, LaTr eliminate the need for an external object detector. LaTr outperforms state-of-the-art STVQA methods on multiple datasets. In particular, +7.6% on TextVQA, +10.8% on ST-VQA and +4.0% on OCR-VQA (all absolute accuracy numbers).\n\nThe official implementation was not released by the authors.\n\n\nNOTE: I have tried my best to implement this paper, and have taken minimum assumptions while implementing, but, one of the essential part of any implementation is to provide pre-trained weights and show the results of your implementation on the dataset mentioned in the paper, however due to resource limitation from my side, I won't be able to provide pre-trained weights. However, I would try to include scripts in the example, so that if someone has the resources, they can use the scripts to obtain pre-trained weights and share it. Open to all feedbacks, and hope this implementation turns out to be useful to the community.\n\n## Demo\n![latr architecture](images/demo.gif)\n\nAn interactive demo for the same can be found out [here](https://huggingface.co/spaces/iakarshu/latr-vqa)\n\n## Install\n\n```python\npip install transformers\npip install sentencepiece==0.1.91\npip install pytesseract\nsudo apt install tesseract-ocr\npip install 'Pillow==7.1.2'\n```\n\n## Usage\n\n* For pre-training task: Refer [here](https://github.com/uakarsh/latr/blob/main/examples/LaTr_PreTraining.ipynb)\n* The training of LaTr from scratch with PyTorch Lightening can be referred [here](https://github.com/uakarsh/latr/tree/main/examples/textvqa)\n\n\n## Results:\n\nCurrently, I used the following configurations:\n\n```python\nclasses : 32128\nhidden_state: 768\nmax_2d_position_embeddings: 1001\nseq_len: 512\nt5_model: \"t5-base\"\nvocab_size: 32128\nhidden_state: 768\nlearning_rate: 0.0001\nmax_2d_position_embeddings: 1001\nmax_steps: 50000\nseq_len: 512\nt5_model: \"t5-base\"\nvocab_size: 32128\nbatch size: 1  (I think, this is a major difference between the training of mine and authors)\n```\n\nAnd have been able to obtain a validation accuracy of: 27.42 percentage (authors were able to achieve 44.03 percentage)\n\n* The results of all the experiments can be found out [here](https://wandb.ai/iakarshu/VQA%20with%20LaTr?workspace=), note that I was not able to save the checkpoint of that (some kaggle error), but I initialized the weights from [here](https://www.kaggleusercontent.com/kf/99663112/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JDENcUm0rUk0qGihFn1QuQ.wKuoRF1z1AmNCwFoZJN3SSFRMNKRvZLlGhzAykt7njLW3OUwV-TQCk9fbUx27ITQ6TpBWeYZl7G3mVorvDQquZfcYHoFam8yZpZ1zl9hmX_YQdZ1KtNrlMv0mKCpr2r6QH7WtUCbi0nWOG3R_31GJHV42pyUXJ1EII9KgnSmjKcTVNjRl7SdrwVnUW8caVtGDTZeMZuS8HH1T_-6pInZMwaZvekEvRqgIM2TArZH-0OVwIszKdfbQftcPz2f9NzpSHeu9bq6ZxhjUcUTCdNJxeNeIcxv4jnfTW146_r_zzmt4SWo8QSsG-zQAPAsxv5JL9nZiP65OUe4uNeWSO-t4ChzpRkUQLnv01ptWkzK0p9j00-xIlC36F5mXXtpbvLHlLXvkBKlrJ4NKEN76RdYAv77sbwoMQZ8RVHRj7-QYcBzaPZgTUNlRi65FnA30v0_UZIMreHyN0H1K7Kdj34TS8_pY058rYVhQY9avwuc32krDOoSG-sQ2FZA7Nvs5CoH0H6ejyvrsMMhCBbROkZDiD0jzeKwlPi-267OqjEMsKar77LsDgzkhccxp6Zgr8ZHTkEnVE553A8Yz7J76Q5vFx-M1ZXhoJIVfZcdSSpoI_jih7woeLdJVWIvctvE1aof88M1PmHPmB9qS2V9S10tK1MBIGeay06xW83d9dd5qD93ugxKZISxEg-IJddlSuII.o1fCKlUduAUrwtk1ANYLug/models/epoch=0-step=34602.ckpt)\n\nThe same weights can be downloaded by the command as follows:\n```\npip install gdown\ngdown 192-AETChd2FoNfut0hkLRwcLMfd5-uIj\n```\n\n* The script of the same can be found out [here](https://www.kaggle.com/code/akarshu121/latr-textvqa-training-with-wandb)\n\n##  License\n\nMIT\n\n## Maintainers\n\n- [uakarsh](https://github.com/uakarsh)\n\n## Contribute\n\n\n## Citations\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2112.12494,\n  doi = {10.48550/ARXIV.2112.12494},\n  url = {https://arxiv.org/abs/2112.12494},\n  author = {Biten, Ali Furkan and Litman, Ron and Xie, Yusheng and Appalaraju, Srikar and Manmatha, R.},\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {LaTr: Layout-Aware Transformer for Scene-Text VQA},\n  publisher = {arXiv},\n  year = {2021},\n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n", "metadata": {"source": "github_readmes\\uakarsh_latr_README.md", "filename": "uakarsh_latr_README.md", "type": "readme_full"}}
{"id": "unum-cloud_UForm_README.md", "paper_id": "unum-cloud_UForm_README", "text": "<h1 align=\"center\">UForm</h1>\n<h3 align=\"center\">\nPocket-Sized Multimodal AI<br/>\nFor Content Understanding and Generation<br/>\n</h3>\n<br/>\n\n<p align=\"center\">\n<a href=\"https://discord.gg/jsMURnSFM2\"><img height=\"25\" src=\"https://github.com/unum-cloud/.github/raw/main/assets/discord.svg\" alt=\"Discord\"></a>\n&nbsp; &nbsp; &nbsp;\n<a href=\"https://www.linkedin.com/company/unum-cloud/\"><img height=\"25\" src=\"https://github.com/unum-cloud/.github/raw/main/assets/linkedin.svg\" alt=\"LinkedIn\"></a>\n&nbsp; &nbsp; &nbsp;\n<a href=\"https://twitter.com/unum_cloud\"><img height=\"25\" src=\"https://github.com/unum-cloud/.github/raw/main/assets/twitter.svg\" alt=\"Twitter\"></a>\n&nbsp; &nbsp; &nbsp;\n<a href=\"https://unum.cloud/post\"><img height=\"25\" src=\"https://github.com/unum-cloud/.github/raw/main/assets/blog.svg\" alt=\"Blog\"></a>\n&nbsp; &nbsp; &nbsp;\n<a href=\"https://github.com/unum-cloud/uform\"><img height=\"25\" src=\"https://github.com/unum-cloud/.github/raw/main/assets/github.svg\" alt=\"GitHub\"></a>\n</p>\n\n<p align=\"center\">\nMultimodal Embeddings from 64 to 768 Dimensions • 1B Parameter Chat\n<br/>\nShort Texts • Images • 🔜 Video Clips • 🔜 Long Documents\n<br/>\nONNX • CoreML • PyTorch\n<br/>\n<a href=\"https://github.com/unum-cloud/uform/blob/main/python/README.md\">Python</a>\n • \n<a href=\"https://github.com/unum-cloud/uform/blob/main/javascript/README.md\">JavaScript</a>\n • \n<a href=\"https://github.com/unum-cloud/uform/blob/main/swift/README.md\">Swift</a>\n</p>\n\n---\n\n![UForm Chat Preview](https://github.com/ashvardanian/usearch-images/blob/main/assets/uform-gen-preview.jpg?raw=true)\n\nWelcome to UForm, a __multimodal__ AI library that's as versatile as it is efficient.\nUForm [tiny embedding models](#encoder) will help you understand and search visual and textual content across various languages.\nUForm [small generative models](#decoder), on the other hand, don't only support conversational and chat use-cases, but are great for fast image captioning and Visual Question Answering (VQA).\nWith compact __custom pre-trained transformer models__, this can run anywhere from your server farm down to your smartphone.\n\n## Features\n\n- __Tiny Embeddings__: 64-dimensional [Matryoshka][matryoshka]-style embeddings for extremely fast [search][usearch].\n- __Throughput__: Thanks to the small size, the inference speed is [2-4x faster](#speed) than competitors.\n- __Portable__: Models come with native ONNX support, making them easy to deploy on any platform.\n- __Quantization Aware__: Down-cast embeddings from `f32` to `i8` without losing much recall.\n- __Multilingual__: Trained on a balanced dataset, the recall is great across over 20 languages.\n\n[usearch]: https://github.com/unum-cloud/usearch\n[matryoshka]: https://arxiv.org/abs/2205.13147\n\n## Models\n\nFor accuracy and speed benchmarks refer to the [evaluation page](https://github.com/unum-cloud/uform/blob/main/BENCHMARKS.md).\n\n### Embedding Models\n\n<table style=\"width:100%; border-collapse:collapse;\">\n    <thead>\n        <tr>\n            <th>Model</th>\n            <th style=\"text-align:right;\">Parameters</th>\n            <th style=\"text-align:right;\">Languages</th>\n            <th style=\"text-align:right;\">Architecture</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td><code><a href=\"https://huggingface.co/unum-cloud/uform-vl-english-large/\">uform3-image-text-english-large</a></code>  🆕</td>\n            <td style=\"text-align:right;\">365 M</td>\n            <td style=\"text-align:right;\">1</td>\n            <td style=\"text-align:right;\">12 layer BERT, ViT-L/14</td>\n        </tr>\n        <tr>\n            <td><code><a href=\"https://huggingface.co/unum-cloud/uform-vl-english/\">uform3-image-text-english-base</a></code></td>\n            <td style=\"text-align:right;\">143 M</td>\n            <td style=\"text-align:right;\">1</td>\n            <td style=\"text-align:right;\">4 layer BERT, ViT-B/16</td>\n        </tr>\n        <tr>\n            <td><code><a href=\"https://huggingface.co/unum-cloud/uform-vl-english-small/\">uform3-image-text-english-small</a></code>  🆕</td>\n            <td style=\"text-align:right;\">79 M</td>\n            <td style=\"text-align:right;\">1</td>\n            <td style=\"text-align:right;\">4 layer BERT, ViT-S/16</td>\n        </tr>\n        <tr>\n            <td><code><a href=\"https://huggingface.co/unum-cloud/uform-vl-multilingual-v2/\">uform3-image-text-multilingual-base</a></code></td>\n            <td style=\"text-align:right;\">206M</td>\n            <td style=\"text-align:right;\">21</td>\n            <td style=\"text-align:right;\">12 layer BERT, ViT-B/16</td>\n        </tr>\n    </tbody>\n</table>\n\n### Generative Models\n\n<table style=\"width:100%; border-collapse:collapse;\">\n    <thead>\n        <tr>\n            <th>Model</th>\n            <th style=\"text-align:right;\">Parameters</th>\n            <th style=\"text-align:right;\">Purpose</th>\n            <th style=\"text-align:right;\">Architecture</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td><code><a href=\"https://huggingface.co/unum-cloud/uform-gen2-dpo/\">uform-gen2-dpo</a></code>  🆕</td>\n            <td style=\"text-align:right;\">1.2 B</td>\n            <td style=\"text-align:right;\">Chat, Image Captioning, VQA</td>\n            <td style=\"text-align:right;\">qwen1.5-0.5B, ViT-H/14</td>\n        </tr>\n        <tr>\n            <td><code><a href=\"https://huggingface.co/unum-cloud/uform-gen2-qwen-500m/\">uform-gen2-qwen-500m</a></code></td>\n            <td style=\"text-align:right;\">1.2 B</td>\n            <td style=\"text-align:right;\">Chat, Image Captioning, VQA</td>\n            <td style=\"text-align:right;\">qwen1.5-0.5B, ViT-H/14</td>\n        </tr>\n        <tr>\n            <td><code><a href=\"https://huggingface.co/unum-cloud/uform-gen/\">uform-gen</a></code> ⚠️</td>\n            <td style=\"text-align:right;\">1.5 B</td>\n            <td style=\"text-align:right;\">Image Captioning, VQA</td>\n            <td style=\"text-align:right;\">llama-1.3B, ViT-B/16</td>\n        </tr>\n    </tbody>\n</table>\n\n## Quick Start Examples\n\n### Embedding Models\n\nFirst, `pip install uform`.\nThen, load the model:\n\n```py\nfrom uform import get_model, Modality\n\nprocessors, models = get_model('unum-cloud/uform3-image-text-english-small')\n\nmodel_text = models[Modality.TEXT_ENCODER]\nmodel_image = models[Modality.IMAGE_ENCODER]\nprocessor_text = processors[Modality.TEXT_ENCODER]\nprocessor_image = processors[Modality.IMAGE_ENCODER]\n```\n\nEmbed images:\n\n```py\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\n\nimage_url = 'https://media-cdn.tripadvisor.com/media/photo-s/1b/28/6b/53/lovely-armenia.jpg'\nimage = Image.open(BytesIO(requests.get(image_url).content))\nimage_data = processor_image(image)\nimage_features, image_embedding = model_image.encode(image_data, return_features=True)\n```\n\nEmbed queries:\n\n```py\ntext = 'a cityscape bathed in the warm glow of the sun, with varied architecture and a towering, snow-capped mountain rising majestically in the background'\ntext_data = processor_text(text)\ntext_features, text_embedding = model_text.encode(text_data, return_features=True)\n```\n\nFor more details check out:\n\n- Python docs on embedding models in [python/README.md](https://github.com/unum-cloud/uform/blob/main/python/README.md#embedding-models)\n- JavaScript docs on embedding models in [javascript/README.md](https://github.com/unum-cloud/uform/blob/main/javascript/README.md#embedding-models)\n- Swift docs on embedding models in [swift/README.md](https://github.com/unum-cloud/uform/blob/main/swift/README.md#embedding-models)\n\n### Generative Models\n\nThe generative models are natively compatible with \n\n```python\nfrom transformers import AutoModel, AutoProcessor\n\nmodel = AutoModel.from_pretrained('unum-cloud/uform-gen2-dpo', trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained('unum-cloud/uform-gen2-dpo', trust_remote_code=True)\n\nprompt = 'Question or Instruction'\nimage = Image.open('image.jpg')\n\ninputs = processor(text=[prompt], images=[image], return_tensors='pt')\n\nwith torch.inference_mode():\n     output = model.generate(\n        **inputs,\n        do_sample=False,\n        use_cache=True,\n        max_new_tokens=256,\n        eos_token_id=151645,\n        pad_token_id=processor.tokenizer.pad_token_id\n    )\nprompt_len = inputs['input_ids'].shape[1]\ndecoded_text = processor.batch_decode(output[:, prompt_len:])[0]\n```\n\nFor more details check out:\n\n- Python docs on generative models in [python/README.md](https://github.com/unum-cloud/uform/blob/main/python/README.md#generative-models)\n- JavaScript docs on generative models 🔜\n- Swift docs on generative models 🔜\n\n## Technical Details\n\n### Down-casting, Quantization, Matryoshka, and Slicing\n\nDepending on the application, the embeddings can be down-casted to smaller numeric representations without losing much recall.\nSwitching from `f32` to `f16` is recommended in almost all cases, unless you are running on very old hardware without half-precision support.\nSwitching to `i8` with linear scaling is also possible, but will be noticeable in the recall on larger collections with millions of searchable entries.\nSimilarly, for higher-dimensional embeddings (512 or 768), a common strategy is to quantize them into single-bit representations for faster search.\n\n```python\nimport numpy as np\n\nf32_embedding: np.ndarray = model.encode_text(text_data, return_features=False)\nf16_embedding: np.ndarray = f32_embedding.astype(np.float16)\ni8_embedding: np.ndarray = (f32_embedding * 127).astype(np.int8)\nb1_embedding: np.ndarray = np.packbits((f32_embedding > 0).astype(np.uint8))\n```\n\nAlternative approach to quantization is to use the Matryoshka embeddings, where the embeddings are sliced into smaller parts, and the search is performed in a hierarchical manner.\n\n```python\nimport numpy as np\n\nlarge_embedding: np.ndarray = model.encode_text(text_data, return_features=False)\nsmall_embedding: np.ndarray = large_embedding[:, :256]\ntiny_embedding: np.ndarray = large_embedding[:, :64]\n```\n\nBoth approaches are natively supported by the [USearch][github-usearch] vector-search engine and the [SimSIMD][github-simsimd] numerics libraries.\nWhen dealing with small collections (up to millions of entries) and looking for low-latency cosine distance calculations, you can [achieve 5x-2500x performance improvement][report-simsimd] over Torch, NumPy, SciPy, and vanilla Python using SimSIMD.\n\n```python\nfrom simsimd import cosine, hamming\n\ndistance: float = cosine(f32_embedding, f32_embedding) # 32x SciPy performance on Apple M2 CPU\ndistance: float = cosine(f16_embedding, f16_embedding) # 79x SciPy performance on Apple M2 CPU\ndistance: float = cosine(i8_embedding, i8_embedding) # 133x SciPy performance on Apple M2 CPU\ndistance: float = hamming(b1_embedding, b1_embedding) # 17x SciPy performance on Apple M2 CPU\n```\n\nSimilarly, when dealing with large collections (up to billions of entries per server) and looking for high-throughput search, you can [achieve 100x performance improvement][report-usearch] over FAISS and other vector-search solutions using USearch.\nHere are a couple of examples:\n\n```python\nfrom usearch.index import Index\n\nf32_index = Index(ndim=64, metric='cos', dtype='f32') # for Matryoshka embeddings\nf16_index = Index(ndim=64, metric='cos', dtype='f16') # for Matryoshka embeddings\ni8_index = Index(ndim=256, metric='cos', dtype='i8') # for quantized embeddings\nb1_index = Index(ndim=768, metric='hamming', dtype='b1') # for binary embeddings\n```\n\n[github-usearch]: https://github.com/unum-cloud/usearch\n[github-simsimd]: https://github.com/ashvardanian/simsimd\n[report-usearch]: https://www.unum.cloud/blog/2023-11-07-scaling-vector-search-with-intel\n[report-simsimd]: https://ashvardanian.com/posts/python-c-assembly-comparison/\n\n### Compact Packaging\n\nPyTorch is a heavy dependency to carry, especially if you run on Edge or IoT devices.\nUsing vanilla ONNX runtime, one can significantly reduce memory consumption and deployment latency.\n\n```sh\n$ conda create -n uform_torch python=3.10 -y\n$ conda create -n uform_onnx python=3.10 -y\n$ conda activate uform_torch && pip install -e \".[torch]\" && conda deactivate\n$ conda activate uform_onnx && pip install -e \".[onnx]\" && conda deactivate\n$ du -sh $(conda info --envs | grep 'uform_torch' | awk '{print $2}')\n> 5.2G    ~/conda/envs/uform_torch\n$ du -sh $(conda info --envs | grep 'uform_onnx' | awk '{print $2}')\n> 461M    ~/conda/envs/uform_onnx\n```\n\nMost of that weight can be further reduced down to 100 MB for both the model and the runtime.\nYou can pick one of many supported [ONNX execution providers][onnx-providers], which includes XNNPACK, CUDA and TensorRT for Nvidia GPUs, OpenVINO on Intel, DirectML on Windows, ROCm on AMD, CoreML on Apple devices, and more to come.\n\n[onnx-providers]: https://onnxruntime.ai/docs/execution-providers/\n\n### Multimodal Chat in CLI\n\nThe generative models can be used for chat-like experiences in the command line.\nFor that, you can use the `uform-chat` CLI tool, which is available in the UForm package.\n\n```bash\n$ pip install uform\n$ uform-chat --model unum-cloud/uform-gen2-dpo --image=zebra.jpg\n$ uform-chat --model unum-cloud/uform-gen2-dpo \\\n>     --image=\"https://bit.ly/3tIVg9M\" \\\n>     --device=\"cuda:0\" \\\n>     --fp16\n```\n", "metadata": {"source": "github_readmes\\unum-cloud_UForm_README.md", "filename": "unum-cloud_UForm_README.md", "type": "readme_full"}}
{"id": "Vchitect_TACA_README.md", "paper_id": "Vchitect_TACA_README", "text": "<div align=\"center\">\n<h1>TACA: Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers</h1>\n</div>\n\n<div align=\"center\">\n    <span class=\"author-block\">\n      <a href=\"https://scholar.google.com/citations?user=FkkaUgwAAAAJ&hl=en\" target=\"_blank\">Zhengyao Lv*</a><sup>1</sup>,</span>\n    </span>\n    <span class=\"author-block\">\n      <a href=\"https://tianlinn.com/\" target=\"_blank\">Tianlin Pan*</a><sup>2,3</sup>,</span>\n    </span>\n    <span class=\"author-block\">\n      <a href=\"https://chenyangsi.github.io/\" target=\"_blank\">Chenyang Si*</a><sup>2</sup>,</span>\n    </span>\n    <span class=\"author-block\">\n      <a href=\"https://frozenburning.github.io/\" target=\"_blank\">Zhaoxi Chen</a><sup>4</sup>,</span>\n    </span>\n    <span class=\"author-block\">\n      <a href=\"https://homepage.hit.edu.cn/wangmengzuo\" target=\"_blank\">Wangmeng Zuo</a><sup>5</sup>,</span>\n    </span>\n    <span class=\"author-block\">\n      <a href=\"https://liuziwei7.github.io/\" target=\"_blank\">Ziwei Liu</a><sup>4†</sup>,</span>\n    </span>\n    <span class=\"author-block\">\n      <a href=\"https://i.cs.hku.hk/~kykwong/\" target=\"_blank\">Kwan-Yee K. Wong</a><sup>1†</sup>\n    </span>\n</div>\n\n<div align=\"center\">\n    <sup>1</sup>The University of Hong Kong &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n    <sup>2</sup>Nanjing University <br> \n    <sup>3</sup>University of Chinese Academy of Sciences &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n    <sup>4</sup>Nanyang Technological University<br> \n    <sup>5</sup>Harbin Institute of Technology\n</div>\n<div align=\"center\">(*Equal Contribution.&nbsp;&nbsp;&nbsp;&nbsp;<sup>†</sup>Corresponding Author.)</div>\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2506.07986/\">Paper</a> | \n    <a href=\"https://vchitect.github.io/TACA/\">Project Page</a> |\n    <a href=\"https://huggingface.co/ldiex/TACA/tree/main\">LoRA Weights</a>\n</p>\n\n# About\nWe propose **TACA**, a parameter-efficient method that dynamically rebalances cross-modal attention in multimodal diffusion transformers to improve text-image alignment.\n\nhttps://github.com/user-attachments/assets/ae15a853-ee99-4eee-b0fd-8f5f53c308f9\n\n# Usage\nFor Stable Diffusion 3.5, simply run:\n``` sh\npython infer/infer_sd3.py\n```\n\nFor FLUX.1, run:\n``` sh\npython infer/infer_flux.py\n```\n\n# Benchmark\nComparison of alignment evaluation on T2I-CompBench for FLUX.1-Dev-based and SD3.5-Medium-based models.\n\n| Model | Attribute Binding | | | Object Relationship | | Complex $\\uparrow$ |\n|---|---|---|---|---|---|---|\n| | Color $\\uparrow$ | Shape $\\uparrow$ | Texture $\\uparrow$ | Spatial $\\uparrow$ | Non-Spatial $\\uparrow$ | |\n| FLUX.1-Dev | 0.7678 | 0.5064 | 0.6756 | 0.2066 | 0.3035 | 0.4359 |\n| FLUX.1-Dev + TACA ($r = 64$) | **0.7843** | **0.5362** | **0.6872** | **0.2405** | 0.3041 | **0.4494** |\n| FLUX.1-Dev + TACA ($r = 16$) | 0.7842 | 0.5347 | 0.6814 | 0.2321 | **0.3046** | 0.4479 |\n| SD3.5-Medium | 0.7890 | 0.5770 | 0.7328 | 0.2087 | 0.3104 | 0.4441 |\n| SD3.5-Medium + TACA ($r = 64$) | **0.8074** | **0.5938** | **0.7522** | **0.2678** | 0.3106 | 0.4470 |\n| SD3.5-Medium + TACA ($r = 16$) | 0.7984 | 0.5834 | 0.7467 | 0.2374 | **0.3111** | **0.4505** |\n\n# Showcases\n![](static/images/short_1.png)\n![](static/images/short_2.png)\n![](static/images/long_1.png)\n![](static/images/long_2.png)\n", "metadata": {"source": "github_readmes\\Vchitect_TACA_README.md", "filename": "Vchitect_TACA_README.md", "type": "readme_full"}}
{"id": "VegB_VLN-Transformer_README.md", "paper_id": "VegB_VLN-Transformer_README", "text": "# Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation\n\nThis repository contains:\n1. the implementation of navigation agents for our paper: [Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2007.00229);\n2. a dataset for pretraining outdoor VLN task.\n\n\n## Data\nIn this project, we use the Touchdown dataset and the StreetLearn dataset. More details regarding these two datasets can be found [here](https://arxiv.org/abs/2001.03671).\n \nOur pre-training dataset is built upon StreetLearn.\nThe guiding instructions for the outdoor VLN task are provided in ```touchdown/datasets/```.\n\nTo download the panoramas, please refer to [Touchdown Dataset](https://github.com/lil-lab/touchdown#data) and\n[StreetLearn Dataset](https://sites.google.com/view/learn-navigate-cities-nips18/dataset).\n\n## Requirements & Setup\n\n- Python 3.6\n- PyTorch 1.7.0\n- Texar\n\nWe conduct experiments on Ubuntu 18.04 and Titan RTX.\n\nPlease run the following lines to download the code and install Texar:\n```bash\ngit clone https://github.com/VegB/VLN-Transformer/\ncd VLN-Transformer/\npip install [--user] -e .  # install Texar\ncd touchdown/\n```\n\n## Quick Start\n\n### Train VLN agent from scratch\nTraining can be performed with the following command:\n```bash\npython main.py --dataset [DATASET] --img_feat_dir [IMG_DIR] --model [MODEL] --exp_name [EXP_NAME]\n```\n- ```DATASET``` is the dataset for outdoor navigation. This repo currently support the following three datasets:\n    - ```touchdown``` is a dataset for outdoor VLN, the instructions are written by human annotators;\n    - ```manh50``` is a subset of StreetLearn, the instructions are generated by Google Map API;\n    - ```manh50_mask``` has the same trajectories as ```manh50```, but the instructions are style-modified (which is what we do in this paper).\n- ```IMG_DIR``` contains the encoded panoramas for ```DATASET```. After you get access to the panoramas, please encode them accordingly.\nEach file in this directory should be a numpy file ```[PANO_ID].npy``` that represent the panorama that has corresponding pano_id. \nThe encoding process are described in [Touchdown paper](https://arxiv.org/pdf/1811.12354.pdf), Section D.1.\n- ```MODEL``` is the navigation agent, may be ```rconcat``` for RCONCAT or ```vlntrans``` for VLN Transformer.\n\nMore parameters and usage are listed [here](https://github.com/VegB/VLN-Transformer/blob/master/touchdown/main.py#L13).\n\nIt should be noted here that ```vlntrans``` use BERT (bert-base-uncased) to encode the instruction and it takes a lot of space, \nwhich means you may need to adjust the batch size accordingly to fit the model into your GPU. \nIn our experiments, we use 3 piece of Titan RTX and a batch size of 30. \nThis is the command we use to pretrain VLN Transformer on our instruction-style-modified dataset:\n```bash\nCUDA_VISIBLE_DEVICES=\"0,1,2\" python main.py --dataset 'manh50_mask' --img_feat_dir '/data/manh50_features_mean/' --model 'vlntrans' --batch_size 30 --max_num_epochs 15 --exp_name 'pretrain_mask'\n```\n\n### Train VLN agent on top of pre-trained models\nWe can finetune the VLN agent on pre-trained models.\n```bash\npython main.py --dataset [DATASET] --img_feat_dir [IMG_DIR] --model [MODEL] --resume_from [PRETRAINED_MODEL] --resume [RESUME_OPTION]\n```\n- ```PRETRAINED_MODEL``` specified the pre-trained model;\n- ```RESUME_OPTION``` specifies the checkpoint\n    - ```latest```: the most recent ckpt;\n    - ```TC_best```: the ckpt with the best TC score on dev set;\n    - ```SPD_best```: the ckpt with the best SPD score on dev set.\n\n### Evaluate outdoor VLN performance\nWe can evaluate the agent's navigation performance on the test set and dev set with the following command:\n```bash\npython main.py --test True --dataset [DATASET] --img_feat_dir [IMG_DIR] --model [MODEL] --resume_from [PRETRAINED_MODEL] --resume [RESUME_OPTION] --CLS [True/False] --DTW [True/False]\n```\n\nThe pre-trained models for VLN Transformer, RCONCAT and GA can be downloaded \nfrom [here](https://github.com/VegB/VLN-Transformer/blob/master/touchdown/checkpoints/).\nPlease place them in ```checkpoints/```.\n\nTo reproduce the results in our paper, please use the following commands:\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" python main.py --test True --dataset 'touchdown' --img_feat_dir [IMG_DIR] --model 'rconcat' --resume_from [PRETRAINED_MODEL] --resume 'TC_best' --CLS True --DTW True\nCUDA_VISIBLE_DEVICES=\"1\" python main.py --test True --dataset 'touchdown' --img_feat_dir [IMG_DIR] --model 'ga' --resume_from [PRETRAINED_MODEL] --resume 'TC_best' --CLS True --DTW True\nCUDA_VISIBLE_DEVICES=\"2\" python main.py --test True --dataset 'touchdown' --img_feat_dir [IMG_DIR] --model 'vlntrans' --batch_size 30 --resume_from [PRETRAINED_MODEL] --resume 'TC_best' --CLS True --DTW True\n```\n- ```PRETRAINED_MODEL``` specified the pre-trained model\n    - ```vanilla```: Navigation agent trained on ```touchdown``` dataset without pre-training on auxiliary datasets.\n    - ```finetuned_manh50```: Pre-trained on ```manh50``` dataset, and finetuned on ```touchdown``` dataset.\n    - ```finetuned_mask```: Pre-trained on ```manh50_mask``` dataset, and finetuned on ```touchdown``` dataset.\n\n## Citing our work\n```\n@misc{zhu2020multimodal,\n    title={Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation},\n    author={Wanrong Zhu and Xin Wang and Tsu-Jui Fu and An Yan and Pradyumna Narayana and Kazoo Sone and Sugato Basu and William Yang Wang},\n    year={2020},\n    eprint={2007.00229},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n## Acknowledgements\n\nThe code and data can't be built without [streetlearn](https://github.com/deepmind/streetlearn), \n[speaker_follower](https://github.com/ronghanghu/speaker_follower),\n[touchdown](https://github.com/lil-lab/touchdown),\nand [Texar](https://github.com/asyml/texar-pytorch).\nWe also thank [@Jiannan Xiang](https://github.com/szxiangjn) for his contribution in reproducing the Touchdown task.\n", "metadata": {"source": "github_readmes\\VegB_VLN-Transformer_README.md", "filename": "VegB_VLN-Transformer_README.md", "type": "readme_full"}}
{"id": "WasifurRahman_BERT_multimodal_transformer_README.md", "paper_id": "WasifurRahman_BERT_multimodal_transformer_README", "text": "# Multimodal Adaptation Gate (MAG)\n\nOpen source code for ACL 2020 Paper: [Integrating Multimodal Information in Large Pretrained Transformers](https://www.aclweb.org/anthology/2020.acl-main.214.pdf)\n\nIf you use the model or results, please consider citing the research paper:\n```\n@inproceedings{rahman-etal-2020-integrating,\n    title = \"Integrating Multimodal Information in Large Pretrained Transformers\",\n    author = \"Rahman, Wasifur  and\n      Hasan, Md Kamrul  and\n      Lee, Sangwu  and\n      Bagher Zadeh, AmirAli  and\n      Mao, Chengfeng  and\n      Morency, Louis-Philippe  and\n      Hoque, Ehsan\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-main.214\",\n    doi = \"10.18653/v1/2020.acl-main.214\",\n    pages = \"2359--2369\",\n    abstract = \"\",\n}\n```\n\n## Getting started\n\n1. Configure `global_configs.py`\n\n   `global_configs.py` defines global constants for runnning experiments. Dimensions of data modality (text, acoustic, visual), cpu/gpu settings, and MAG's injection position. Default configuration is set to **MOSI**. For running experiments on **MOSEI** or on custom dataset, make sure that **ACOUSTIC_DIM** and **VISUAL_DIM** are set approperiately.\n\n   ```python\n   os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n   os.environ[\"WANDB_PROGRAM\"] = \"multimodal_driver.py\"\n\n   DEVICE = torch.device(\"cuda:0\")\n\n    # MOSI SETTING\n    ACOUSTIC_DIM = 74\n    VISUAL_DIM = 47\n    TEXT_DIM = 768\n\n    # MOSEI SETTING\n    # ACOUSTIC_DIM = 74\n    # VISUAL_DIM = 35\n    # TEXT_DIM = 768\n\n    # CUSTOM DATASET\n    # ACOUSTIC_DIM = ??\n    # VISUAL_DIM = ??\n    # TEXT_DIM = ??\n\n   XLNET_INJECTION_INDEX = 1\n   ```\n\n2. Download datasets\n   Inside `./datasets` folder, run `./download_datasets.sh` to download MOSI and MOSEI datasets\n\n3. Training MAG-BERT / MAG-XLNet on MOSI\n\n   First, install python dependancies using `pip install -r requirements.txt`\n\n   **Training scripts:**\n\n   - MAG-BERT `python multimodal_driver.py --model bert-base-uncased`\n   - MAG-XLNet `python multimodal_driver.py --model xlnet-base-cased`\n\n   By default, `multimodal_driver.py` will attempt to create a [Weights and Biases (W&B)](https://www.wandb.com/) project to log your runs and results. If you wish to disable W&B logging, set environment variable to `WANDB_MODE=dryrun`.\n\n4. Model usage\n\n   We would like to thank [huggingface](https://huggingface.co/) for providing and open-sourcing BERT / XLNet code for developing our models. Note that bert.py / xlnet.py are based on huggingface's implmentation.\n\n   **MAG**\n\n   ```python\n   from modeling import MAG\n\n   hidden_size, beta_shift, dropout_prob = 768, 1e-3, 0.5\n   multimodal_gate = MAG(hidden_size, beta_shift, dropout_prob)\n\n   fused_embedding = multimodal_gate(text_embedding, visual_embedding, acoustic_embedding)\n   ```\n\n   **MAG-BERT**\n\n   ```python\n   from bert import MAG_BertForSequenceClassification\n\n   class MultimodalConfig(object):\n       def __init__(self, beta_shift, dropout_prob):\n           self.beta_shift = beta_shift\n           self.dropout_prob = dropout_prob\n\n   multimodal_config = MultimodalConfig(beta_shift=1e-3, dropout_prob=0.5)\n   model = MAG_BertForSequenceClassification.from_pretrained(\n           'bert-base-uncased', multimodal_config=multimodal_config, num_labels=1,\n       )\n\n   outputs = model(input_ids, visual, acoustic, attention_mask, position_ids)\n   logits = outputs[0]\n   ```\n\n   **MAG-XLNet**\n\n   ```python\n   from xlnet import MAG_XLNetForSequenceClassification\n\n   class MultimodalConfig(object):\n       def __init__(self, beta_shift, dropout_prob):\n           self.beta_shift = beta_shift\n           self.dropout_prob = dropout_prob\n\n   multimodal_config = MultimodalConfig(beta_shift=1e-3, dropout_prob=0.5)\n   model = MAG_XLNet_ForSequenceClassification.from_pretrained(\n           'xlnet-base-cased', multimodal_config=multimodal_config, num_labels=1,\n       )\n\n   outputs = model(input_ids, visual, acoustic, attention_mask, position_ids)\n   logits = outputs[0]\n   ```\n\n   For MAG-BERT / MAG-XLNet usage, visual, acoustic are torch.FloatTensor of shape (batch_size, sequence_length, modality_dim).\n\n   input_ids, attention_mask, position_ids are torch.LongTensor of shape (batch_size, sequence_length). For more details on how these tensors should be formatted / generated, please refer to `multimodal_driver.py`'s `convert_to_features` method and [huggingface's documentation](https://huggingface.co/transformers/preprocessing.html)\n\n## Dataset Format\n\nAll datasets are saved under `./datasets/` folder and is encoded as .pkl file.\nFormat of dataset is as follows:\n\n```python\n{\n    \"train\": [\n        (words, visual, acoustic), label_id, segment,\n        ...\n    ],\n    \"dev\": [ ... ],\n    \"test\": [ ... ]\n}\n```\n\n- words (List[str]): List of words\n- visual (np.array): Numpy array of shape (sequence_len, VISUAL_DIM)\n- acoustic (np.array): Numpy array of shape (seqeunce_len, ACOUSTIC_DIM)\n- label_id (float): Label for data point\n- segment (Any): Unique identifier for each data point\n\nDataset is encoded as python dictionary and saved as .pkl file\n\n```python\nimport pickle as pkl\n\n# NOTE: Use 'wb' mode\nwith open('data.pkl', 'wb') as f:\n    pkl.dump(data, f)\n```\n\n## Contacts\n\n- Wasifur Rahman: rahmanwasifur@gmail.com\n- Sangwu Lee: sangwulee2@gmail.com\n- Kamrul Hasan: mhasan8@cs.rochester.edu\n", "metadata": {"source": "github_readmes\\WasifurRahman_BERT_multimodal_transformer_README.md", "filename": "WasifurRahman_BERT_multimodal_transformer_README.md", "type": "readme_full"}}
{"id": "weimin17_Multimodal_Transformer_README.md", "paper_id": "weimin17_Multimodal_Transformer_README", "text": "# Multimodal Transformer\nThe repository for the paper \"A Multimodal Transformer: Fusing Clinical Notes With Structured EHR Data for Interpretable In-Hospital Mortality Prediction\" submitted to AMIA'22 Annual Symposium.\n\n# Setup\nThe codes are tested on CUDA 11.4 with 24GB RAM GPU. For environment setup, please follow the install instruction in Section 'Clinical Data Processing'. \n\n# Clinical Data Processing\n## Structured Clinical Variables Processing\nClone https://github.com/YerevaNN/mimic3-benchmarks (Harutyunyan et al.) to 'Multimodal_Transformer/mimic3-benchmarks' folder. Setup the environment, and run all data generation steps to generate training data without text features.\n\ncreate folder 'data-mimic3' under 'Multimodal_Transformer' folder, and all the MIMIC-III processed data will be stored in 'data-mimi3' folder.\n\n## Unstructured Clinical Notes Processing\nClinical Notes processing is based on repository in https://github.com/kaggarwal/ClinicalNotesICU. \n\n### Requirenments\nsetup the environment for notes processing and model training. Install environment:\n\n~~~~\npip install -r requrements.txt\n~~~~\n\nUpdate all paths and configuration in 'mmtransformer/config.py'. \n\n\n### Notes Processing\n\n+ Run 'mmtransformer/scripts/extract_notes.py', the folder 'data-mimic3/root/test_text_fixed/', and 'data-mimic3/root/text_fixed/' will be generated.\n+ Run 'mmtransformer/scripts/extract_T0.py' file.\n\n# Train and Test\n\nFor our well-trained model, you can download from [GoogleDrive](https://drive.google.com/file/d/1Wch0pEgQ8PeWE9p77B6rdNuo9l28CZNv/view?usp=sharing). Unzip the file and put them in './Multimodal_Transformer/mmtransformer/models/Checkpoints' and './Multimodal_Transformer/mmtransformer/models/Data' accordingly. Or you can generate the files yourself.\n\n## Test\n\nFor model with only clinical notes (mbert), run\n\n~~~~\npython mbert.py --gpu_id 1\n~~~~\n\nFor multimodal transformer, run\n\n~~~~\npython IHM_mmtransformer.py --mode test --model_type both --model_name BioBert --TSModel Transformer --checkpoint_path Multimodal_Transformer --MaxLen 512 --NumOfNotes 0 --TextModelCheckpoint BioClinicalBERT_FT --freeze_model 1 --number_epoch 5 --batch_size 5 --load_model 1 --gpu_id 1\n~~~~\n\n## Train\n\nFor multimodal transformer training, run\n\n~~~~\npython IHM_mmtransformer.py --mode train --model_type both --model_name BioBert --TSModel Transformer --checkpoint_path Multimodal_Transformer --MaxLen 512 --NumOfNotes 0 --TextModelCheckpoint BioClinicalBERT_FT --freeze_model 1 --number_epoch 5 --batch_size 5 --load_model 0 --gpu_id 1\n~~~~\n\n\n# Visualization\nThe output of all analysis are in 'Analysis' folder. For important clinical words analysis and visualization in clinical notes, \n\n1. Run 'notes_analysis.py' to get the IG value with associated words, stored in file 'Analysis/bert_analysis_pred_all2.pkl'\n\n2. Run 'notes_analysis3.py' to get the word list with frequency, stored in 'pred_tokenlist_top10_l0_2.txt'. We further filtered the list to remove the irrelavent words and tokens, which is stored in 'filter_pred_tokenlist_top10_l0_2.txt'.\n\nIt will also generate the word cloud 'filter_pred_tokenlist_top10_l0_2.png'.\n\n\n# Credits\nThe code is based on repository by Khadanga et al. given in https://github.com/kaggarwal/ClinicalNotesICU, and by Deznabi et al. given in https://github.com/Information-Fusion-Lab-Umass/ClinicalNotes_TimeSeries for experimental setup.\n\n\nThe MIMIC-III clinical variables pre-processing is clone from repository by Harutyunyan et al. given in https://github.com/YerevaNN/mimic3-benchmarks\n", "metadata": {"source": "github_readmes\\weimin17_Multimodal_Transformer_README.md", "filename": "weimin17_Multimodal_Transformer_README.md", "type": "readme_full"}}
{"id": "wzk1015_CNMT_README.md", "paper_id": "wzk1015_CNMT_README", "text": "\n\n## Introduction\n\nCode for our AAAI 2021 paper *Confidence-aware Non-repetitive Multimodal Transformers for TextCaps* [[PDF]](https://arxiv.org/pdf/2012.03662.pdf).\n\n\n\n## Installation\n\nOur implementation is based on Pythia framework (now called [*mmf*](https://github.com/facebookresearch/mmf)), and built upon [M4C-Captioner](https://github.com/ronghanghu/pythia/tree/project/m4c_captioner_pre_release/projects/M4C_Captioner). Please refer to [Pythia's document](https://mmf.sh/docs/) for more details on installation requirements.\n\n```shell\n# install pythia based on requirements.txt\npython setup.py build develop  \n```\n\n\n\n## Data Preparation\n\nThe following is open-source data of TextCaps dataset from [M4C-Captioner's Github repository](https://github.com/ronghanghu/pythia/tree/project/m4c_captioner_pre_release/projects/M4C_Captioner). Please download them from the links below and and extract them under `data` directory.\n\n*  [object Faster R-CNN Features of TextCaps](https://dl.fbaipublicfiles.com/pythia/features/open_images.tar.gz)\n*   [OCR Faster R-CNN Features of TextCaps](https://dl.fbaipublicfiles.com/pythia/m4c/data/m4c_textvqa_ocr_en_frcn_features.tar.gz)\n*  [detectron weights of TextCaps](http://dl.fbaipublicfiles.com/pythia/data/detectron_weights.tar.gz)\n\nOur `imdb` files include new OCR tokens and recognition confidence extracted with pretrained OCR systems ( [CRAFT](https://github.com/clovaai/CRAFT-pytorch), [ABCNet](https://github.com/Yuliang-Liu/bezier_curve_text_spotting) and [four-stage STR](https://github.com/Yuliang-Liu/bezier_curve_text_spotting)). The three imdb files should be downloaded from the links below and **put under `data/imdb/`**.\n\n| file name                          | download link                                                |\n| ---------------------------------- | ------------------------------------------------------------ |\n| imdb_train.npy                     | [Google Drive](https://drive.google.com/file/d/1EzF2WB81BTs2Bgt6kFdq2PTRlQl8EQ-y/view?usp=sharing)  [Baidu Netdisk](https://pan.baidu.com/s/1pAg8oF1pTZEJ3g60G5O4bg)(password: sxbk) |\n| imdb_val_filtered_by_image_id.npy  | [Google Drive](https://drive.google.com/file/d/1FuqUGIsOqCkCqEGKIQAkc_08aMpjVJls/view?usp=sharing)  [Baidu Netdisk](https://pan.baidu.com/s/1Z2K3hhG21W5Vl3c75K50Iw)(password: i6pf) |\n| imdb_test_filtered_by_image_id.npy | [Google Drive](https://drive.google.com/file/d/1lu3aW0oTh6CO0_L64W9PE5UNW4_H7Cj2/view?usp=sharing)  [Baidu Netdisk](https://pan.baidu.com/s/1Wrp3HA0OgLyHMEzy_rUXmQ)(password: uxew) |\n\n\n\n\nFinally, your `data` directory structure should look like this:\n\n```shell\ndata\n|-detectron\t\t\t\t\t\t\t\n|---...\n|-m4c_textvqa_ocr_en_frcn_features\n|---...\n|-open_images\t\t\t\t\t\t\n|---...\n|-vocab_textcap_threshold_10.txt \t#already provided\n|-imdb\t\t\t\t\t\t\t\t\n|---imdb_train.npy\t\t\t\t\t\n|---imdb_val_filtered_by_image_id.npy\t\n|---imdb_test_filtered_by_image_id.npy\t\t\n```\n\n\n\n## Pretrained Model\n\n| download link                                                | description | val set CIDEr | test set CIDEr |\n| ------------------------------------------------------------ | ----------- | ------------- | -------------- |\n| [Google Drive](https://drive.google.com/file/d/1VfdvR12fPKNJnljjzSZ9lMIPw1Foa4WF/view?usp=sharing())  [Baidu Netdisk](https://pan.baidu.com/s/1ctuiob1whlgM7MimwlRiGg)(password: c4be) | CNMT best   | 101.6         | 93.0           |\n\n\n\n\n\n## Training\n\nWe provide an example script for training on TextCaps dataset for 12000 iterations and evaluating every 500 iterations.\n\n```shell\n./train.sh\n```\n\nThis may take approximately 13 hours, depending on GPU devices. Please refer to our paper for implementation details.\n\nFirst-time training will download `fasttext` model . You may also download it manually and put it under `pythia/.vector_cache/`.\n\nDuring training, log file can be found under `save/cnmt/m4c_textcaps_cnmt/logs/`. You may also run training in background and check log file for training status.\n\n\n\n## Evaluation\n\nAssume that checkpoint of the trained model is saved at `save/cnmt/m4c_textcaps_cnmt/best.ckpt` (otherwise modify the `resume_file` parameter in the shell script).\n\nRun the following script to generate prediction json file:\n\n```shell\n#evaluate on validation set\n./eval_val.sh \n#evaluate on test set\n./eval_test.sh\n```\n\nThe prediction json file will be saved under `save/eval/m4c_textcaps_cnmt/reports/`. You can submit the json file to the TextCaps EvalAI server for result.\n\n\n\n## Citation\n\n```\n@article{wang2020confidenceaware,\n  title={Confidence-aware Non-repetitive Multimodal Transformers for TextCaps}, \n  author={Wang, Zhaokai and Bao, Renda and Wu, Qi and Liu, Si},\n  year={2020},\n  journal={arXiv preprint arXiv:2012.03662},\n}\n```\n\n", "metadata": {"source": "github_readmes\\wzk1015_CNMT_README.md", "filename": "wzk1015_CNMT_README.md", "type": "readme_full"}}
{"id": "X-PLUG_mPLUG-Owl_README.md", "paper_id": "X-PLUG_mPLUG-Owl_README", "text": "<div align=\"center\">\n\n<h2>mPLUG-Owl: The Powerful MLLM Family</h2>\n\n</div>\n\n<!--![summary_tab](https://z1.ax1x.com/2023/11/03/piM1rGQ.md.png)!-->\n![summary_tab](https://picx.zhimg.com/70/v2-0c20dc670d9a55c03fb8d7b253ebfe1d_1440w.avis?source=172ae18b&biz_tag=Post)\n\n\n\n- [**mPLUG-Owl**](mPLUG-Owl) (Arxiv 2023) - mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality\n\n- [**mPLUG-Owl2**](mPLUG-Owl2) (Arxiv 2023) - mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration\n\n- [**mPLUG-Owl3**](mPLUG-Owl3) (Arxiv 2024) - mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models\n\n## News and Updates\n* ```2024.08.12``` 🔥🔥🔥 We release **mPLUG-Owl3**. The source code and weights are avaliable at [HuggingFace](https://huggingface.co/mPLUG/mPLUG-Owl3-7B-240728).\n* ```2024.04.05``` **mPLUG-Owl2** is accepted by CVPR 2024 as a Highlight.\n* ```2024.02.01``` We relaese **mPLUG-Owl2.1**, a Chinese enhanced version of mPLUG-Owl2. The weight is available at [HuggingFace](https://huggingface.co/Mizukiluke/mplug_owl_2_1).\n\n## License\n\nThe content of this project itself is licensed under [LICENSE](LICENSE).\n\n\n## Misc\n\n<div align=\"center\">\n\n[![Stargazers repo roster for @X-PLUG/mPLUG-Owl](https://reporoster.com/stars/X-PLUG/mPLUG-Owl)](https://github.com/X-PLUG/mPLUG-Owl/stargazers)\n\n[![Forkers repo roster for @X-PLUG/mPLUG-Owl](https://reporoster.com/forks/X-PLUG/mPLUG-Owl)](https://github.com/X-PLUG/mPLUG-Owl/network/members)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=X-PLUG/mPLUG-Owl&type=Date)](https://star-history.com/#X-PLUG/mPLUG-Owl&Date)\n\n</div>\n", "metadata": {"source": "github_readmes\\X-PLUG_mPLUG-Owl_README.md", "filename": "X-PLUG_mPLUG-Owl_README.md", "type": "readme_full"}}
{"id": "X-PLUG_mPLUG_README.md", "paper_id": "X-PLUG_mPLUG_README", "text": "# mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections. (EMNLP 2022)\n\n[https://arxiv.org/abs/2205.12005](https://arxiv.org/abs/2205.12005)\n\n\n## Introduction\nWe presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from inefficiency and linguistic signal overwhelmed by long visual sequences in cross modal alignment. To address both problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross modal skip-connections. mPLUG achieves state-of-the-art results on a wide range of vision language downstream tasks, including image captioning, image-text retrieval, visual grounding and visual question answering.\n\n<img src=\"mplug_framework.png\" width=\"600\"> \n\n\n## News\n\n* 2023.5.08: Moved from [AliceMind repo](https://github.com/alibaba/AliceMind) for further update.\n* 2022.8.28: Released mPLUG downstream tasks!\n\n\n## Pre-trained models and datasets\n\n* Pre-trained models\n\n \nFor VQA and image captioning tasks, we do an additional continue pre-training on 4M image-text pairs based mplug.en.large to get mplug.en.large.v2.\n \n \n|Model | Visual Backbone | Text Enc Layers | Fusion Layers | Text Dec Layers | #params | Download |\n|------------------------|-------------------------------------------|------|------|------|------|-----|\n|mplug.en.base | [vit-b-16](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/ViT-B-16.tar) | 6 | 6 | 12 | 350M | [mplug.en.base](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/mplug_base.pth) |\n|mplug.en.large | [vit-l-14](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/ViT-L-14.tar) | 6 | 6 | 12 | 600M | [mplug.en.large](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/mplug_large.pth) |\n|mplug.en.large.v2 | [vit-l-14](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/ViT-L-14.tar) | 6 | 6 | 12 | 600M | [mplug.en.large.v2](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/mplug_large_v2.pth) |\n|mplug.en.huge | vit-l-14 | 24 | 6 | 12 | 1.1B | comming soon |\n                                                                     \n\n* Pre-train Datasets\n\n                                                                        \n| | COCO | VG | SBU | CC3M | CC13M |\n|------------------------|-------------------------------------------|------|------|------|------|\n|image | 113K | 100K | 860K | 3M | 10M | \n|text | 567K | 769K | 860K | 3M | 10M |\n\n\n## Results\n- Image-text\n<table>\n<thead>\n  <tr align=\"center\">\n    <th>Task</th>\n    <th>VQA</th>\n    <th>Image Captioning</th>\n    <th colspan=\"2\">Retrieval</th>\n    <th colspan=\"3\">Referring Expression&nbsp;&nbsp;&nbsp;Comprehension</th>\n    <th colspan=\"2\">Visual Entailment</th>\n  </tr>\n</thead>\n<tbody>\n  <tr align=\"center\">\n    <td>Dataset</td>\n    <td>VQA v2</td>\n    <td>COCO</td>\n    <td>MSCOCO</td>\n    <td>Flickr30K</td>\n    <td>RefCOCO</td>\n    <td>RefCOCO+</td>\n    <td>RefCOCOg</td>\n    <td>SNLI-VE</td>\n    <td>NLVR2</td>\n  </tr>\n  <tr align=\"center\">\n    <td>Split</td>\n    <td>test-dev/test-std</td>\n    <td>Karpathy&nbsp;test (CE/CIDEr)</td>\n    <td>5k test (TR/IR)</td>\n    <td>1k test (TR/IR)</td>\n    <td>val/test-a/test-b</td>\n    <td>val/test-a/test-b</td>\n    <td>val-u/test-u</td>\n    <td>val/test</td>\n    <td>dev/test-P</td>\n  </tr>\n  <tr align=\"center\">\n    <td>Metric</td>\n    <td>Acc.</td>\n    <td>CIDEr</td>\n    <td>R@1</td>\n    <td>R@1</td>\n    <td colspan=\"3\">Acc.</td>\n    <td>Acc.</td>\n    <td>Acc.</td>\n  </tr>\n  <tr align=\"center\">\n    <td>mPLUG<sub>Base</td>\n    <td>79.79/79.98</td>\n    <td>137.5/150.4</td>\n    <td>-/-</td>\n    <td>-/-</td>\n    <td>-/-</td>\n    <td>-/-</td>\n    <td>-/-</td>\n    <td>-/-</td>\n    <td>-/-</td>\n  </tr>\n  <tr align=\"center\">\n    <td>mPLUG<sub>Large</td>\n    <td>81.27/81.26</td>\n    <td>141.0/155.1</td>\n    <td>82.8/65.8</td>\n    <td>97.6/88.4</td>\n    <td>92.40/94.51/88.42</td>\n    <td>86.02/90.17 / 78.17</td>\n    <td>85.88/86.42</td>\n    <td>89.45/89.29</td>\n    <td>84.58/84.95</td>\n  </tr>\n  <tr align=\"center\">\n    <td>mPLUG<sub>Huge</td>\n    <td>82.27/82.41</td>\n    <td>142.3/158.7</td>\n    <td>-/-</td>\n    <td>-/-</td>\n    <td>-/-/-</td>\n    <td>-/-/-</td>\n    <td>-/-</td>\n    <td>-/-</td>\n    <td>-/-/-</td>\n  </tr>\n</tbody>\n</table>\n\n- Video-text\n\n<table>\n<thead>\n  <tr align=\"center\">\n    <th>Task</th>\n    <th>Video Retrieval</th>\n    <th colspan=\"2\">Video QA</th>\n    <th>Video Captioning</th>\n  </tr>\n</thead>\n<tbody>\n  <tr align=\"center\">\n    <td>Dataset</td>\n    <td>MSRVTT</td>\n    <td>MSRVTT-QA</td>\n    <td>MSVD-QA</td>\n    <td>VATEX</td>\n  </tr>\n  <tr align=\"center\">\n    <td>Split</td>\n    <td>test</td>\n    <td>test</td>\n    <td>test</td>\n    <td>test(CE)</td>\n  </tr>\n  <tr align=\"center\">\n    <td>Metric</td>\n    <td>R@1</td>\n    <td>Acc.</td>\n    <td>Acc.</td>\n    <td>CIDEr</td>\n  </tr>\n  <tr align=\"center\">\n    <td>mPLUG</td>\n    <td>38.1</td>\n    <td>21.1</td>\n    <td>37.2</td>\n    <td>42.0</td>\n  </tr>\n</tbody>\n</table>\n\n\n## Requirements\n* [PyTorch](https://pytorch.org/) version >= 1.11.0\n\n* Install other libraries via\n```\npip install -r requirements.txt\n```\n\n\n## Pre-training\n\n\nComming soon.\n\n\n## Fine-tuning\n\n[Download json files of downstream tasks](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/data.tar)\n\n### Visual Question Answering\n\n1. Download VQA v2 dataset and Visual Genome dataset from the original websites [VQA 2.0](https://eval.ai/web/challenges/challenge-page/830/leaderboard/2278).\n2. Download and extract the provided dataset json files.\n3. In configs/vqa_mplug_base.yaml, set the paths for the json files and the image paths.\n4. Finetune the pre-trained mplug_base or large model using 8 A100 GPUs:\n<pre>sh scripts/vqa_mplug_base.sh</pre> \n<pre>sh scripts/vqa_mplug_large.sh</pre>                                             \n5. Evaluate the result using the official evaluation server.\n       \n                                                                                          \n### Image Captioning\n     \n                                                                                          \n1. Download COCO Caption dataset from the original websites.\n2. Download and extract the provided dataset json files.\n3. Download language evalution tool([language_evalution](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/mPLUG/language_evaluation.tar)).\n4. In configs/caption_mplug_base.yaml, set the paths for the json files and the image paths.\n5. Finetune the pre-trained mplug_base or large model using 8 A100 GPUs:\n<pre>sh scripts/caption_mplug_base.sh</pre> \n<pre>sh scripts/caption_mplug_large.sh</pre>  \n\n                                                                                          \n### Image-text Retrieval\n1. Download MSCOCO or Flickr30k datasets from the original websites.\n2. Download and extract the provided dataset json files.\n3. In configs/retrieval_flickr30k_mplug_large.yaml or configs/retrieval_coco_mplug_large.yaml, set the paths for the json files and the image path.\n4. Finetune the pre-trained checkpoint using 8 A100 GPUs:\n<pre>sh scripts/retrieval_flickr30k_mplug_large.sh</pre> \n<pre>sh scripts/retrieval_coco_mplug_large.sh</pre>\n\n### Visual Grounding\n1. Download RefCOCO datasets from the original websites.\n2. Download and extract the provided dataset json files.\n3. In configs/grounding_mplug_large.yaml, set the paths for the json files and the image path. Data preparation can follow [TransVG](https://github.com/djiajunustc/TransVG)\n4. Finetune the pre-trained checkpoint using 8 A100 GPUs:\n<pre> sh scripts/grounding_mplug_base.sh </pre>\n\n### Zero-shot Video-text Retrieval\n1. Download MSRVTT datasets from the original websites.\n2. In configs/retrieval_msrvtt_mplug_large.yaml, set the paths for the json files and the video paths.\n3. To perform zero-shot evaluation, run：\n<pre>sh scripts/retrieval_msrvtt_mplug_large.sh</pre> \n\n### Zero-shot Video Question Answering\n1. Download MSRVTT-QA datasets from the original websites.\n2. In configs/videoqa_msrvtt_mplug_base.yaml, set the paths for the json files and the video paths.\n3. To perform zero-shot evaluation, run：\n<pre>sh scripts/videoqa_msrvtt_mplug_base.sh</pre> \n\n### Zero-shot Video Captioning\n1. Download VATEX datasets from the original websites.\n2. In configs/videocap_vatex_mplug_large.yaml, set the paths for the json files and the video paths.\n3. To perform zero-shot evaluation, run：\n<pre>sh scripts/videocap_vatex_mplug_large.sh</pre> \n\n\n\n## Citation\nIf you use our work, please cite:\n```\n@article{li2022mplug,\n  title={mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections},\n  author={Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},\n  journal={arXiv preprint arXiv:2205.12005},\n  year={2022}\n}\n```\n## Acknowledgement\n\nThe implementation of mPLUG relies on resources from [ALBEF](https://github.com/salesforce/ALBEF), [BLIP](https://github.com/salesforce/BLIP), and [timm](https://github.com/rwightman/pytorch-image-models/tree/master/timm). We thank the original authors for their open-sourcing.\n\n", "metadata": {"source": "github_readmes\\X-PLUG_mPLUG_README.md", "filename": "X-PLUG_mPLUG_README.md", "type": "readme_full"}}
{"id": "xjchenGit_MTDVocaLiST_README.md", "paper_id": "xjchenGit_MTDVocaLiST_README", "text": "# MTDVocaLiST\nWe proposed a lightweight audio-visual synchronization (AVS)model **[MTDVocaLiST](https://arxiv.org/abs/2210.15563)**. MTDVocaLiST reduces the model size of **[VocaLiST](https://github.com/vskadandale/vocalist)** by 83.52%, yet still maintaining similar performance. For more details about VocaLiST, please visit its [[project webpage]](https://ipcv.github.io/VocaLiST/).\nAudio-visual synchronization aims to determine whether the mouth movements and speech in the video are synchronized. This repository is the official repository for the paper\n[Multimodal Transformer Distillation for Audio-Visual Synchronization](https://arxiv.org/abs/2210.15563). The paper has been accepted by ICASSP 2024.\n\n![](Vocalist_Illustration.png)\n\n## Datasets and preprocessing\nThere are 2 datasets involved in this work: i) The AV speech dataset of LRS2, and ii) the AV singing voice dataset of Acappella. The LRS2 dataset can be requested for download [here](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html). All the models in this work operate on audios sampled at 16kHz and videos with 25fps. The preprocessing steps are the same as [Wav2Lip](https://github.com/Rudrabha/Wav2Lip/blob/master/preprocess.py). The preprocessing step aims to obtain the cropped RGB face frames of size 3x96x96 in the .jpg format and audios of 16kHz sampling rate for each of the video samples in respective datasets.\n\n## Leverage pre-trained MTDVocaLiST only\nYou can download the pre-trained MTDVocaLiST from [here](https://github.com/xjchenGit/MTDVocaLiST/releases/download/v1.0/pure_MTDVocaLiST.pth).\n```python\nimport torch\nfrom models.student_thin_200_all import SyncTransformer\n\ncpk = torch.load(\"pretrained/pure_MTDVocaLiST.pth\", map_location='cpu')\nmodel = SyncTransformer(d_model=200)\nmodel.load_state_dict(cpk[\"state_dict\"])\n```\n\n## Training (Multimodal Transformer Distillation)\nYou need to download the pre-trained VocaLiST model first from [[Weights]](https://drive.google.com/drive/folders/1-g4qHUNNcCZpmSqEflKMxPMvwnn9e88N?usp=sharing).\n\n```bash\nbash run_train_student.sh\n```\n\n## Evaluation (Inference)\n\n```\npython3 test_stu.py --data_root /path/to/lip-sync/LRS2_wav2lip/main/ --checkpoint_path /path/to/Best.pth\n```\n\n## Comparison with the SOTA AVS model\n\n<div class=\"center\" style=\"text-align: center\">\n    <div class=\"center col-md-8\" style=\"text-align: center\">\n        <img src=\"figures/size_and_acc.jpg\"/>\n    </div>\n</div>\n\n## Citation\nIf you find our work useful, please consider cite\n```\n@INPROCEEDINGS{10446372,\n  author={Chen, Xuanjun and Wu, Haibin and Wang, Chung-Che and Lee, Hung-Yi and Jang, Jyh-Shing Roger},\n  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n  title={Multimodal Transformer Distillation for Audio-Visual Synchronization}, \n  year={2024},\n  volume={},\n  number={},\n  pages={7755-7759},\n  doi={10.1109/ICASSP48485.2024.10446372}\n}\n\n@inproceedings{kadandale2022vocalist,\n  title={VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices},\n  author={Kadandale, Venkatesh S and Montesinos, Juan F and Haro, Gloria},\n  booktitle={Interspeech},\n  pages={3128--3132},\n  year={2022}\n}\n```\n## Acknowledgement\nIf you have any questions, please feel free to contact me by email at d12942018@ntu.edu.tw.\n", "metadata": {"source": "github_readmes\\xjchenGit_MTDVocaLiST_README.md", "filename": "xjchenGit_MTDVocaLiST_README.md", "type": "readme_full"}}
{"id": "yaohungt_Multimodal-Transformer_README.md", "paper_id": "yaohungt_Multimodal-Transformer_README", "text": "![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg)  \n\n# Multimodal Transformer for Unaligned Multimodal Language Sequences\n\n> Pytorch implementation for learning Multimodal Transformer for unaligned multimodal language sequences.\n\nCorrespondence to: \n  - Yao-Hung Hubert Tsai (yaohungt@cs.cmu.edu)\n  - Shaojie Bai (shaojieb@andrew.cmu.edu)\n\n## Paper\n[**Multimodal Transformer for Unaligned Multimodal Language Sequences**](https://arxiv.org/pdf/1906.00295.pdf)<br>\n[Yao-Hung Hubert Tsai](https://yaohungt.github.io) *, [Shaojie Bai](https://jerrybai1995.github.io) *, [Paul Pu Liang](http://www.cs.cmu.edu/~pliang/), [J. Zico Kolter](http://zicokolter.com), [Louis-Philippe Morency](https://www.cs.cmu.edu/~morency/), and [Ruslan Salakhutdinov](https://www.cs.cmu.edu/~rsalakhu/)<br>\nAssociation for Computational Linguistics (ACL), 2019. (*equal contribution)\n\nPlease cite our paper if you find our work useful for your research:\n\n```tex\n@inproceedings{tsai2019MULT,\n  title={Multimodal Transformer for Unaligned Multimodal Language Sequences},\n  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J. Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},\n  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  month = {7},\n  year={2019},\n  address = {Florence, Italy},\n  publisher = {Association for Computational Linguistics},\n}\n```\n\n## Overview\n\n### Overall Architecture for Multimodal Transformer\n<p align=\"center\">\n<img src='imgs/architecture.png' width=\"500px\"/>\n\nMultimodal Transformer (MulT) merges multimodal time-series via a feed-forward fusion process from multiple directional pairwise crossmodal transformers. Specifically, each crossmodal transformer serves to repeatedly reinforce a *target modality* with the low-level features from another *source modality* by learning the attention across the two modalities' features. A MulT architecture hence models all pairs of modalities with such crossmodal transformers, followed by sequence models (e.g., self-attention transformer) that predicts using the fused features.\n\n\n### Crossmodal Attention for Two Sequences from Distinct Modalities \n<p align=\"center\">\n<img src='imgs/cm.png' width=\"1000px\"/>\n  \nThe core of our proposed model are crossmodal transformer and crossmodal attention module.   \n  \n## Usage\n\n### Prerequisites\n- Python 3.6/3.7\n- [Pytorch (>=1.0.0) and torchvision](https://pytorch.org/)\n- CUDA 10.0 or above\n\n### Datasets\n\nData files (containing processed MOSI, MOSEI and IEMOCAP datasets) can be downloaded from [here](https://www.dropbox.com/sh/hyzpgx1hp9nj37s/AAB7FhBqJOFDw2hEyvv2ZXHxa?dl=0).\n  \nI personally used command line to download everything:\n~~~~\nwget https://www.dropbox.com/sh/hyzpgx1hp9nj37s/AADfY2s7gD_MkR76m03KS0K1a/Archive.zip?dl=1\nmv 'Archive.zip?dl=1' Archive.zip\nunzip Archive.zip\n~~~~\n\nTo retrieve the meta information and the raw data, please refer to the [SDK for these datasets](https://github.com/A2Zadeh/CMU-MultimodalSDK).\n\n### Run the Code\n\n1. Create (empty) folders for data and pre-trained models:\n~~~~\nmkdir data pre_trained_models\n~~~~\n\nand put the downloaded data in 'data/'.\n\n2. Command as follows\n~~~~\npython main.py [--FLAGS]\n~~~~\n\nNote that the defualt arguments are for unaligned version of MOSEI. For other datasets, please refer to Supplmentary.\n\n### If Using CTC\n\nTransformer requires no CTC module. However, as we describe in the paper, CTC module offers an alternative to applying other kinds of sequence models (e.g., recurrent architectures) to unaligned multimodal streams.\n\nIf you want to use the CTC module, plesase install warp-ctc from [here](https://github.com/baidu-research/warp-ctc).\n\nThe quick version:\n~~~~\ngit clone https://github.com/SeanNaren/warp-ctc.git\ncd warp-ctc\nmkdir build; cd build\ncmake ..\nmake\ncd ../pytorch_binding\npython setup.py install\nexport WARP_CTC_PATH=/home/xxx/warp-ctc/build\n~~~~\n\n### Acknowledgement\nSome portion of the code were adapted from the [fairseq](https://github.com/pytorch/fairseq) repo.\n\n\n", "metadata": {"source": "github_readmes\\yaohungt_Multimodal-Transformer_README.md", "filename": "yaohungt_Multimodal-Transformer_README.md", "type": "readme_full"}}
{"id": "YaoZhang93_mmFormer_README.md", "paper_id": "YaoZhang93_mmFormer_README", "text": "# mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation\nThis is the implementation for the paper:\n\n[mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation](https://arxiv.org/abs/2206.02425)\n\nAccepted to MICCAI 2022 (Student Travel Award)\n\n## Abstract\n\nAccurate brain tumor segmentation from Magnetic Resonance Imaging (MRI) is desirable to joint learning of multimodal images. However, in clinical practice, it is not always possible to acquire a complete set of MRIs, and the problem of missing modalities causes severe performance degradation in existing multimodal segmentation methods. In this work, we present the first attempt to exploit the Transformer for multimodal brain tumor segmentation that is robust to any combinatorial subset of available modalities. Concretely, we propose a novel multimodal Medical Transformer (mmFormer) for incomplete multimodal learning with three main components: the hybrid modality-specific encoders that bridge a convolutional encoder and an intra-modal Transformer for both local and global context modeling within each modality; an inter-modal Transformer to build and align the long-range correlations across modalities for modality-invariant features with global semantics corresponding to tumor region; a decoder that performs a progressive up-sampling and fusion with the modality-invariant features to generate robust segmentation. Besides, auxiliary regularizers are introduced in both encoder and decoder to further enhance the model's robustness to incomplete modalities. We conduct extensive experiments on the public BraTS 2018 dataset for brain tumor segmentation. The results demonstrate that the proposed mmFormer outperforms the state-of-the-art methods for incomplete multimodal brain tumor segmentation on almost all subsets of incomplete modalities, especially by an average 19.07% improvement of Dice on tumor segmentation with only one available modality. \n\n![image](https://github.com/YaoZhang93/mmFormer/blob/main/figs/overview.png)\n\n## Usage. \n\n* Environment Preparation\n  * Download the cuda and pytorch from [Google Drive](https://drive.google.com/drive/folders/1x6z7Ot3Xfrg1dokR9cdeoRSKbQJRTpv7?usp=sharing).\n  * Set the environment path in `job.sh`.\n* Data Preparation\n  * Download the data from [MICCAI 2018 BraTS Challenge](https://www.med.upenn.edu/sbia/brats2018/data.html).\n  * Set the data path in `preprocess.py` and then run `python preprocess.py`.\n  * Set the data path in `job.sh`.\n* Train\n  * Train the model by `sh job.sh`. \n\n* Test\n  * The trained model should be located in `mmFormer/output`. \n  * Uncomment the evaluation command in  `job.sh` and then inference on the test data by `sh job.sh`.\n  * The pre-trained [model](https://drive.google.com/file/d/1oKgjXzSfWOG5VT64EE1lfV6rdtjkyC5B/view?usp=sharing) and [log](https://drive.google.com/file/d/165u-MGAiS0_PkExXRkI4KrainRlc_Ibo/view?usp=sharing) are available.\n\n## Citation\n\nIf you find this code and paper useful for your research, please kindly cite our paper.\n\n```\n@inproceedings{zhang2022mmformer,\n  title={mmformer: Multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation},\n  author={Zhang, Yao and He, Nanjun and Yang, Jiawei and Li, Yuexiang and Wei, Dong and Huang, Yawen and Zhang, Yang and He, Zhiqiang and Zheng, Yefeng},\n  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2022: 25th International Conference, Singapore, September 18--22, 2022, Proceedings, Part V},\n  pages={107--117},\n  year={2022},\n  organization={Springer}\n}\n```\n\n## Reference\n\n* [TransBTS](https://github.com/Wenxuan-1119/TransBTS)\n* [RFNet](https://github.com/dyh127/RFNet)\n\n", "metadata": {"source": "github_readmes\\YaoZhang93_mmFormer_README.md", "filename": "YaoZhang93_mmFormer_README.md", "type": "readme_full"}}
{"id": "yashkant_sam-textvqa_README.md", "paper_id": "yashkant_sam-textvqa_README", "text": "Spatially Aware Multimodal Transformers for TextVQA\n===================================================\n<h4>\nYash Kant, Dhruv Batra, Peter Anderson, Alex Schwing, Devi Parikh, Jiasen Lu, Harsh Agrawal\n</br>\n<span style=\"font-size: 14pt; color: #555555\">\nPublished at ECCV, 2020\n</span>\n</h4>\n<hr>\n\n**Paper:** [arxiv.org/abs/2007.12146](https://arxiv.org/abs/2007.12146)\n\n**Project Page:** [yashkant.github.io/projects/sam-textvqa](https://yashkant.github.io/projects/sam-textvqa.html)\n\nWe propose a novel spatially aware self-attention layer such that each visual entity only looks at neighboring entities defined by a spatial graph and use it to solve TextVQA.\n<p align=\"center\">\n  <img src=\"tools/sam-textvqa-large.png\">\n</p>\n\n\n## Repository Setup\n\nCreate a fresh conda environment, and install all dependencies.\n\n```text\nconda create -n sam python=3.6\nconda activate sam\ncd sam-textvqa\npip install -r requirements.txt\n```\n\nInstall pytorch\n```\nconda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n```\n\nFinally, install apex from: https://github.com/NVIDIA/apex\n\n## Data Setup\nDownload files from the [dropbox link](https://www.dropbox.com/sh/dk6oubjlt2x7w0h/AAAKExm33IKnVe8mkC4tOzUKa) and place it in the ``data/`` folder.\nEnsure that data paths match the directory structure provided in ``data/README.md``\n\n## Run Experiments\nFrom the below table pick the suitable configuration file:\n\n | Method  |  context (c)   |  Train splits   |  Evaluation Splits  | Config File|\n | ------- | ------ | ------ | ------ | ------ |\n | SA-M4C  | 3 | TextVQA | TextVQA | train-tvqa-eval-tvqa-c3.yml |\n | SA-M4C  | 3 | TextVQA + STVQA | TextVQA | train-tvqa_stvqa-eval-tvqa-c3.yml |\n | SA-M4C  | 3 | STVQA | STVQA | train-stvqa-eval-stvqa-c3.yml |\n | SA-M4C  | 5 | TextVQA | TextVQA | train-tvqa-eval-tvqa-c5.yml |\n\nTo run the experiments use:\n```\npython train.py \\\n--config config.yml \\\n--tag experiment-name\n```\n\n\nTo evaluate the pretrained checkpoint provided use:\n```\npython train.py \\\n--config configs/train-tvqa_stvqa-eval-tvqa-c3.yml \\\n--pretrained_eval data/pretrained-models/best_model.tar\n```\nNote: The beam-search evaluation is \nundergoing changes and will be updated.\n\n**Resources Used**: We ran all the experiments on 2 Titan Xp gpus. \n\n## Citation\n```\n@inproceedings{kant2020spatially,\n  title={Spatially Aware Multimodal Transformers for TextVQA},\n  author={Kant, Yash and Batra, Dhruv and Anderson, Peter \n          and Schwing, Alexander and Parikh, Devi and Lu, Jiasen\n          and Agrawal, Harsh},\n  booktitle={ECCV}\n  year={2020}}\n```\n\n## Acknowledgements\nParts of this codebase were borrowed from the following repositories:\n- [12-in-1: Multi-Task Vision and Language Representation Learning](https://github.com/facebookresearch/vilbert-multi-task): Training Setup\n- [MMF: A multimodal framework for vision and language research](https://github.com/facebookresearch/mmf/): Dataset processors and M4C model\n\nWe thank <a href=\"https://abhishekdas.com/\">Abhishek Das</a>, <a href=\"https://amoudgl.github.io/\">Abhinav Moudgil</a> for their feedback and <a href=\"https://ronghanghu.com/\">Ronghang Hu</a> for sharing an early version of his work. \nThe Georgia Tech effort was supported in part by NSF, AFRL, DARPA, ONR YIPs, ARO PECASE, Amazon. \nThe views and conclusions contained herein are those of the authors and should not be interpreted\n as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.\n\n\n## License\nMIT\n", "metadata": {"source": "github_readmes\\yashkant_sam-textvqa_README.md", "filename": "yashkant_sam-textvqa_README.md", "type": "readme_full"}}
{"id": "YeexiaoZheng_Multimodal-Sentiment-Analysis_README.md", "paper_id": "YeexiaoZheng_Multimodal-Sentiment-Analysis_README", "text": "# Multimodal-Sentiment-Analysis\n多模态情感分析——基于BERT+ResNet50的多种融合方法，数据学院人工智能课程第五次实验代码\n\n本项目基于Hugging Face和torchvision实现，共有五种融合方法（2Naive 3Attention），在Models文件夹中查看\n\n## Project Structure\n\n```\n|-- Multimodal-Sentiment-Analysis\n    |-- Config.py\n    |-- main.py\n    |-- README.md\n    |-- requirements.txt\n    |-- Trainer.py\n    |-- data\n    |   |-- .DS_Store\n    |   |-- test.json\n    |   |-- test_without_label.txt\n    |   |-- train.json\n    |   |-- train.txt\n    |   |-- data\n    |-- Models\n    |   |-- CMACModel.py\n    |   |-- HSTECModel.py\n    |   |-- NaiveCatModel.py\n    |   |-- NaiveCombineModel.py\n    |   |-- OTEModel.py\n    |   |-- __init__.py\n    |-- src\n    |   |-- CrossModalityAttentionCombineModel.png\n    |   |-- HiddenStateTransformerEncoderCombineModel.png\n    |   |-- OutputTransformerEncoderModel.png\n    |-- utils\n        |-- common.py\n        |-- DataProcess.py\n        |-- __init__.py\n        |-- APIs\n        |   |-- APIDataset.py\n        |   |-- APIDecode.py\n        |   |-- APIEncode.py\n        |   |-- APIMetric.py\n        |   |-- __init__.py\n```\n\n## Requirements\n\nchardet==4.0.0\nnumpy==1.22.2\nPillow==9.2.0\nscikit_learn==1.1.1\ntorch==1.8.2\ntorchvision==0.9.2\ntqdm==4.63.0\ntransformers==4.18.0\n\n```shell\npip install -r requirements.txt\n```\n\n## Model\n\n两个Naive方法就不展示了\n\n**CrossModalityAttentionCombine**\n\n![CrossModalityAttentionCombineModel](./src/CrossModalityAttentionCombineModel.png)\n\n\n\n**HiddenStateTransformerEncoder**\n\n![HiddenStateTransformerEncoderCombineModel](./src/HiddenStateTransformerEncoderCombineModel.png)\n\n**OutputTransformerEncoder**\n\n![OutputTransformerEncoderModel](./src/OutputTransformerEncoderModel.png)\n\n## Train\n\n需下载数据集，并放在data文件夹中解压，数据集地址：链接: https://pan.baidu.com/s/10fOExXqSCS4NmIjfsfuo9w?pwd=gqzm 提取码: gqzm 复制这段内容后打开百度网盘手机App，操作更方便哦\n\n```shell\npython main.py --do_train --epoch 10 --text_pretrained_model roberta-base --fuse_model_type OTE 单模态(--text_only --img_only)\n```\n\nfuse_model_type可选：CMAC、HSTEC、OTE、NaiveCat、NaiveCombine\n\ntext_pretrain_model可在Hugging Face上选择合适的\n\n## Test\n\n```shell\npython main.py --do_test --text_pretrained_model roberta-base --fuse_model_type OTE --load_model_path $your_model_path$ 单模态(--text_only --img_only)\n```\n\n## Config\n\n```python\nclass config:\n    # 根目录\n    root_path = os.getcwd()\n    data_dir = os.path.join(root_path, './data/data/')\n    train_data_path = os.path.join(root_path, 'data/train.json')\n    test_data_path = os.path.join(root_path, 'data/test.json')\n    output_path = os.path.join(root_path, 'output')\n    output_test_path = os.path.join(output_path, 'test.txt')\n    load_model_path = None\n\n    # 一般超参\n    epoch = 20\n    learning_rate = 3e-5\n    weight_decay = 0\n    num_labels = 3\n    loss_weight = [1.68, 9.3, 3.36]\n\n    # Fuse相关\n    fuse_model_type = 'NaiveCombine'\n    only = None\n    middle_hidden_size = 64\n    attention_nhead = 8\n    attention_dropout = 0.4\n    fuse_dropout = 0.5\n    out_hidden_size = 128\n\n    # BERT相关\n    fixed_text_model_params = False\n    bert_name = 'roberta-base'\n    bert_learning_rate = 5e-6\n    bert_dropout = 0.2\n\n    # ResNet相关\n    fixed_img_model_params = False\n    image_size = 224\n    fixed_image_model_params = True\n    resnet_learning_rate = 5e-6\n    resnet_dropout = 0.2\n    img_hidden_seq = 64\n\n\n    # Dataloader params\n    checkout_params = {'batch_size': 4, 'shuffle': False}\n    train_params = {'batch_size': 16, 'shuffle': True, 'num_workers': 2}\n    val_params = {'batch_size': 16, 'shuffle': False, 'num_workers': 2}\n    test_params =  {'batch_size': 8, 'shuffle': False, 'num_workers': 2}\n\n```\n\n\n\n## Result\n\n| Model                         | Acc        |\n| ----------------------------- | ---------- |\n| NaiveCat                      | 71.25      |\n| NaiveCombine                  | 73.625     |\n| CrossModalityAttentionCombine | 67.1875    |\n| HiddenStateTransformerEncoder | 73.125     |\n| **OutputTransformerEncoder**  | **74.625** |\n\n#### 消融实验\n\nOutputTransformerEncoderModel Result：（另一模态输入文本为空字符串或空白图片）\n\n| Feature    | Acc    |\n| ---------- | ------ |\n| Text Only  | 71.875 |\n| Image Only | 63     |\n\n## Reference\n\nJoint Fine-Tuning for Multimodal Sentiment Analysis：[guitld/Transfer-Learning-with-Joint-Fine-Tuning-for-Multimodal-Sentiment-Analysis: This is the code for the Paper \"Guilherme L. Toledo, Ricardo M. Marcacini: Transfer Learning with Joint Fine-Tuning for Multimodal Sentiment Analysis (LXAI Research Workshop at ICML 2022)\". (github.com)](https://github.com/guitld/Transfer-Learning-with-Joint-Fine-Tuning-for-Multimodal-Sentiment-Analysis)\n\nIs cross-attention preferable to self-attention for multi-modal emotion recognition：[smartcameras/SelfCrossAttn: PyTorch implementation of the models described in the IEEE ICASSP 2022 paper \"Is cross-attention preferable to self-attention for multi-modal emotion recognition?\" (github.com)](https://github.com/smartcameras/SelfCrossAttn)\n\nMultimodal_Sentiment_Analysis_With_Image-Text_Interaction_Network：[Multimodal Sentiment Analysis With Image-Text Interaction Network | IEEE Journals & Magazine | IEEE Xplore](https://ieeexplore.ieee.org/abstract/document/9736584/)\n\nCLMLF：[Link-Li/CLMLF (github.com)](https://github.com/Link-Li/CLMLF)\n", "metadata": {"source": "github_readmes\\YeexiaoZheng_Multimodal-Sentiment-Analysis_README.md", "filename": "YeexiaoZheng_Multimodal-Sentiment-Analysis_README.md", "type": "readme_full"}}
{"id": "ygjin11_r2-play_README.md", "paper_id": "ygjin11_r2-play_README", "text": "# Read to Play (R2-Play)\n\n## Overview\nOfficial codebase for Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction. This paper explores multimodal guidance for agents enabling them to comprehend gameplay instructions, thereby facilitating a “read-to-play” capability.\n\nCreating a generalist agent that can accomplish diverse tasks is an enduring goal in artificial intelligence. The recent advancement in integrating textual guidance or visual trajectory into a single decision-making agent presents a potential solution. This line of research provides task-specific context to guide the agent. Although textual guidance and visual trajectory each offer advantages, they also have distinct limitations: (1) Textual guidance lacks visually-grounded information, which diminishes its expressiveness for decision-making tasks based on visual observations; (2) Without clear task instructions, deriving an effective strategy from a visual trajectory is extremely difficult, which is similar to people's difficulty understanding player intentions when watching game videos without explanations. The complementary relationship between textual guidance and visual trajectory suggests their combination enhances guidance effectiveness, as illustrated in Figure 1. As a result, this paper aims to develop an agent capable of adapting to new tasks through multimodal guidance.\n\n<div align=center><img src=\"teaser.jpg\"  width=\"50%\" height=\"auto\"></div>\n<center><b><font size ='1'>Figure1: Imagine an agent learning to play Palworld (a Pok\\'emon-like game). (1) The agent exhibits confusion when only relying on textual guidance. (2) The agent is confused when presented with images of a Pal sphere and a Pal. (3) The agent understands how to catch a pet through multimodal guidance, which combines textual guidance with images of the Pal sphere and Pal.</font></b></center></font>\n\n## Install Dependencies\nDependencies can be installed with the following command:\n```\nconda env create -f env.yml\n```\n\n## Download Offline Datasets\nDownload the dataset and place it in the data/dataset.\n```\ngsutil -m cp -R gs://atari-replay-datasets/dqn/[GAME_NAME] [DIRECTORY_NAME]\n```\n\n## Game Instruction Set\nThe Game Instruction Set comprises the following components stored in the designated directories:\n* Game Description (stored in data/instruction/raw/desc)\n* Game Trajectory (stored in data/instruction/raw/traj)\n* Game Guidance (stored in data/instruction/raw/guid)\n```\n# a example of Game Guidance\n\"1\": {\n  \"action\": \"[noop]\",\n  \"guidance\": \"noop\",\n  \"elements\": {\n  \"ball\": [\n    [\n    42,\n    323\n    ],\n    [\n    51,\n    333\n    ]\n  ],\n  \"paddle\": [\n    [\n    33,\n    463\n    ],\n    [\n    102,\n    472\n    ]\n  ]\n  }\n}\n```\n\n## Train\nThe training configuration is available at config/config_main/train.yaml.\n```\nsh offline/run.sh \n```\n\n## Download Weights\nWe provide model weights trained on large-scale offline datasets of ~4M(100k × 37) and ~8M(200k × 37) respectively:\n  * [4M](https://drive.google.com/file/d/1va6rVOgmnIfqpOX4xxLW1IyiEeCuR6l2/view?usp=drive_link)\n  * [8M](https://drive.google.com/file/d/12NaAWil_0GAfiFWCTMAA77JC1UE2gywH/view?usp=drive_link)\n\n\n## Evaluation \nThe evaluation configuration is available at config/config_main/eval.yaml.\n```\nsh eval/run.sh\n```\n\n## Citation\n```\n@article{Jin2024,\n  title={Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction},\n  author={Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He, Jie Fu},\n  journal={arXiv preprint arXiv:2402.04154},\n  year={2024}\n}\n```\n", "metadata": {"source": "github_readmes\\ygjin11_r2-play_README.md", "filename": "ygjin11_r2-play_README.md", "type": "readme_full"}}
{"id": "yikaiw_TokenFusion_README.md", "paper_id": "yikaiw_TokenFusion_README", "text": "# Multimodal Token Fusion for Vision Transformers\n\nBy Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, Yunhe Wang.\n\n[**[Paper]**](https://arxiv.org/pdf/2204.08721.pdf)\n\nThis repository is a PyTorch implementation of \"Multimodal Token Fusion for Vision Transformers\", in CVPR 2022. \n\n<div align=\"center\">\n   <img src=\"./figs/framework.png\" width=\"960\">\n</div>\n\nHomogeneous predictions,\n<div align=\"center\">\n   <img src=\"./figs/homogeneous.png\" width=\"720\">\n</div>\n\nHeterogeneous predictions,\n<div align=\"center\">\n   <img src=\"./figs/heterogeneous.png\" width=\"720\">\n</div>\n\n\n## Datasets\n\nFor semantic segmentation task on NYUDv2 ([official dataset](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)), we provide a link to download the dataset [here](https://drive.google.com/drive/folders/1mXmOXVsd5l9-gYHk92Wpn6AcKAbE0m3X?usp=sharing). The provided dataset is originally preprocessed in this [repository](https://github.com/DrSleep/light-weight-refinenet), and we add depth data in it.\n\nFor image-to-image translation task, we use the sample dataset of [Taskonomy](http://taskonomy.stanford.edu/), where a link to download the sample dataset is [here](https://github.com/alexsax/taskonomy-sample-model-1.git).\n\nPlease modify the data paths in the codes, where we add comments 'Modify data path'.\n\n\n## Dependencies\n```\npython==3.6\npytorch==1.7.1\ntorchvision==0.8.2\nnumpy==1.19.2\n```\n\n\n## Semantic Segmentation\n\n\nFirst, \n```\ncd semantic_segmentation\n```\n\nDownload the [segformer](https://github.com/NVlabs/SegFormer) pretrained model (pretrained on ImageNet) from [weights](https://drive.google.com/drive/folders/1b7bwrInTW4VLEm27YawHOAMSMikga2Ia), e.g., mit_b3.pth. Move this pretrained model to folder 'pretrained'.\n\nTraining script for segmentation with RGB and Depth input,\n```\npython main.py --backbone mit_b3 -c exp_name --lamda 1e-6 --gpu 0 1 2\n```\n\nEvaluation script,\n```\npython main.py --gpu 0 --resume path_to_pth --evaluate  # optionally use --save-img to visualize results\n```\n\nCheckpoint models, training logs, mask ratios and the **single-scale** performance on NYUDv2 are provided as follows:\n\n| Method | Backbone | Pixel Acc. (%) | Mean Acc. (%) | Mean IoU (%) | Download | \n|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|\n|[CEN](https://github.com/yikaiw/CEN)| ResNet101 | 76.2 | 62.8 | 51.1 | [Google Drive](https://drive.google.com/drive/folders/1wim_cBG-HW0bdipwA1UbnGeDwjldPIwV?usp=sharing)|\n|[CEN](https://github.com/yikaiw/CEN)| ResNet152 | 77.0 | 64.4 | 51.6 | [Google Drive](https://drive.google.com/drive/folders/1DGF6vHLDgBgLrdUNJOLYdoXCuEKbIuRs?usp=sharing)|\n|Ours| SegFormer-B3 | 78.7 | 67.5 | 54.8 | [Google Drive](https://drive.google.com/drive/folders/14fi8aABFYqGF7LYKHkiJazHA58OBW1AW?usp=sharing)|\n\n\nMindspore implementation is available at: https://gitee.com/mindspore/models/tree/master/research/cv/TokenFusion\n\n## Image-to-Image Translation\n\nFirst, \n```\ncd image2image_translation\n```\nTraining script, from Shade and Texture to RGB,\n```\npython main.py --gpu 0 -c exp_name\n```\nThis script will auto-evaluate on the validation dataset every 5 training epochs. \n\nPredicted images will be automatically saved during training, in the following folder structure:\n\n```\ncode_root/ckpt/exp_name/results\n  ├── input0  # 1st modality input\n  ├── input1  # 2nd modality input\n  ├── fake0   # 1st branch output \n  ├── fake1   # 2nd branch output\n  ├── fake2   # ensemble output\n  ├── best    # current best output\n  │    ├── fake0\n  │    ├── fake1\n  │    └── fake2\n  └── real    # ground truth output\n```\n\nCheckpoint models:\n\n| Method | Task | FID | KID | Download | \n|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|\n| [CEN](https://github.com/yikaiw/CEN) |Texture+Shade->RGB | 62.6 | 1.65 | - |\n| Ours | Texture+Shade->RGB | 45.5 | 1.00 | [Google Drive](https://drive.google.com/drive/folders/1vkcDv5bHKXZKxCg4dC7R56ts6nLLt6lh?usp=sharing)|\n\n## 3D Object Detection (under construction)\n\nData preparation, environments, and training scripts follow [Group-Free](https://github.com/zeliu98/Group-Free-3D) and [ImVoteNet](https://github.com/facebookresearch/imvotenet).\n\nE.g.,\n```\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --master_port 2229 --nproc_per_node 4 train_dist.py --max_epoch 600 --val_freq 25 --save_freq 25 --lr_decay_epochs 420 480 540 --num_point 20000 --num_decoder_layers 6 --size_cls_agnostic --size_delta 0.0625 --heading_delta 0.04 --center_delta 0.1111111111111 --weight_decay 0.00000001 --query_points_generator_loss_coef 0.2 --obj_loss_coef 0.4 --dataset sunrgbd --data_root . --use_img --log_dir log/exp_name\n```\n\n## Citation\n\nIf you find our work useful for your research, please consider citing the following paper.\n```\n@inproceedings{wang2022tokenfusion,\n  title={Multimodal Token Fusion for Vision Transformers},\n  author={Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wenbing and Sun, Fuchun and Wang, Yunhe},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2022}\n}\n```\n\n\n", "metadata": {"source": "github_readmes\\yikaiw_TokenFusion_README.md", "filename": "yikaiw_TokenFusion_README.md", "type": "readme_full"}}
{"id": "YiLunLee_missing_aware_prompts_README.md", "paper_id": "YiLunLee_missing_aware_prompts_README", "text": "# Multimodal Prompting with Missing Modalities for Visual Recognition (CVPR 2023)\nOfficial PyTorch implementaton of CVPR 2023 paper \"[Multimodal Prompting with Missing Modalities for Visual Recognition](https://arxiv.org/abs/2303.03369)\".  \nYou can visit our project website [here](https://yilunlee.github.io/missing_aware_prompts/).\n\n## Introduction\nIn this paper, we tackle two challenges in multimodal learning for visual recognition: 1) when missing-modality occurs either during training or testing in real-world situations; and 2) when the computation resources are not available to finetune on heavy transformer models. To this end, we propose to utilize prompt learning and mitigate the above two challenges together. Specifically, our modality-missing-aware prompts can be plugged into multimodal transformers to handle general missing-modality cases, while only requiring less than 1% learnable parameters compared to training the entire model. \n\n<div align=\"center\">\n  <img src=\"fig/model.jpeg\"/>\n</div>\n\n## Usage\n### Enviroment\n#### Prerequisites\nPython = 3.7.13\n\nPytorch = 1.10.0\n\nCUDA = 11.3\n\n#### Other requirements\n```\npip install -r requirements.txt\n```\n\n### Prepare Dataset\nWe use three vision and language datasets: [MM-IMDb](https://github.com/johnarevalo/gmu-mmimdb), [UPMC Food-101](https://visiir.isir.upmc.fr/explore), and [Hateful Memes](https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set/). Please download the datasets by yourself. We use `pyarrow` to serialize the datasets, the conversion codes are located in `vilt/utils/wirte_*.py`. Please see `DATA.md` to organize the datasets, otherwise you may need to revise the `write_*.py` files to meet your dataset path and files. Run the following script to create the pyarrow binary file:\n```\npython make_arrow.py --dataset [DATASET] --root [YOUR_DATASET_ROOT]\n```\n\n### Evaluation\n```\npython run.py with data_root=<ARROW_ROOT> \\\n        num_gpus=<NUM_GPUS> \\\n        num_nodes=<NUM_NODES> \\\n        per_gpu_batchsize=<BS_FITS_YOUR_GPU> \\\n        <task_finetune_mmimdb or task_finetune_food101 or task_finetune_hatememes> \\\n        load_path=<MODEL_PATH> \\\n        exp_name=<EXP_NAME> \\\n        prompt_type=<PROMPT_TYPE> \\\n        test_ratio=<TEST_RATIO> \\\n        test_type=<TEST_TYPE> \\\n        test_only=True     \n```\n\n### Train\n1. Download the pre-trained ViLT model weights from [here](https://github.com/dandelin/ViLT.git).\n\n2. Start to train.\n```\npython run.py with data_root=<ARROW_ROOT> \\\n        num_gpus=<NUM_GPUS> \\\n        num_nodes=<NUM_NODES> \\\n        per_gpu_batchsize=<BS_FITS_YOUR_GPU> \\\n        <task_finetune_mmimdb or task_finetune_food101 or task_finetune_hatememes> \\\n        load_path=<PRETRAINED_MODEL_PATH> \\\n        exp_name=<EXP_NAME>\n```\n\n\n## Citation\nIf you find this work useful for your research, please cite:\n```Bibtex\n@inproceedings{lee2023cvpr,\n title = {Multimodal Prompting with Missing Modalities for Visual Recognition},\n author = {Yi-Lun Lee and Yi-Hsuan Tsai and Wei-Chen Chiu and Chen-Yu Lee},\n booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n year = {2023}\n}\n```\n\n## Acknowledgements\nThis code is based on [ViLT](https://github.com/dandelin/ViLT.git).\n", "metadata": {"source": "github_readmes\\YiLunLee_missing_aware_prompts_README.md", "filename": "YiLunLee_missing_aware_prompts_README.md", "type": "readme_full"}}
{"id": "ymhzyj_UMMAFormer_README.md", "paper_id": "ymhzyj_UMMAFormer_README", "text": "<div align=\"center\">\n\n## **\\[ACM MM'23\\] UMMAFormer**: A Universal Multimodal-adaptive Transformer Framework For Temporal Forgery Localization\n\n<a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a>\n[![Conference](https://img.shields.io/badge/ACM%20MM-2023-orange)](https://www.acmmm2023.org/)\n\n\n<a href=\"https://paperswithcode.com/sota/temporal-forgery-localization-on-lav-df?p=ummaformer-a-universal-multimodal-adaptive-1\">\n    <img src=\"https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/ummaformer-a-universal-multimodal-adaptive-1/temporal-forgery-localization-on-lav-df&style=flat-square\">\n</a>\n\n<!-- [![License](https://img.shields.io/badge/license-Apache%202-blue)](https://github.com/nku-shengzheliu/SER30K/blob/main/LICENSE) -->\n\n</div>\n\n\n<div align=center>\n    <a href=\"https://ymhzyj.github.io/\" target=\"_blank\">Rui Zhang</a>\n    <!-- Rui Zhang -->\n    <a href=\"https://ccs.scu.edu.cn/info/1052/2601.htm\" target=\"_blank\">Hongxia wang</a>\n    Mingshan Du\n    Hanqing Liu\n    Yang Zhou\n    Qiang Zeng\n</div>\n<div align=center>\nSchool of Cyber Science and Engineering, Sichuan University\n</div>\n\n**Temporal Video Inpainting Localization(TVIL) dataset** and pytorch training/validation code for **UMMAFormer**. This is the official repository of our Work accepted to ACM MM'23. If you have any question, please contact _zhangrui1997[at]stu.scu.edu.cn_. The paper can be found in [arxiv](https://arxiv.org/abs/2308.14395)　or [ACM MM](https://dl.acm.org/doi/abs/10.1145/3581783.3613767).\n\n<p align=\"center\">\n<img src=\"./figures/overview_frameworks.png\" alt=\"drawing\" width=\"70%\" height=\"70%\"/>\n    <h4 align=\"center\">Overview of UMMAFormer</h4>\n</p>\n\n\n## Abstract\n\nThe emergence of  artificial intelligence-generated content~(AIGC) has raised concerns about the authenticity of multimedia content in various fields. Existing research has limitations and is not widely used in industrial settings, as it is only focused on binary classification tasks of complete videos. We propose a novel universal transformer framework for temporal forgery localization (TFL) called UMMAFormer, which predicts forgery segments with multimodal adaptation. We also propose a Temporal Feature Abnormal Attention (TFAA) module based on temporal feature reconstruction to enhance the detection of temporal differences. In addition, we introduce a parallel cross-attention feature pyramid network (PCA-FPN) to optimize the Feature Pyramid Network (FPN) for subtle feature enhancement. To address the lack of available datasets, we introduce a novel temporal video inpainting localization (TVIL) dataset that is specifically tailored for video inpainting scenes. Our experiments demonstrate that our proposed method achieves state-of-the-art performance on benchmark datasets, Lav-DF, TVIL, Psynd surpassing the previous best results significantly.\n\n<p align=\"center\">\n<img src=\"./figures/vilsamples.png\" alt=\"drawing\" width=\"100%\" height=\"100%\"/>\n    <h4 align=\"center\">Movitation of UMMAFormer</h4>\n</p>\n\n\n\n## TVIL dataset\n\n### a. Data Download\nIf you need the TVIL dataset for academic purposes, please download the full data from [BaiduYun Disk](https://pan.baidu.com/s/1h3sHu56z3slJnPCH47QRsg?pwd=8tj1) (8tj1) or [OneDrive](https://1drv.ms/f/s!AgVmq0AY0Su8grknz_DKYQN2IbA63Q?e=hi5mbj).\n\n### b. Data Sources\nThe raw data is coming from [Youtube VOS 2018](https://codalab.lisn.upsaclay.fr/competitions/7685#participate-get_data).\n\n<!-- 原图 -->\n\n### c. Inpainting Methods\nWe use four different video inpainting methods to create new videos. They are [E2FGVI](https://github.com/MCG-NKU/E2FGVI), [FGT](https://github.com/hitachinsk/FGT), [FuseFormer](https://github.com/ruiliu-ai/fuseformer), and [STTN](https://github.com/researchmm/STTN), respectively. We used [XMEM](https://github.com/hkchengrex/XMem) to generate the inpainting mask.\n\n\n![Inpainting Sample](figures/gifs/5ef6573a99_result.gif)\n![Inpainting Sample](figures/gifs/c3bb62a2f7_result.gif)\n\n\n\n### d. Feature Extract\nWe also provided [TSN features](https://pan.baidu.com/s/1h3sHu56z3slJnPCH47QRsg?pwd=8tj1) (code：8tj1) as used in the paper, specifically extracted by [mmaction2==0.24.1](https://github.com/open-mmlab/mmaction2).\n\n\n\n\n## Code\n\n### Requirements\n\n- Linux\n- Python 3.5+\n- PyTorch 1.11\n- TensorBoard\n- CUDA 11.0+\n- GCC 4.9+\n- NumPy 1.11+\n- PyYaml\n- Pandas\n- h5py\n- joblib\n- einops\n\n\n### Compilation\n\nPart of NMS is implemented in C++. The code can be compiled by\n\n```shell\ncd ./libs/utils\npython setup.py install --user\ncd ../..\n```\n\nThe code should be recompiled every time you update PyTorch.\n\n### To Reproduce Our Results\n\n1. Download Features and Annotations\n    We provided the following features and annotations for download:\n    \n    annotations and features of Lav-DF from [BaiduYun](https://pan.baidu.com/s/1GGwfkjjYlBtQyB4uVU38mA?pwd=k6jq) (code：k6jq) or [OneDrive](https://1drv.ms/f/s!AgVmq0AY0Su8gr05BrQEdS2BE9f75A?e=8L9GFk)\n\n    annotations and features of Psynd from [BaiduYun](https://pan.baidu.com/s/1CgkDNeisV9HtfqEzzYyIhQ?pwd=m6iq) (code：m6iq) or [OneDrive](https://1drv.ms/f/s!AgVmq0AY0Su8gv5hWzRE2sWkumDcCA?e=GSPe2z)\n\n    annotations and features of TVIL from [BaiduYun Disk](https://pan.baidu.com/s/1h3sHu56z3slJnPCH47QRsg?pwd=8tj1) (8tj1) or [OneDrive](https://1drv.ms/f/s!AgVmq0AY0Su8grknz_DKYQN2IbA63Q?e=hi5mbj)\n\n    These features are the same as those used in our paper and are extracted using the bylo-a and tsn models. They can be directly used for training and testing. The labels, on the other hand, have been converted from their original different forms to fit the format of our code. The ground truth values remain unchanged and are the same as the original ones.\n\n    **optional**\n\n    _You also can extract features on your own using [mmaction==0.24.1](https://github.com/open-mmlab/mmaction2) and [BYOL-A](https://github.com/nttcslab/byol-a). First, you need to apply to the official source for the original datasets from [Lav-DF](https://github.com/ControlNet/LAV-DF) and [Psynd](https://scholarbank.nus.edu.sg/handle/10635/227398). Then you need to download [mmaction==0.24.1](https://github.com/open-mmlab/mmaction2) and [BYOL-A](https://github.com/nttcslab/byol-a) and set up the environment following official instructions. Furthermore, you need to extract frames and optical flow from the videos. You can use mmaction for this purpose. In the case of Lav-DF, you also need to separate the corresponding audio from the original video. And The pre-trained model also needs to be downloaded from [tsn_rgb](https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_320p_1x1x8_50e_activitynet_clip_rgb/tsn_r50_320p_1x1x8_50e_activitynet_clip_rgb_20210301-c0f04a7e.pth) and [tsn_flow](https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_320p_1x1x8_150e_activitynet_clip_flow/tsn_r50_320p_1x1x8_150e_activitynet_clip_flow_20200804-8622cf38.pth)._\n    _You can use the following command to generate a video list txt file for Lav-DF and extract visual features._\n    ```shell\n    python tools/gen_lavdf_filelist.py\n    bash tools/gen_tsn_features_lavdf.sh\n    ```\n\n    _For audio features, please put the code tools/byola_extract_lavdf.py in the BYOL-A directory and use following command._\n    ```shell\n    python bylo-a/byola_extract_lavdf.py\n    ```\n\n2. Unpack Features and Annotations\n\n* Unpack the file under *./data* (or elsewhere and link to *./data*).\n* The folder structure should look like\n```\nThis folder\n│   README.md\n│   ...  \n│\n└───data/\n│    └───lavdf/\n│    │\t └───annotations\n│    │\t └───feats   \n│    │\t 　　　└───byola   \n│    │\t 　　　   └───train   \n│    │\t 　　　   └───dev   \n│    │\t 　　　   └───test   \n│    │\t 　　  └───tsn   \n│    │\t 　　　   └───flow\n│    │\t 　　　      └───train   \n│    │\t 　　　      └───dev   \n│    │\t 　　　      └───test  \n│    │\t         └───rgb\n│    │\t 　　　      └───train   \n│    │\t 　　　      └───dev   \n│    │\t 　　　      └───test\n│    └───...\n|\n└───libs\n│\n│   ...\n```\n\n3. Training and Evaluation\n   Train our UMMAFormer with TSN and BYOL-A features. This will create an experiment folder _./paper_results_ that stores training config, logs, and checkpoints.\n    ```shell\n    python ./train.py ./configs/UMMAFormer/dataset.yaml\n    ```\n   Then you can run the evaluation process using the loaded model and evaluation dataset.\n    ```shell\n    python ./eval.py ./configs/UMMAFormer/dataset.yaml ./paper_results/dataset/model_best.pth.tar\n    ```\n    To modify the configuration file for psynd, you need to change the value of \"test_split\" to the corresponding subset name, such as \"test_cellular\" or \"test_landline.\"　For each subset, you can calculate IoU for each subset using the following command:\n    ```shell\n    python tools/test_miou.py\n    ```\n    You need to modify the 'split' variable in the code, as well as the addresses of the labels and results.\n4. Evaluating Our Pre-trained Model\n   We also provide a pre-trained models. The following link is for Baidu cloud drive. Considering that some users may not be able to access it, we have additionally provided a [OneDrive link](https://1drv.ms/u/s!AgVmq0AY0Su8grtFydwbVa-fdxMdmQ?e=H71mMB).\n\n| Dataset        | Modal | Config | Pretrained |  AP@0\\.5 |  AP@0\\.75 | AP@0\\.95 | AR@10    | AR@20  |  AR@50 | AR@100 |\n|----------------|-------|--------|------------|----------|-----------|----------|----------|--------|--------|--------|\n| Lav\\-DF        | V     | [Yaml](./configs/UMMAFormer/lavdf_tsn.yaml)   | [Ckpt](https://pan.baidu.com/s/1oL2BVmBZ4Bk3FQ7qg9Rv6Q?pwd=9rhd)       | 97\\.30   | 92\\.96    | 25\\.68   | 90\\.19   | 90\\.85 | 91\\.14 | 91\\.18 |\n| Lav\\-DF        | V\\+A  | [Yaml](./configs/UMMAFormer/lavdf_tsn_byola.yaml)   | [Ckpt](https://pan.baidu.com/s/1gjhiAi7pdGJ8ml-lct4nkw?pwd=gms3)       | 98\\.83   | 95\\.54    | 37\\.61   | 92\\.10   | 92\\.42 | 92\\.47 | 92\\.48 |\n| Lav\\-DF Subset | V     | [Yaml](./configs/UMMAFormer/lavdfvm_tsn.yaml)   | [Ckpt](https://pan.baidu.com/s/18h8FIc6-lzCl0lNBjVD3fA?pwd=ju3r)       | 98\\.83   | 95\\.95    | 30\\.11   | 92\\.32   | 92\\.65 | 92\\.74 | 92\\.75 |\n| Lav\\-DF Subset | V\\+A  | [Yaml](./configs/UMMAFormer/lavdfvm_tsn_byola.yaml)   | [Ckpt](https://pan.baidu.com/s/1WLtze3rgRl8Is_5azZ-WQw?pwd=axn5)  | 98\\.54   | 94\\.30    | 37\\.52   | 91\\.61   | 91\\.97 | 92\\.06 | 92\\.06 |\n| TVIL           | V     | [Yaml](./configs/UMMAFormer/tvil_tsn.yaml)   | [Ckpt](https://pan.baidu.com/s/1_YFse6-Bvhlqb_JrzbMPPg?pwd=dmra)       | 88\\.68   | 84\\.70    | 62\\.43   | 87\\.09   | 88\\.21 | 90\\.43 | 91\\.16 |\n| Psynd\\-Test    | A     | [Yaml](./configs/UMMAFormer/psynd_byola.yaml)   | [Ckpt](https://pan.baidu.com/s/1sgNVaITzXz0qVQgSj0vf0A?pwd=ndh6)       | 100\\.00  | 100\\.00   | 79\\.87   | 97\\.60   | 97\\.60 | 97\\.60 | 97\\.60 |\n\n\n## Results\n\n<img src=\"figures/gifs/000000_1.gif\" alt=\"TFL Result 1 (Lav-DF)\" style=\"zoom:30%;\"/>\n<img src=\"figures/gifs/000014_1.gif\" alt=\"TFL Result ２ (Lav-DF)\" style=\"zoom:30%;\"/>\n<img src=\"figures/gifs/522ea1a892_result_1.gif\" alt=\"TFL Result 1 (TVIL)\" style=\"zoom:30%;\"/>\n<img src=\"figures/gifs/ea8a5b5a78_result_2.gif\" alt=\"TFL Result ２ (TVIL)\" style=\"zoom:30%;\"/>\n\n\n\n## TODO List\n- [x] Release full code.\n- [x] Release TVIL datasets and TSN features.\n- [x] Release TSN features and BYOL-A features for Lav-DF and Psynd \n- [x] Release our pre-trained model\n\n\n\n## Cite UMMAFormer\n\n```shell\n@inproceedings{DBLP:conf/mm/ZhangWDLZZ23,\n  author       = {Rui Zhang and\n                  Hongxia Wang and\n                  Mingshan Du and\n                  Hanqing Liu and\n                  Yang Zhou and\n                  Qiang Zeng},\n  title        = {UMMAFormer: {A} Universal Multimodal-adaptive Transformer Framework\n                  for Temporal Forgery Localization},\n  booktitle    = {Proceedings of the 31st {ACM} International Conference on Multimedia,\n                  {MM} 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023},\n  pages        = {8749--8759},\n  publisher    = {{ACM}},\n  year         = {2023},\n  url          = {https://doi.org/10.1145/3581783.3613767},\n  doi          = {10.1145/3581783.3613767},\n}\n```\n\n## Acknowledgement\n\nThanks for the work of [Actionformer](https://github.com/happyharrycn/actionformer_release). Our code is based on the implementation of them.\n", "metadata": {"source": "github_readmes\\ymhzyj_UMMAFormer_README.md", "filename": "ymhzyj_UMMAFormer_README.md", "type": "readme_full"}}
{"id": "youngbin-ro_audiotext-transformer_README.md", "paper_id": "youngbin-ro_audiotext-transformer_README", "text": "# korean-audiotext-transformer\n> Multimodal Transformer for Korean Sentiment Analysis with Audio and Text Features\n\n<br/>\n\n## Overview\n![overview](https://github.com/youngbin-ro/korean-audiotext-transformer/blob/master/images/overview.png?raw=true)\n\n- **STEP1**: Convert input audio to text using [Google ASR API](https://cloud.google.com/speech-to-text/)\n- **STEP2**: Extract MFCC feature from input audio\n- **STEP3**: Conduct MLM on [KoBERT](http://aiopen.etri.re.kr/service_dataset.php) through colloquial review texts crawled from various services\n- **STEP4**: Extract sentence embedding by using the text obtained in **STEP1** as an input variable of the BERT learned in **STEP3**\n- **STEP5**: Obtain fused representation by using the MFCC feature (from **STEP2**) and the sentence embedding (from **STEP4**) as input variables of the Crossmodal Transformer ([Tsai et al., 2019](https://www.aclweb.org/anthology/P19-1656/))\n\n<br/>\n\n## Datasets\n#### Korean Multimodal Sentiment Analysis Dataset\n\n- 자율지능 디지털 동반자 [감정 분류용 데이터셋](http://aicompanion.or.kr/nanum/tech/data_introduce.php?offset=8&idx=23) (registration and authorization are needed for downloading)\n- **Classes**: 분노(anger), 공포(fear), 중립(neutrality), 슬픔(sadness), 행복(happiness), 놀람(surprise), 혐오(disgust)\n- **Provided Modality**: Video, Audio, Text\n  - We only use audio and text data\n  - When testing, text is obtained via ASR from audio without using the data provided\n  - Vision modality is not considered in this project\n- **Train / Dev / Test Split**\n  \n  - Based on audio: 8278 / 1014 / 1003\n  - Based on text: 280 / 35 / 35\n- **Preprocess**\n  \n  - Locate downloaded dataset as follows:\n  \n    ```\n    korean-audiotext-transformer/    \n    └── data/\n        └── 4.1 감정분류용 데이터셋/\n            ├── 000/\n            ├── 001/\n            ├── 002/\n            ├── ...\n            ├── 099/\n            ├── participant_info.xlsx\n            ├── rename_file.sh\n            ├── Script.hwp\n            └── test.py\n    ```\n  \n  - Convert ```Script.hwp``` to ```script.txt```\n  \n    ```\n    cd data/4.1 감정분류용 데이터셋\n    hwp5txt Script.hwp --output script.txt\n    ```\n  \n  - Generate ```{train, dev, test}.pkl```\n  \n    ```\n    python preprocess.py \\\n      raw_path='./data/4.1 감정분류용 데이터셋' \\\n      script_path'./data/4.1 감정분류용 데이터셋/script.txt' \\\n      save_path='./data' \\\n      train_size=.8\n    ```\n\n- **Preprocessed Output** (<i>train.pkl</i>)\n\n| person_idx |                                             audio |            sentence | emotion |\n| ---------: | ------------------------------------------------: | ------------------: | ------: |\n|          0 | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 오늘 입고 나가야지. |    행복 |\n|          2 | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 오늘 입고 나가야지. |    행복 |\n|          7 | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 오늘 입고 나가야지. |    행복 |\n|         12 | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 오늘 입고 나가야지. |    행복 |\n|         17 | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 오늘 입고 나가야지. |    행복 |\n\n<br/>\n\n## Usage\n#### Prerequisites\n- We recommend to use conda environment to setup\n```\nconda create -n <your_env_name> python=3.6\nconda activate <your_env_name>\nconda install pip\npip install -r requirements.txt\n```\n\n#### Before training,\n\nDownload fine-tuned [BERT](https://drive.google.com/file/d/1WI-FLaMG-5TXwkykF3iUQJ1zYrbZSvzu/view?usp=sharing), and locate the model as follows:\n\n- The model was already fine-tuned with a [Korean sentimental analysis dataset](http://aicompanion.or.kr/nanum/tech/data_introduce.php?idx=47)\n- We use this model as a text embedding module (do not fine-tune anymore)\n\n```\nkorean-audiotext-transformer/    \n└── KoBERT/\n    ├── args.bin\n    ├── config.json\n    ├── model.bin\n    ├── tokenization.py\n    └── vocab.list\n```\n\n#### Train AudioText Transformer\n\n- specified hyper-parameters are the best ones on development dataset\n\n```shell\npython train.py \\\n  --data_path='./data' \\\n  --bert_path='./KoBERT' \\\n  --save_path='./result' \\\n  --attn_dropout=.2 \\\n  --relu_dropout=.1 \\\n  --emb_dropout=.2 \\\n  --res_dropout=.1 \\\n  --out_dropout=.1 \\\n  --n_layers=2 \\\n  --d_model=64 \\\n  --n_heads=8 \\\n  --lr=1e-5 \\\n  --epochs=10 \\\n  --batch_size=64 \\\n  --clip=1.0 \\\n  --warmup_percent=.1 \\\n  --max_len_audio=400 \\\n  --sample_rate=48000 \\\n  --resample_rate=16000 \\\n  --n_fft_size=400 \\\n  --n_mfcc=40\n```\n\n#### Train Audio-Only Baseline\n\n```shell\npython train.py --only_audio \\\n  --n_layers=4 \\\n  --n_heads=8 \\\n  --lr=1e-3 \\\n  --epochs=10 \\\n  --batch_size=64 \\\n```\n\n#### Train Text-Only Baseline\n\n```shell\npython train.py --only_text \\\n  --n_layers=4 \\\n  --n_heads=8 \\\n  --lr=1e-3 \\\n  --epochs=10 \\\n  --batch_size=64 \\\n```\n\n#### Evaluate Models\n\n```shell\npython eval.py [--FLAGS]\n```\n\n<br/>\n\n## Results\n### Text-Only Baseline\n![text_only](https://github.com/youngbin-ro/korean-audiotext-transformer/blob/master/images/text_only.png?raw=true)\n\n| Emotion  |    Total    | 공포  | 놀람  | 분노  | 슬픔  | 중립  | 행복  | 혐오 |\n| :------: | :---------: | :---: | :---: | :---: | :---: | :---: | :---: | :--: |\n| F1-score | **33.95** | 75.00 | 33.33 | 44.44 | 22.22 | 18.18 | 44.44 | 0.00 |\n\n![text_cm](https://github.com/youngbin-ro/korean-audiotext-transformer/blob/master/images/text_cm.png?raw=true)\n\n------------------\n\n\n### Audio-Only Baseline\n![audio_only](https://github.com/youngbin-ro/korean-audiotext-transformer/blob/master/images/audio_only.png?raw=true)\n\n| Emotion  |    Total    | 공포  | 놀람 | 분노  | 슬픔  | 중립  | 행복  | 혐오 |\n| :------: | :---------: | :---: | :--: | :---: | :---: | :---: | :---: | :--: |\n| F1-score | **35.28** | 31.84 | 42.68 | 24.71 | 47.32 | 35.80 | 44.52 | 20.12 |\n\n![audio_cm](https://github.com/youngbin-ro/korean-audiotext-transformer/blob/master/images/audio_cm.png?raw=true)\n\n------------------\n\n\n### Multimodal (Crossmodal) Transformer\n![crossmodal](https://github.com/youngbin-ro/korean-audiotext-transformer/blob/master/images/crossmodal.png?raw=true)\n\n| Emotion  |    Total    | 공포  | 놀람  | 분노  | 슬픔  | 중립  | 행복  | 혐오 |\n| :------: | :---------: | :---: | :---: | :---: | :---: | :---: | :---: | :--: |\n| F1-score | **52.54** | 44.18 | 34.44 | 50.95 | 81.81 | 34.28 | 65.93 | 56.19 |\n\n![cross_cm](https://github.com/youngbin-ro/korean-audiotext-transformer/blob/master/images/cross_cm.png?raw=true)\n\n<br/>\n\n## References\n- https://github.com/yaohungt/Multimodal-Transformer\n- https://github.com/Donghwa-KIM/audiotext-transformer\n- https://github.com/JhnLee/multimodal-transformer\n", "metadata": {"source": "github_readmes\\youngbin-ro_audiotext-transformer_README.md", "filename": "youngbin-ro_audiotext-transformer_README.md", "type": "readme_full"}}
{"id": "Yuan-ManX_ComfyUI-HunyuanVideo-Avatar_README.md", "paper_id": "Yuan-ManX_ComfyUI-HunyuanVideo-Avatar_README", "text": "# ComfyUI-HunyuanVideo-Avatar\n\nComfyUI-HunyuanVideo-Avatar is now available in ComfyUI, [HunyuanVideo-Avatar](https://github.com/Tencent-Hunyuan/HunyuanVideo-Avatar) is a multimodal diffusion transformer (MM-DiT)-based model capable of simultaneously generating dynamic, emotion-controllable, and multi-character dialogue videos. \n\n\n## Installation\n\n1. Make sure you have ComfyUI installed\n\n2. Clone this repository into your ComfyUI's custom_nodes directory:\n```\ncd ComfyUI/custom_nodes\ngit clone https://github.com/Yuan-ManX/ComfyUI-HunyuanVideo-Avatar.git\n```\n\n3. Install dependencies:\n```\ncd ComfyUI-HunyuanVideo-Avatar\npip install -r requirements.txt\n```\n\n### Installation Guide for Linux\n\nWe recommend CUDA versions 12.4 or 11.8 for the manual installation.\n\nConda's installation instructions are available [here](https://docs.anaconda.com/free/miniconda/index.html).\n\n```shell\n\n# Install PyTorch and other dependencies using conda\n# For CUDA 11.8\nconda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n# For CUDA 12.4\nconda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia\n\n# Install pip dependencies\npython -m pip install -r requirements.txt\n\n# Install flash attention v2 for acceleration (requires CUDA 11.8 or above)\npython -m pip install ninja\npython -m pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3\n```\n\nIn case of running into float point exception(core dump) on the specific GPU type, you may try the following solutions:\n\n```shell\n# Option 1: Making sure you have installed CUDA 12.4, CUBLAS>=12.4.5.8, and CUDNN>=9.00 (or simply using our CUDA 12 docker image).\npip install nvidia-cublas-cu12==12.4.5.8\nexport LD_LIBRARY_PATH=/opt/conda/lib/python3.8/site-packages/nvidia/cublas/lib/\n\n# Option 2: Forcing to explicitly use the CUDA 11.8 compiled version of Pytorch and all the other packages\npip uninstall -r requirements.txt  # uninstall all packages\npip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu118\npip install -r requirements.txt\npip install ninja\npip install git+https://github.com/Dao-AILab/flash-attention.git@v2.6.3\n```\n\nAdditionally, you can also use HunyuanVideo Docker image. Use the following command to pull and run the docker image.\n\n```shell\n# For CUDA 12.4 (updated to avoid float point exception)\ndocker pull hunyuanvideo/hunyuanvideo:cuda_12\ndocker run -itd --gpus all --init --net=host --uts=host --ipc=host --name hunyuanvideo --security-opt=seccomp=unconfined --ulimit=stack=67108864 --ulimit=memlock=-1 --privileged hunyuanvideo/hunyuanvideo:cuda_12\npip install gradio==3.39.0 diffusers==0.33.0 transformers==4.41.2\n\n# For CUDA 11.8\ndocker pull hunyuanvideo/hunyuanvideo:cuda_11\ndocker run -itd --gpus all --init --net=host --uts=host --ipc=host --name hunyuanvideo --security-opt=seccomp=unconfined --ulimit=stack=67108864 --ulimit=memlock=-1 --privileged hunyuanvideo/hunyuanvideo:cuda_11\npip install gradio==3.39.0 diffusers==0.33.0 transformers==4.41.2\n```\n\n\n\n\n## Model\n\n### Download Pretrained Models\n\nHunyuanVideo-Avatar Pretrained [Models](https://huggingface.co/tencent/HunyuanVideo-Avatar)\n\nAll models are stored in `ComfyUI/models/HunyuanVideo-Avatar/weights` by default, and the file structure is as follows\n\n```shell\nHunyuanVideo-Avatar\n  ├──weights\n  │  ├──ckpts\n  │  │  ├──README.md\n  │  │  ├──hunyuan-video-t2v-720p\n  │  │  │  ├──transformers\n  │  │  │  │  ├──mp_rank_00_model_states.pt\n  │  │  │  │  ├──mp_rank_00_model_states_fp8.pt\n  │  │  │  │  ├──mp_rank_00_model_states_fp8_map.pt\n  │  │  │  ├──vae\n  │  │  │  │  ├──pytorch_model.pt\n  │  │  │  │  ├──config.json\n  │  │  ├──llava_llama_image\n  │  │  │  ├──model-00001-of-00004.safatensors\n  │  │  │  ├──model-00002-of-00004.safatensors\n  │  │  │  ├──model-00003-of-00004.safatensors\n  │  │  │  ├──model-00004-of-00004.safatensors\n  │  │  │  ├──...\n  │  │  ├──text_encoder_2\n  │  │  ├──whisper-tiny\n  │  │  ├──det_align\n  │  │  ├──...\n```\n\n### Download HunyuanVideo-Avatar model\n\nTo download the HunyuanCustom model, first install the huggingface-cli. (Detailed instructions are available [here](https://huggingface.co/docs/huggingface_hub/guides/cli).)\n\n```shell\npython -m pip install \"huggingface_hub[cli]\"\n```\n\nThen download the model using the following commands:\n\n```shell\n# Switch to the directory named 'HunyuanVideo-Avatar/weights'\ncd HunyuanVideo-Avatar/weights\n# Use the huggingface-cli tool to download HunyuanVideo-Avatar model in HunyuanVideo-Avatar/weights dir.\n# The download time may vary from 10 minutes to 1 hour depending on network conditions.\nhuggingface-cli download tencent/HunyuanVideo-Avatar --local-dir ./\n```\n\n\n## Requirements\n\n* An NVIDIA GPU with CUDA support is required. \n  * The model is tested on a machine with 8GPUs.\n  * **Minimum**: The minimum GPU memory required is 24GB for 704px768px129f but very slow.\n  * **Recommended**: We recommend using a GPU with 96GB of memory for better generation quality.\n  * **Tips**: If OOM occurs when using GPU with 80GB of memory, try to reduce the image resolution. \n* Tested operating system: Linux\n\n", "metadata": {"source": "github_readmes\\Yuan-ManX_ComfyUI-HunyuanVideo-Avatar_README.md", "filename": "Yuan-ManX_ComfyUI-HunyuanVideo-Avatar_README.md", "type": "readme_full"}}
{"id": "zdou0830_METER_README.md", "paper_id": "zdou0830_METER_README", "text": "# METER: A Multimodal End-to-end TransformER Framework\n\n## Install\n\n```bash\npip install -r requirements.txt\npip install -e .\n```\n\n## Pre-trained Checkpoints\n\nHere are the pre-trained models:\n1. METER-CLIP16-RoBERTa (resolution: 288^2) pre-trained on GCC+SBU+COCO+VG [link](https://github.com/zdou0830/METER/releases/download/checkpoint/meter_clip16_288_roberta_pretrain.ckpt)\n2. METER-CLIP16-RoBERTa (resolution: 224^2) pre-trained on GCC+SBU+COCO+VG [link](https://github.com/zdou0830/METER/releases/download/checkpoint2/meter_clip16_224_roberta_pretrain.ckpt)\n3. METER-SwinBase-RoBERTa (resolution: 384^2) pre-trained on GCC+SBU+COCO+VG [link](https://github.com/zdou0830/METER/releases/download/checkpoint2/meter_swinbase_384_roberta_pretrain.ckpt)\n4. METER-CLIP16-RoBERTa fine-tuned on VQAv2 (resolution: 576^2) [link](https://github.com/zdou0830/METER/releases/download/checkpoint/meter_clip16_288_roberta_vqa.ckpt)\n5. METER-CLIP16-RoBERTa fine-tuned on NLVR2 (resolution: 288^2) [link](https://github.com/zdou0830/METER/releases/download/checkpoint/meter_clip16_288_roberta_nlvr2.ckpt)\n6. METER-CLIP16-RoBERTa fine-tuned on SNLI-VE (resolution: 384^2) [link](https://github.com/zdou0830/METER/releases/download/checkpoint/meter_clip16_288_roberta_snli.ckpt)\n7. METER-CLIP16-RoBERTa fine-tuned on Flickr30k IR/TR (resolution: 384^2) [link](https://github.com/zdou0830/METER/releases/download/checkpoint/meter_clip16_288_roberta_flickr.ckpt)\n8. METER-CLIP16-RoBERTa fine-tuned on COCO IR/TR (resolution: 384^2) [link](https://github.com/zdou0830/METER/releases/download/checkpoint/meter_clip16_288_roberta_coco.ckpt)\n\n\n## Dataset Preparation\n\nWe follow [ViLT](https://github.com/dandelin/ViLT) and use `pyarrow` to serialize the datasets. See [this link](https://github.com/dandelin/ViLT/blob/master/DATA.md) for details.\n\n## Pre-training\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_mlm_itm_clip_bert per_gpu_batchsize=<BS_FITS_YOUR_GPU> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE>\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_mlm_itm_clip_bert per_gpu_batchsize=32 clip16 text_roberta image_size=288\n``` \n\n## Fine-tuning on Downstream Tasks\n\n### VQAv2\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_vqa_clip_bert per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> <IMAGE_AUGMENTATION>\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_vqa_clip_bert per_gpu_batchsize=32 load_path=meter_pretrain.ckpt clip16 text_roberta image_size=576 clip_randaug \n``` \n\n### Flickr30k IR/TR\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_irtr_f30k_clip_bert get_recall_metric=False per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> <IMAGE_AUGMENTATION>\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_irtr_f30k_clip_bert get_recall_metric=False per_gpu_batchsize=32 load_path=meter_pretrain.ckpt clip16 text_roberta image_size=384 clip_randaug \n``` \n\n### COCO IR/TR\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_irtr_coco_clip_bert get_recall_metric=False per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> <IMAGE_AUGMENTATION>\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_irtr_coco_clip_bert get_recall_metric=False per_gpu_batchsize=32 load_path=meter_pretrain.ckpt clip16 text_roberta image_size=384 clip_randaug \n``` \n\n### NLVR2\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES>  task_finetune_nlvr2_clip_bert per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> <IMAGE_AUGMENTATION>\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1  task_finetune_nlvr2_clip_bert per_gpu_batchsize=32 load_path=meter_pretrain.ckpt clip16 text_roberta image_size=288 clip_randaug \n``` \n\n### SNLI-VE\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_snli_clip_bert per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> <IMAGE_AUGMENTATION>\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_snli_clip_bert per_gpu_batchsize=8 load_path=meter_pretrain.ckpt clip16 text_roberta image_size=384 clip_randaug \n``` \n\n## Evaluation on Downstream Tasks\n\n### VQAv2\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_vqa_clip_bert per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> test_only=True\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_vqa_clip_bert per_gpu_batchsize=32 load_path=meter_vqa.ckpt clip16 text_roberta image_size=576 test_only=True\n``` \n\nThen, submit the json file in the `result` directory to [eval.ai](https://eval.ai/web/challenges/challenge-page/830/overview) evaluation server to get the test-dev and/or test-std scores.\n\n\n### Flickr30k IR/TR\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_irtr_f30k_clip_bert get_recall_metric=True per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> test_only=True\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_irtr_f30k_clip_bert get_recall_metric=True per_gpu_batchsize=32 load_path=meter_f30k.ckpt clip16 text_roberta image_size=384 test_only=True\n``` \n\nThe returned values are IR R@1, R@5, R@10 and TR R@1, R@5, R@10.\n\n### COCO IR/TR\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_irtr_coco_clip_bert get_recall_metric=True per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> test_only=True\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_irtr_coco_clip_bert get_recall_metric=True per_gpu_batchsize=32 load_path=meter_coco.ckpt clip16 text_roberta image_size=384 test_only=True\n``` \n\nThe returned values are IR R@1, R@5, R@10 and TR R@1, R@5, R@10.\n\n### NLVR2\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES>  task_finetune_nlvr2_clip_bert per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> test_only=True\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1  task_finetune_nlvr2_clip_bert per_gpu_batchsize=32 load_path=meter_nlvr2.ckpt clip16 text_roberta image_size=288 test_only=True\n``` \n\n### SNLI-VE\n\n```bash\nexport MASTER_ADDR=$DIST_0_IP\nexport MASTER_PORT=$DIST_0_PORT\nexport NODE_RANK=$DIST_RANK\npython run.py with data_root=<ARROW_ROOT> num_gpus=<NUM_GPUS> num_nodes=<NUM_NODES> task_finetune_snli_clip_bert per_gpu_batchsize=<BS_FITS_YOUR_GPU> load_path=<PRETRAINED_MODEL> <IMAGE_ENCODER> <TEXT_ENCODER> image_size=<IMAGE_SIZE> test_only=True\n```\n\nHere is an example:\n```bash\npython run.py with data_root=/data2/dsets/dataset num_gpus=8 num_nodes=1 task_finetune_snli_clip_bert per_gpu_batchsize=8 load_path=meter_snli.ckpt clip16 text_roberta image_size=384 test_only=True\n``` \n\n\n\n## Citation\n\n```\n@inproceedings{dou2022meter,\n  title={An Empirical Study of Training End-to-End Vision-and-Language Transformers},\n  author={Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and Liu, Zicheng and Zeng, Michael},\n  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year={2022},\n  url={https://arxiv.org/abs/2111.02387},\n}\n```\n\n## Acknowledgements\n\nThe code is based on [ViLT](https://github.com/dandelin/ViLT) licensed under [Apache 2.0](https://github.com/dandelin/ViLT/blob/master/LICENSE) and some of the code is borrowed from [CLIP](https://github.com/openai/CLIP) and [Swin-Transformer](https://github.com/microsoft/Swin-Transformer).\n", "metadata": {"source": "github_readmes\\zdou0830_METER_README.md", "filename": "zdou0830_METER_README.md", "type": "readme_full"}}
{"id": "zhang-haojie_LetsTalk_README.md", "paper_id": "zhang-haojie_LetsTalk_README", "text": "<div align=\"center\">\n\n<h1> Efficient Long-duration Talking Video Synthesis with Linear Diffusion Transformer under Multimodal Guidance </h1>\n\n####  <p align=\"center\"> [Haojie Zhang](https://zhang-haojie.github.io/), [Zhihao Liang](https://lzhnb.github.io/), Ruibo Fu, Bingyan Liu, Zhengqi Wen, </p>\n####  <p align=\"center\"> Xuefei Liu, Jianhua Tao, Yaling Liang</p>\n\n<a href='https://zhang-haojie.github.io/project-pages/letstalk.html'><img src='https://img.shields.io/badge/Project-Page-green'></a> \n<a href='https://arxiv.org/abs/2411.16748'><img src='https://img.shields.io/badge/Technique-Report-red'></a> \n\n</div>\n\n\n## 🚀 Introduction\n**TL;DR:** We propose LetsTalk, a diffusion transformer for audio-driven portrait animation. By leveraging DC-VAE and linear attention, LetsTalk enables efficient multimodal fusion and consistent portrait generation, while memory bank and noise-regularized training further improve the quality and stability of long-duration videos.\n\n<div align=\"center\">\n<img width=\"800\" alt=\"image\" src=\"assets/teaser.webp?raw=true\">\n</div>\n\n**Abstract:** Long-duration talking video synthesis faces enduring challenges in achieving high video quality, portrait and temporal consistency, and computational efficiency. As video length increases, issues such as visual degradation, identity inconsistency, temporal incoherence, and error accumulation become increasingly problematic, severely affecting the realism and reliability of the results.\nTo address these challenges, we present LetsTalk, a diffusion transformer framework equipped with multimodal guidance and a novel memory bank mechanism, explicitly maintaining contextual continuity and enabling robust, high-quality, and efficient generation of long-duration talking videos. In particular, LetsTalk introduces a noise-regularized memory bank to alleviate error accumulation and sampling artifacts during extended video generation. To further improve efficiency and spatiotemporal consistency, LetsTalk employs a deep compression autoencoder and a spatiotemporal-aware transformer with linear attention for effective multimodal fusion. We systematically analyze three fusion schemes and show that combining deep (Symbiotic Fusion) for portrait features and shallow (Direct Fusion) for audio achieves superior visual realism and precise speech-driven motion, while preserving diversity of movements. Extensive experiments demonstrate that LetsTalk establishes new state-of-the-art in generation quality, producing temporally coherent and realistic talking videos with enhanced diversity and liveliness, and maintains remarkable efficiency with 8x fewer parameters than previous approaches.\n\n\n## 🎁 Overview\n\n<div align=\"center\">\n<img width=\"800\" alt=\"image\" src=\"assets/Pipeline.webp?raw=true\">\n</div>\n\nOverview of our LetsTalk framework for robust long-duration talking head video generation. Our system combines a deep compression autoencoder to reduce spatial redundancy while preserving temporal features, and transformer blocks with intertwined temporal and spatial attention to effectively capture both intra-frame details and long-range dependencies. \nPortrait and audio embeddings are extracted; Symbiotic Fusion integrates the portrait embedding, and Direct Fusion incorporates the audio embedding, providing effective multimodal guidance for video synthesis. Portrait embeddings are repeated along the temporal axis for consistent conditioning across frames. \nTo further support long-sequence generation, a memory bank module is introduced to maintain temporal consistency, while a dedicated noise-regularized training strategy helps align the memory bank usage between training and inference stages, ensuring stable and high-fidelity generation.\n\n<div align=\"center\">\n<img width=\"800\" alt=\"image\" src=\"assets/schemes.png?raw=true\">\n</div>\n\nIllustration of three multimodal fusion schemes, our transformer backbone is formed by the left-side blocks.\n\n(a) **Direct Fusion**. Directly feeding condition into each block's cross-attention module;\n\n(b) **Siamese Fusion**. Maintaining a similar transformer and feeding the condition into it, extracting the corresponding features to guide the features in the backbone;\n\n(c) **Symbiotic Fusion**. Concatenating modality with the input at the beginning, then feeding it into the backbone, achieving fusion via the inherent self-attention mechanisms.\n\n<!-- \n## 📆 TODO\n- [ ] Release code (coming soon!!!) -->\n\n<!-- ## Visualization\n\n### English\n\n### Chinese\n\n### Singing\n\n### AI-generated Portraits -->\n\n\n\n## 🎫 Citation\nIf you find this project useful in your research, please consider the citation:\n\n```BibTeX\n@article{zhang2024efficient,\n  title={Efficient Long-duration Talking Video Synthesis with Linear Diffusion Transformer under Multimodal Guidance},\n  author={Zhang, Haojie and Liang, Zhihao and Fu, Ruibo and Liu, Bingyan and Wen, Zhengqi and Liu, Xuefei and Tao, Jianhua and Liang, Yaling},\n  journal={arXiv preprint arXiv:2411.16748},\n  year={2024}\n}\n```\n", "metadata": {"source": "github_readmes\\zhang-haojie_LetsTalk_README.md", "filename": "zhang-haojie_LetsTalk_README.md", "type": "readme_full"}}
{"id": "zhangxuying1004_RSTNet_README.md", "paper_id": "zhangxuying1004_RSTNet_README", "text": "# RSTNet: Relationship-Sensitive Transformer Network\nThis repository contains the reference code for our paper [_RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words_ (CVPR 2021)](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RSTNet_Captioning_With_Adaptive_Attention_on_Visual_and_Non-Visual_Words_CVPR_2021_paper.pdf).\n\n<p align=\"center\">\n  <img src=\"images/RSTNet.png\" alt=\"Relationship-Sensitive Transformer\" width=\"800\"/>\n</p>\n\n## Tips\nIf you have any questions about our work, feel free to post issues on this github project. I will answer your questions and update code monthly.  \nIf you are in hurry, please email me via [zhangxuying1004@gmail.com](zhangxuying1004@gmail.com).   \nIf our work is helpful to you or gives some inspiration to you, please star this project and cite our paper. Thank you!  \n```\n@inproceedings{zhang2021rstnet,\n  title={RSTNet: Captioning with adaptive attention on visual and non-visual words},\n  author={Zhang, Xuying and Sun, Xiaoshuai and Luo, Yunpeng and Ji, Jiayi and Zhou, Yiyi and Wu, Yongjian and Huang, Feiyue and Ji, Rongrong},\n  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages={15465--15474},\n  year={2021}\n}\n```\n\n## Environment setup\nClone the repository and create the `m2release` conda environment using the `environment.yml` file:\n```\nconda env create -f environment.yml\nconda activate m2release\n```\n\nThen, download spacy data by executing the following command:   \n```python -m spacy download en``` or ```python -m spacy download en_core_web_sm```.\n\nYou also need to create 5 new folders, namely ```Datasets```, ```save_language_models```, ```language_tensorboard_logs```,  ```save_transformer_models``` and ```transformer_tensorboard_logs``` in the root directory of this project.\n\n## Data preparation\nTo run our code, you need to put annotations folder ```m2_annotations```, visual features folder ```X101-features``` for the COCO dataset into ```Datasets```.  \n\nMost annotations have been prepared by [1], please download [m2_annotations](https://drive.google.com/drive/folders/1tJnetunBkQ4Y5A3pq2P53yeJuGa4lX9e?usp=sharing) and put it into this root directory.  \n\nVisual features are computed with the code provided by [2]. To reproduce our result, please download the COCO features file such as ```X-101-features.tgz``` in [grid-feats-vqa\n](https://github.com/facebookresearch/grid-feats-vqa) and rename the extracted folder as ```X101-features```. Considering that this feature file is huge, you can alternatively save the features as float16 for storage space saving by executing the following command:\n```\npython switch_datatype.py\n```\nIn order to solve the shape difference and match the feat shape with region feat shape (`50` regions), please execute the following command to reshape the visual to `49(7x7)` and save all visual features as a h5py file.\n```\npython feats_process.py\n```\nNote that you can also access to my processed offline image features [coco_grid_feats](https://pan.baidu.com/s/1myelTYJE8a1HDZHkoccfIA) in Baidu Netdisk with the extraction code ```cvpr``` for convenience.  \n\n<!-- Note that, you can also use my processed offline image features [X101_grid_feats_coco_trainval.hdf5](https://pan.xunlei.com/s/VMyFV3OcrpOj7TdWkt5_amwiA1) with extraction code ```wsvg``` and my processed online image features [X101_grid_feats_coco_test.hdf5](https://pan.xunlei.com/s/VN-YFlVCAGDe_glAUaPkNOg2A1) with extraction code ```qzwm``` for convenience.  -->\n\nIn addition, if you want to extract the grid-based features of your custom image dataset, you can refer to the codes in project [grid-feats-vqa\n](https://github.com/facebookresearch/grid-feats-vqa).\n\n## Training procedure\nRun `python train_language.py` and `python train_transformer.py` in sequence using the following arguments:\n\n| Argument | Possible values |\n|------|------|\n| `--exp_name` | Experiment name|\n| `--batch_size` | Batch size (default: 50) |\n| `--workers` | Number of workers, accelerate model training in the xe stage.|\n| `--head` | Number of heads (default: 8) |\n| `--resume_last` | If used, the training will be resumed from the last checkpoint. |\n| `--resume_best` | If used, the training will be resumed from the best checkpoint. |\n| `--features_path` | Path to visual features file (h5py)|\n| `--annotation_folder` | Path to m2_annotations |\n\nFor example, to train our BERT-based language model with the parameters used in our experiments, use\n```\npython train_language.py --exp_name bert_language --batch_size 50 --features_path /path/to/features --annotation_folder /path/to/annotations\n```\nto train our rstnet model with the parameters used in our experiments, use\n```\npython train_transformer.py --exp_name rstnet --batch_size 50 --m 40 --head 8 --features_path /path/to/features --annotation_folder /path/to/annotations\n```\nThe figure below shows the changes of cider value during the training of rstnet. You can also visualize the training details by calling the tensorboard files in ```tensorboard_logs```.\n<p align=\"center\">\n  <img src=\"images/train_cider.png\" alt=\"cider changes\" width=\"800\"/>\n</p>\n\n## Evaluation\n### Offline Evaluation  \nRun `python test_offline.py` to evaluate the performance of rstnet on the Karpathy test split of MS COCO dataset.\n\n### Online Evaluation  \nRun `python test_online.py` to generate required files and evaluate the performance of rstnet on the official test server of MS COCO dataset.  \n\nNote that, to reproduce the our reported results, you can also download our pretrained model files in the ```pretrained_models``` folder and put them into folder ```saved_language_models``` and folder ```saved_language_models``` repectively . The results of offline evaluation (Karpathy test split of MS COCO) are as follows：   \n<p align=\"center\">\n  <img src=\"images/results.png\" alt=\"offline evaluation\" width=\"800\"/>\n</p>\n\n#### References\n[1] Cornia, M., Stefanini, M., Baraldi, L., & Cucchiara, R. (2020). Meshed-memory transformer for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.  \n[2] Jiang, H., Misra, I., Rohrbach, M., Learned-Miller, E., & Chen, X. (2020). In defense of grid features for visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.   \n\n\n#### Acknowledgements\nThank Cornia _et.al_ for their open source code [meshed-memory-transformer\n](https://github.com/aimagelab/meshed-memory-transformer), on which our implements are based.  \nThank Jiang _et.al_ for the significant discovery in visual representation [2], which has given us a lot of inspiration.\n", "metadata": {"source": "github_readmes\\zhangxuying1004_RSTNet_README.md", "filename": "zhangxuying1004_RSTNet_README.md", "type": "readme_full"}}
{"id": "zjunlp_MKGformer_README.md", "paper_id": "zjunlp_MKGformer_README", "text": "# MKGFormer\n\nCode for the SIGIR 2022 paper \"[Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion](https://arxiv.org/pdf/2205.02357.pdf)\"\n\n- ❗NOTE: We provide some KGE baselines at [OpenBG-IMG](https://github.com/OpenBGBenchmark/OpenBG-IMG).\n- ❗NOTE: We release a new MKG task \"[Multimodal Analogical Reasoning over Knowledge Graphs](https://arxiv.org/abs/2210.00312) (ICLR'2023)\" at [MKG_Analogy](https://zjunlp.github.io/project/MKG_Analogy/).\n\n# Model Architecture\n\n<div align=center>\n<img src=\"resource/model.png\" width=\"75%\" height=\"75%\" />\n</div>\n \n \n Illustration of MKGformer for (a) Unified Multimodal KGC Framework and (b) Detailed M-Encoder.\n\n\n# Requirements\n\nTo run the codes (**Python 3.8**), you need to install the requirements:\n```\npip install -r requirements.txt\n```\n\nData Preprocess\n==========\nTo extract visual object images int MNER and MRE tasks, we first use the NLTK parser to extract noun phrases from the text and apply the [visual grouding toolkit](https://github.com/zyang-ur/onestage_grounding) to detect objects. Detailed steps are as follows:\n\n1. Using the NLTK parser (or Spacy, textblob) to extract noun phrases from the text.\n2. Applying the [visual grouding toolkit](https://github.com/zyang-ur/onestage_grounding) to detect objects. Taking the twitter2017 dataset as an example, the extracted objects are stored in `twitter2017_aux_images`. The images of the object obey the following naming format: `id_pred_yolo_crop_num.png`, where `id` is the order of the raw image corresponding to the object, `num` is the number of the object predicted by the toolkit. (`id` is doesn't matter.)\n3. Establishing the correspondence between the raw images and the objects. We construct a dictionary to record the correspondence between the raw images and the objects. Taking `twitter2017/twitter2017_train_dict.pth` as an example, the format of the dictionary can be seen as follows: `{imgname:['id_pred_yolo_crop_num0.png', 'id_pred_yolo_crop_num1.png', ...] }`, where key is the name of raw images, value is a List of the objects (Note that in `train/val/test.txt`, text and raw image have a one-to-one relationship, so the `imgnae` can be used as a unique identifier for the raw images).\n\nThe detected objects and the dictionary of the correspondence between the raw images and the objects are available in our data links.\n\n# Data Download\n\nThe datasets that we used in our experiments are as follows:\n\n\n+ Twitter2017\n    \n    You can download the twitter2017 dataset from [Google Drive](https://drive.google.com/file/d/1ogfbn-XEYtk9GpUECq1-IwzINnhKGJqy/view?usp=sharing).\n\n    For more information regarding the dataset, please refer to the [UMT](https://github.com/jefferyYu/UMT/) repository.\n\n+ MRE\n    \n    The MRE dataset comes from [MEGA](https://github.com/thecharm/Mega), many thanks.\n\n    You can download the **MRE dataset with detected visual objects** from [Google Drive](https://drive.google.com/file/d/1q5_5vnHJ8Hik1iLA9f5-6nstcvvntLrS/view?usp=sharing) or using following command:\n    \n    ```bash\n    cd MRE\n    wget 121.41.117.246/Data/re/multimodal/data.tar.gz\n    tar -xzvf data.tar.gz\n    ```\n\n+ MKG\n\n    + FB15K-237-IMG\n\n        You can download the image data of FB15k-237 from [mmkb](https://github.com/mniepert/mmkb) which provides a list of image URLs, and refer to more information of description of entity from [kg-bert](https://github.com/yao8839836/kg-bert) repositories.\n        \n       - **❗NOTE: we have found a severe bug in the code of data preprocessing for FB15k-237-IMG, which leads to the unfair performance comparison; we have updated the performance in [arxiv](https://arxiv.org/pdf/2205.02357.pdf) and released the [checkpoints](https://drive.google.com/drive/folders/1NsLA7mXaVnhlYNvzRDWIcBxq2CpKF_6m) (The model trained with/without the severe bug).**\n\n    + WN18-IMG\n\n        Entity images in WN18 can be obtained from ImageNet, the specific steps can refer to RSME. the [RSME](https://github.com/wangmengsd/RSME) repository.\n\nWe also provide additional network disk links for **multimodal KG data (Images) at [GoogleDrive](https://drive.google.com/file/d/197c4fCLVC6F7sCBqZIDwm5tjWeGJnZ5-/view?usp=share_link) or [Baidu Pan](https://pan.baidu.com/s/1TVArQSLmPjr2FsC8NkSiOA) with extraction (code:ilbd)**.\n\nThe expected structure of files is:\n\n\n```\nMKGFormer\n |-- MKG\t# Multimodal Knowledge Graph\n |    |-- dataset       # task data\n |    |-- data          # data process file\n |    |-- lit_models    # lightning model\n |    |-- models        # mkg model\n |    |-- scripts       # running script\n |    |-- main.py   \n |-- MNER\t# Multimodal Named Entity Recognition\n |    |-- data          # task data\n |    |    |-- twitter2017\n |    |    |    |-- twitter17_detect            # rcnn detected objects\n |    |    |    |-- twitter2017_aux_images      # visual grounding objects\n |    |    |    |-- twitter2017_images          # raw images\n |    |    |    |-- train.txt                   # text data\n |    |    |    |-- ...\n |    |    |    |-- twitter2017_train_dict.pth  # {imgname: [object-image]}\n |    |    |    |-- ...\n |    |-- models        # mner model\n |    |-- modules       # running script\n |    |-- processor     # data process file\n |    |-- utils\n |    |-- run_mner.sh\n |    |-- run.py\n |-- MRE    # Multimodal Relation Extraction\n |    |-- data          # task data\n |    |    |-- img_detect   # rcnn detected objects\n |    |    |-- img_org      # raw images\n |    |    |-- img_vg       # visual grounding objects\n |    |    |-- txt          # text data\n |    |    |    |-- ours_train.txt\n |    |    |    |-- ours_val.txt\n |    |    |    |-- ours_test.txt\n |    |    |    |-- mre_train_dict.pth  # {imgid: [object-image]}\n |    |    |    |-- ...\n |    |    |-- vg_data      # [(id, imgname, noun_phrase)], not useful\n |    |    |-- ours_rel2id.json         # relation data\n |    |-- models        # mre model\n |    |-- modules       # running script\n |    |-- processor     # data process file\n |    |-- run_mre.sh\n |    |-- run.py\n```\n\n# How to run\n\n\n+ ## MKG Task\n\n    - First run Image-text Incorporated Entity Modeling to train entity embedding.\n\n    ```shell\n        cd MKG\n        bash scripts/pretrain_fb15k-237-image.sh\n    ```\n\n    - Then do Missing Entity Prediction.\n\n\n    ```shell\n        bash scripts/fb15k-237-image.sh\n    ```\n\n+ ## MNER Task\n\n    To run mner task, run this script.\n\n    ```shell\n    cd MNER\n    bash run_mner.sh\n    ```\n\n+ ## MRE Task\n\n    To run mre task, run this script.\n\n    ```shell\n    cd MRE\n    bash run_mre.sh\n    ```\n\n# Acknowledgement\n\nThe acquisition of image data for the multimodal link prediction task refer to the code from [https://github.com/wangmengsd/RSME](https://github.com/wangmengsd/RSME), many thanks.\n\n# Papers for the Project & How to Cite\nIf you use or extend our work, please cite the paper as follows:\n\n```bibtex\n@inproceedings{DBLP:conf/sigir/ChenZLDTXHSC22,\n  author    = {Xiang Chen and\n               Ningyu Zhang and\n               Lei Li and\n               Shumin Deng and\n               Chuanqi Tan and\n               Changliang Xu and\n               Fei Huang and\n               Luo Si and\n               Huajun Chen},\n  editor    = {Enrique Amig{\\'{o}} and\n               Pablo Castells and\n               Julio Gonzalo and\n               Ben Carterette and\n               J. Shane Culpepper and\n               Gabriella Kazai},\n  title     = {Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge\n               Graph Completion},\n  booktitle = {{SIGIR} '22: The 45th International {ACM} {SIGIR} Conference on Research\n               and Development in Information Retrieval, Madrid, Spain, July 11 -\n               15, 2022},\n  pages     = {904--915},\n  publisher = {{ACM}},\n  year      = {2022},\n  url       = {https://doi.org/10.1145/3477495.3531992},\n  doi       = {10.1145/3477495.3531992},\n  timestamp = {Mon, 11 Jul 2022 12:19:20 +0200},\n  biburl    = {https://dblp.org/rec/conf/sigir/ChenZLDTXHSC22.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n", "metadata": {"source": "github_readmes\\zjunlp_MKGformer_README.md", "filename": "zjunlp_MKGformer_README.md", "type": "readme_full"}}
